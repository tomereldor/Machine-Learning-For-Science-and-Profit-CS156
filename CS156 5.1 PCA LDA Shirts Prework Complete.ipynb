{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS156 Assignment 5\n",
    "## PCA & LDA for Classifying EigenFashion \n",
    "### Tomer Eldor\n",
    "#### Minerva Schools\n",
    "*Text Analysis is after each corresponding code block, and complete longer analysis of results is at the end.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# original code reference: (By Joel Grus, author of the blog mentioned in the preclass-work)\n",
    "# adapted from https://github.com/joelgrus/shirts/blob/master\n",
    "\n",
    "### Setup ###\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import shuffle, seed\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics #, linear_model,\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetching Files (globbing them)\n",
    "girls_files = glob('/Users/tomereldor/PycharmProjects/CS156ML/PCA/women/womensample/*')\n",
    "boys_files = glob('/Users/tomereldor/PycharmProjects/CS156ML/PCA/men/mensample/*')\n",
    "\n",
    "# STANDARTIZE SIZE and SHAPE \n",
    "STANDARD_SIZE = (138,138) #most common size found in the dataset\n",
    "HALF_SIZE = (STANDARD_SIZE[0]/2,STANDARD_SIZE[1]/2)\n",
    "\n",
    "def img_to_array(filename):\n",
    "    # takes a filename and turns it into a numpy array of RGB pixels\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    img = list(img.getdata())\n",
    "    img = map(list, img)\n",
    "    img = np.asarray(img) # np.asarray() will extricate the RGB channels from each pixel in the original image),\n",
    "    s = img.shape[0] * img.shape[1] #size\n",
    "    img_wide = img.reshape(1, s) #reshaping \n",
    "    # Image.close() \n",
    "    return img_wide[0]\n",
    "\n",
    "# Processing files: converting Image to Numpy RGB arrays\n",
    "raw_data = [(img_to_array(filename),'girl',filename) for filename in girls_files] + \\\n",
    "           [(img_to_array(filename),'boy',filename) for filename in boys_files]\n",
    "\n",
    "## print(raw_data[:10]) #debug\n",
    "\n",
    "# shuffle the data ('randomly', though I'm seeding for replicating results)\n",
    "seed(0)\n",
    "shuffle(raw_data)\n",
    "\n",
    "# pull out the features and the labels\n",
    "data = np.array([cd for (cd,_y,f) in raw_data])\n",
    "labels = np.array([_y for (cd,_y,f) in raw_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset from the into 80% training data and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X   Train N: 336, X   Test N: 84\n",
      "Y   Train N: 336, Y   Test N: 84\n"
     ]
    }
   ],
   "source": [
    "train_split = int(0.8*len(data)) \n",
    "#splitting: \n",
    "X_train , X_test = data[:train_split] , data[train_split:] \n",
    "y_train , y_test = labels[:train_split] , labels[train_split:]\n",
    "\n",
    "#debug / verification:\n",
    "print(\"X   Train N: {}, X   Test N: {}\".format(len(X_train), len(X_test)))\n",
    "print(\"Y   Train N: {}, Y   Test N: {}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Build a simple linear classifier using the original pixel data. What is your error rate on the training data? What is your error rate on your testing data?\n",
    "I start with a simple linear model - **Logistic Regression**.\n",
    "I chose logistic regression since it is simple and good as a starting point, in case it works well. It is useful for me since I'm more familiar with it and wanted to see how well it would perform in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Trainig set (no reduction)\n",
      "\n",
      "Accuracy Score on test set: 1.000\n",
      "[[173   0]\n",
      " [  0 163]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       173\n",
      "          1       1.00      1.00      1.00       163\n",
      "\n",
      "avg / total       1.00      1.00      1.00       336\n",
      "\n",
      "____________________________________________________\n",
      "\n",
      "Logistic Regression: TEST set (no reduction)\n",
      "Accuracy Score on test set: 0.631\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[27 10]\n",
      " [21 26]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.73      0.64        37\n",
      "          1       0.72      0.55      0.63        47\n",
      "\n",
      "avg / total       0.65      0.63      0.63        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LOGISTIC REGRESSION\n",
    "# fitting the model\n",
    "clf_log =  LogisticRegression(penalty='l2')  # sepcifying penalty (regularization term) of L2, with a dual formulation only for the L2 penalty.\n",
    "clf_log.fit(X_train, y_train)\n",
    "\n",
    "# TRAIN set prediction and accuracy\n",
    "print \"Logistic Regression: Trainig set (no reduction)\"\n",
    "predicted_train_log = clf_log.predict(X_train)\n",
    "score_log_train = clf_log.score(X_train, y_train)\n",
    "print \"\\nAccuracy Score on test set: %.3f\" % score_log_train\n",
    "print(metrics.confusion_matrix(y_train, predicted_train_log))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_train, predicted_train_log))\n",
    "print \"____________________________________________________\"\n",
    "\n",
    "# TEST set prediction and accuracy\n",
    "print \"\\nLogistic Regression: TEST set (no reduction)\"\n",
    "predicted_test_log = clf_log.predict(X_test)\n",
    "score_log_test = clf_log.score(X_test, y_test)\n",
    "print \"Accuracy Score on test set: %.3f\" % score_log_test\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_test, predicted_test_log))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_test, predicted_test_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with no dimensionality regression performs reasonably but unimpressively, with 0.63 accuracy score overall on the test set. Conversely, we have perfect 1.00 accuracy on the training set, but that isn't surprising and is even not a very good sign since it means that our model is overfitting on the data - what logistic regression can often result with...\n",
    "Also, we see there is some imbalance with the precision-recall per class - the model performs with much higher precision for boys (0.72 vs 0.56) while exhibiting the opposite trend - much higher recall for girls (0.55 vs 0.73). That shows us the model is not balanced in determining male/female; according to the confusion matrix, we see it predicts significantly more false positives. Theoretically, since this is a logistic regression, in this case of too many false positives, we could try adjusting the cutoff value to classify as positive to be slightly higher and see if our accuracy improves.\n",
    "Additionally, I don't think this is the ideal case to use logistic regression since I usually work with logistic regressions when I want to manually insert the features, having an idea of which features or combinations of features should make sense (thus not include everything). However, in this image recognition, the model is left with inspecting all the raw pixel colors data and make sense of it itself. \n",
    "That's why reducing dimensionality should visibly improve logistic regression.\n",
    "Therefore, I wanted to try another kind of model - SVC linear classifier - and see if it performs better on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVC Classifier, TRAINING Set (no reduction)\n",
      "Accuracy Score on training set: 1.000\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[173   0]\n",
      " [  0 163]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       173\n",
      "          1       1.00      1.00      1.00       163\n",
      "\n",
      "avg / total       1.00      1.00      1.00       336\n",
      "\n",
      "____________________________________________________\n",
      "\n",
      "Linear SVC Classifier, TEST SET (no reduction)\n",
      "Accuracy Score on test set: 0.631\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[28  9]\n",
      " [22 25]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.64        37\n",
      "          1       0.74      0.53      0.62        47\n",
      "\n",
      "avg / total       0.66      0.63      0.63        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LINEAR SVC MODEL\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "# fitting the model\n",
    "clf_svc =  LinearSVC(random_state = 42) #(if we give it the answer to the question about the meaning of life and everything as a random seed it might perform better since it knows already the answer to everything)\n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "# Train set prediction and accuracy\n",
    "print \"\\nLinear SVC Classifier, TRAINING Set (no reduction)\"\n",
    "predicted_train_svc = clf_svc.predict(X_train)\n",
    "score_svc_train = clf_svc.score(X_train, y_train)\n",
    "print \"Accuracy Score on training set: %.3f\" % score_svc_train\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_train, predicted_train_svc))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_train, predicted_train_svc))\n",
    "print \"____________________________________________________\"\n",
    "\n",
    "# Test set prediction and accuracy\n",
    "predicted_test_svc = clf_svc.predict(X_test)\n",
    "score_svc_test = clf_svc.score(X_test,y_test)\n",
    "print \"\\nLinear SVC Classifier, TEST SET (no reduction)\"\n",
    "print \"Accuracy Score on test set: %.3f\" % score_svc_test\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_test, predicted_test_svc))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_test, predicted_test_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a linear SVC classifier on the (non-reduced) data, the model did a fair but unimpressive job predicting on the test set, with an accuracy of 0.631. Similarly, it does predict with perfect 1.00 accuracy on the training set, but that just means overfitting, and we should not be too happy about this. This explains the major differences between 1.00 accuracy on the training set to 0.63 accuracies on the test set. This also exhibits the same trend of predicting too many false positives over false negatives and thus creates an imbalanced prediction.\n",
    "\n",
    "\n",
    "Also, I'm not using many samples - only 200 in the first trials; while we have many many features for the models to include. Thus there might be more features than observations, which is bad for the model. Because of the curse of dimensionality, this makes it harder for the model to find the correct costs/distances/what features to use and how to correctly classify, having so many features but few samples.\n",
    "\n",
    "\n",
    "Perhaps SVC with a nonlinear kernel, such as RBF SVM, would have been more flexible and improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with PCA and LDA\n",
    "Below I create both PCA and LDA principal components for dimensionality reductions parallelly (as it's more concise), then train the same model as before on PCA data and LDA data and compare results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:57: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### PCA! - find the principal components\n",
    "pca = RandomizedPCA(n_components=70, random_state=0)\n",
    "X_PCA = pca.fit_transform(data)\n",
    "y = [1 if label == 'boy' else 0 for label in labels]\n",
    "\n",
    "### LDA! - find the LDA reduced dimensions\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=15)\n",
    "#X_LDA = lda.fit(X_PCA, y).transform(X_PCA)\n",
    "X_LDA = lda.fit_transform(data,y)\n",
    "\n",
    "# Percentage of variance explained for each component\n",
    "pca_var_ratios = pca.explained_variance_ratio_\n",
    "lda_var_ratios = lda.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing number of components for LDA and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Dimensions: \n",
      "component 1 adds 0.3223 to variance, now total variance is 0.3223\n",
      "component 2 adds 0.1032 to variance, now total variance is 0.4254\n",
      "component 3 adds 0.0459 to variance, now total variance is 0.4713\n",
      "component 4 adds 0.0329 to variance, now total variance is 0.5042\n",
      "component 5 adds 0.0288 to variance, now total variance is 0.5330\n",
      "component 6 adds 0.0253 to variance, now total variance is 0.5583\n",
      "component 7 adds 0.0196 to variance, now total variance is 0.5779\n",
      "component 8 adds 0.0158 to variance, now total variance is 0.5937\n",
      "component 9 adds 0.0143 to variance, now total variance is 0.6080\n",
      "component 10 adds 0.0116 to variance, now total variance is 0.6196\n",
      "component 11 adds 0.0112 to variance, now total variance is 0.6308\n",
      "component 12 adds 0.0103 to variance, now total variance is 0.6411\n",
      "component 13 adds 0.0092 to variance, now total variance is 0.6503\n",
      "component 14 adds 0.0082 to variance, now total variance is 0.6585\n",
      "component 15 adds 0.0074 to variance, now total variance is 0.6659\n",
      "component 16 adds 0.0068 to variance, now total variance is 0.6728\n",
      "component 17 adds 0.0065 to variance, now total variance is 0.6793\n",
      "component 18 adds 0.0062 to variance, now total variance is 0.6855\n",
      "component 19 adds 0.0060 to variance, now total variance is 0.6915\n",
      "component 20 adds 0.0056 to variance, now total variance is 0.6972\n",
      "component 21 adds 0.0055 to variance, now total variance is 0.7027\n",
      "component 22 adds 0.0052 to variance, now total variance is 0.7079\n",
      "component 23 adds 0.0049 to variance, now total variance is 0.7128\n",
      "component 24 adds 0.0046 to variance, now total variance is 0.7174\n",
      "component 25 adds 0.0044 to variance, now total variance is 0.7218\n",
      "component 26 adds 0.0043 to variance, now total variance is 0.7262\n",
      "component 27 adds 0.0040 to variance, now total variance is 0.7302\n",
      "component 28 adds 0.0038 to variance, now total variance is 0.7340\n",
      "component 29 adds 0.0037 to variance, now total variance is 0.7377\n",
      "component 30 adds 0.0036 to variance, now total variance is 0.7413\n",
      "component 31 adds 0.0033 to variance, now total variance is 0.7446\n",
      "component 32 adds 0.0032 to variance, now total variance is 0.7478\n",
      "component 33 adds 0.0031 to variance, now total variance is 0.7509\n",
      "component 34 adds 0.0030 to variance, now total variance is 0.7540\n",
      "component 35 adds 0.0030 to variance, now total variance is 0.7570\n",
      "component 36 adds 0.0029 to variance, now total variance is 0.7598\n",
      "component 37 adds 0.0027 to variance, now total variance is 0.7626\n",
      "component 38 adds 0.0026 to variance, now total variance is 0.7652\n",
      "component 39 adds 0.0026 to variance, now total variance is 0.7677\n",
      "component 40 adds 0.0025 to variance, now total variance is 0.7702\n",
      "component 41 adds 0.0025 to variance, now total variance is 0.7727\n",
      "component 42 adds 0.0024 to variance, now total variance is 0.7750\n",
      "component 43 adds 0.0024 to variance, now total variance is 0.7774\n",
      "component 44 adds 0.0023 to variance, now total variance is 0.7797\n",
      "component 45 adds 0.0022 to variance, now total variance is 0.7819\n",
      "component 46 adds 0.0022 to variance, now total variance is 0.7841\n",
      "component 47 adds 0.0021 to variance, now total variance is 0.7862\n",
      "component 48 adds 0.0020 to variance, now total variance is 0.7883\n",
      "component 49 adds 0.0020 to variance, now total variance is 0.7903\n",
      "component 50 adds 0.0019 to variance, now total variance is 0.7922\n",
      "component 51 adds 0.0019 to variance, now total variance is 0.7941\n",
      "component 52 adds 0.0019 to variance, now total variance is 0.7960\n",
      "component 53 adds 0.0019 to variance, now total variance is 0.7979\n",
      "component 54 adds 0.0018 to variance, now total variance is 0.7997\n",
      "component 55 adds 0.0018 to variance, now total variance is 0.8014\n",
      "component 56 adds 0.0018 to variance, now total variance is 0.8032\n",
      "component 57 adds 0.0017 to variance, now total variance is 0.8049\n",
      "component 58 adds 0.0017 to variance, now total variance is 0.8066\n",
      "component 59 adds 0.0016 to variance, now total variance is 0.8082\n",
      "component 60 adds 0.0016 to variance, now total variance is 0.8098\n",
      "component 61 adds 0.0015 to variance, now total variance is 0.8113\n",
      "component 62 adds 0.0015 to variance, now total variance is 0.8128\n",
      "component 63 adds 0.0015 to variance, now total variance is 0.8143\n",
      "component 64 adds 0.0014 to variance, now total variance is 0.8157\n",
      "component 65 adds 0.0014 to variance, now total variance is 0.8172\n",
      "component 66 adds 0.0014 to variance, now total variance is 0.8186\n",
      "component 67 adds 0.0014 to variance, now total variance is 0.8199\n",
      "component 68 adds 0.0013 to variance, now total variance is 0.8213\n",
      "component 69 adds 0.0013 to variance, now total variance is 0.8226\n",
      "component 70 adds 0.0013 to variance, now total variance is 0.8239\n",
      "PCA components selected:  70\n"
     ]
    }
   ],
   "source": [
    "# CHOOSING THE NUMBER OF COMPONENTS TO USE IN EITHER PCA AND LDA!\n",
    "# For that, we will compute when the total variance explained is higher than 0.99\n",
    "def select_n_components(var_ratio, goal_var):\n",
    "        total_variance = 0.0 # set initial variance explained\n",
    "        n_components = 0     # set initial number of features\n",
    "        components = []\n",
    "        var_inc = []\n",
    "        var_total = []\n",
    "        # For the explained variance of each feature:\n",
    "        for explained_variance in var_ratio:\n",
    "            # Add the explained variance to the total\n",
    "            total_variance += explained_variance\n",
    "            # Add one to the number of components\n",
    "            n_components += 1\n",
    "            # If we reach our goal level of explained variance\n",
    "            print \"component %d adds %.4f to variance, now total variance is %.4f\" % (n_components, explained_variance, total_variance)\n",
    "            #results.append([n_components, explained_variance, total_variance])\n",
    "            components.append(n_components)\n",
    "            var_inc.append(explained_variance)\n",
    "            var_total.append(total_variance)\n",
    "            if total_variance >= goal_var:\n",
    "                break     # End the loop if we reached our goal variance explained\n",
    "        return n_components, components,var_inc,var_total   # Return the number of components\n",
    "\n",
    "print \"PCA Dimensions: \"\n",
    "pca_n_components, pca_components_ids, pca_var_inc, pca_var_total = select_n_components(pca_var_ratios, 0.95)\n",
    "print \"PCA components selected: \", pca_n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4HVWZ9v/vnZkkzDMBIpNMyiRGNCgHaTGIDCIiaEOL\nw8uvW2xbbMVWkUC3tkA784oEaURmUJl8RYLAwUYEwoySACrQEEKYwgzhJHl+f6y1k8rmnH1q77Pr\njPfnuuraNa2qVbWHZ6+hqhQRmJmZ9WbUQGfAzMyGBgcMMzMrxQHDzMxKccAwM7NSHDDMzKwUBwwz\nMyvFAWOYknS8pHMaLH9I0ntb3HbLaVvV2/HUrXuapK9VlI9lkjavYtt1+9lE0guSVPW+zMpywGiC\npIclvZK/yAsknSVpYmH5+yXdkJcvlHS9pP3qttGRf3S+1A9ZHm4X2ZQ6noj4x4j45kDmoc87iXg0\nIlYLXyg1IvTXH5G+csBoTgD7RsRqwC7ArsDXASQdDFwM/AyYEhHrA98APli3jSOAZ/KrDT2V/+OX\nNLrqfQyk4X58LRoSfwwcMJongIhYAFwFvCXP/w5wQkScFREv5nX+JyKOWp4wlUYOBj4LbCVpl9I7\nldaQdKWkJyU9k8c3Kix/k6ROSc9LuhpYpy794bmE9JSkr9Ytk6SvSPpLXn6hpDXKpO0mn+Mk/Zek\nR3Ip7DRJ4/OyL0u6WdKoPP2Pku7Naabmf1mfkTQ/D19ssJ+L8/YX5ePerrDsLEkn5vE9JD0q6Zhc\n6psv6RMN8vvjWn7z8i9JelzSY5KOpIcvtqRDJM2pm/cFSZfl8Q9IuiO/P49IOr6wXu3YPynpEeDa\nwrzaufqEpPty6fUvkv5PIX1vxzhB0nfye7hI0u8L78lukv6Q598paY8G5/yh/Dn5c/4MnilpXGH5\nB/M2Fkm6UdJb69J+WdLdwEu146rb/vaSZudtL5D0lcJ79P18XI9J+p6ksXXH/qXCsR8gaR9J90t6\nWtK/FfZxvKRL8mf8BUm3SdqhsHwbpZqBRfmzuV9h2VmSTpX065z2j5I2q0tby/9cSR8pk1bSDaTf\nlXvysuXpBp2I8FByAB4C3pvHNwH+BMwEtgaWAVN7SX84MJ/04bgC+EHd8ruBQ3tIuxbwIWA8MAm4\nCLi0sPwm4BRgLPBu4AXg53nZdsCLwPS8/DvA64Vj+XxOv2Fefhpwfpm03eTze8BlwOo5n5cD38zL\nBHSSSl5bAs8CO+RlU/M5PA+YQArETxbyeHztePL0J4CJOU/fBe4sLDsLODGP7wF05fSjgX2Al4HV\nS+R3BrAA2BZYJedtKbB5N8e9CvA8sEVh3q3AR/L4e4Dt8/hb8nb3rzv2n+XtjM/zlgKj8jr7AG/K\n4+/Ox7BTyWP8v8B1wAb5Pdgtn7eNgKeB9+f19srTazf4/N+T060B3Fg4zzsDC0mlbpE+6w8BYwtp\n78hpx3ez7cnA48C/AOPye/H2vOxE0udz7Tz8gfTnrHjsX8vH/mnS5+bc/PnYDniF/N3M52gx6bs0\nGvgi8Lc8PgZ4EDg2j+9J+h5tVfhcPQW8jfRn+1xWfE8mAv9LqjkQsGNed5ve0ubly4DNBvo3rtff\nwIHOwFAa8of+BdIP3UPAj/KX+135yz2ul/TXAN/J44fmL9joFvOyE/BMHt+U9CO+SmH5eawIGMfV\nfTgn5i9N7cf4PmDPwvIN8/ZG9Za2m3y9VPzgA+8E/laYnkqqkrsP+HLd/GW1L2eedxJwRh5fKWDU\n7XONnHbVPF0fMF4m//DmeQuBab3lFzgT+FZh2Vb0EDDy8p8DXy+s+zwwoYd1v1f4LNSCw9S687E8\nYHST/lLgc70dI+nH6xXgLd1s48vA2XXzfgsc3uDz/5nC9D7Ag3n8x+Qf8cLyecC7C2n/ocHn+VDg\n9h6W/YUc1PL03oX3qHbsytOT82dh18L6t7EiOB8P3FRYJtKfuOnA7sDjdfs+H/hG4XM1q+7478vj\nhwA31KX9CXBcb2nz9LKePleDaXCVVPMOiIi1ImKziPhcRCwm/QBC+qHtlqSNSf9Yzs+zriD9m9y3\nzE4lrSLp9Fyt8BxwA7CGJOX9LoqIVwtJHimMbwQ8WpuIiFcKeYb043SppGclPUv6Me8C1i+RtpjH\ndUkB5fbCtq4i/SuspX8EuD7v88d1mwjgsbpj2KhuHSSNkvTtXDXzHOnHKKirhit4JiKWFaZfASaX\nyO9Kx57z06gN4wLgsDz+MeCyiHgt53mapOuUqhSfA47qJr+P0YNcxfLHXN2xiPSDU0zf7THmdcaT\n/kXXmwocUjv2vN3pNPgc0/P7MxX4Yt22Nmbl96/H4yOV2P/aw7KNSP/eu9svpGOPPF77DjxZWP4q\n6VzUFD/PQQoYG/HG97u2rymF6ScK47VzDOn4d6s7/o+RvkO9pR0yHDCa94YfjIi4n/RB+3CDdLWi\n6pWSFpC+HOOBfyi53y+S/rW+PSLWIFVx1PKzAFhT0iqF9TctjC8gfSFTgtSWsnZh+f8C++RAuFZE\nrBkRkyK10/SWtuhp0hdh+8K21oiI1Qvp9yX9i78W+K+69CruKx/D493s5+PAfqRSzhrAm3LaZhuk\ne8vvSsdO+lGI+o0UXAOsK2lH0j/m8wvLzidVfU3JeT69m/x2u+3cTvAL4GRg3YhYkxTYyhzv08Br\nwBbdLHuUVGorvu+rRsTJDbZXfz5q78+jpKq84rYmR8RFvR1fIX13eYT0gz61h/22ovh5FimwPZ6H\nTevW3TTvvzePAp11x79aRBzdh3wOOg4Y7fNF4DhJ/yBpVSW7S/pJXn4Eqb1jJ1L95o6kBvB9Ja1Z\nYvurkv4pvSBprbwtACLif0nF7hMkjZW0O+kHteYXwAclvSs3Fp7Iyj82pwPfkrQppJKCpP1Lpl0u\n/1s7A/h+/veOpCmS9s7j6+TlnyS1QXxQ0j51mzkul6a2B44ELuxmV5NJ1WKLJE0C/pMWepn0ll9S\nr7dPSNo2B8pv9LK9JcAlpLakNUkBpJjnRRHRJWka6d9nUXfntDZvXB6ejohl+Zzt3c36PR3jWcB3\nJW2YS2e75ffyXGA/SXvn+RNyI/IbSnUFn83naC3gq6x4f84A/r98bEiapNTQP6lMPoFfAxtI+mel\nRu7JtW3lfXxd0jr5M3QcUOqanB68TdKBSr21vkAKqDcDtwAvKzXOj5HUQerleEHJ/L9Z0t/ntGMl\n7Spp65J5egJwt9phpscfpYj4JfBR4FOkfyRPkH5cL5f0DtI/lR9HxJOF4UpSI9thAJL+JOmw7vfA\n90nVJ0+TGgB/U7f8Y6TGzGdIX6izC3m7j9Qz6wLSv6hnWLl64Aekxt7Zkp7P259WMm29Y0l1zjfn\nqpfZwJvzstNJDfVXR8SzpAbKM+oC5g05/TXAyRFxbTf7+DmpVDSf1PHgpgb56U7xffxKT/mNiN+S\nzvt1wAOkUlFvLiA1Hl9cV0X0T8C/5/P7dVKnhZ7ytNK8iHgJ+Gfgklxtdijp/WqkuL1/Be4F5pDe\nv2+T2jseAw4g/fA/Rap++Vca/y6cTzpHfyF9dr+Z83g78Bng1JzHB1i59NwwoOdjfB+wP+m78wDQ\nkRf/B+kP0T2kjiG31fbb0+Z6mb6c9F1dRCqtfigilkZEF+mP1gdI37NTSe05D/Z2DDn/e5Pem1pp\n5dukWoQyZgI/z9VZB5dM0+9qDUXV7UCaQfrSjQLOjIiT6pbvD/w7qdGnC/hCRPwhL3uY1HC4DOiK\niGnYsCRpKqmefWzdD60NEpIeAj4VEdcNdF5apdSdeYuI8HVQLRhT5caV+lqfSvrH9TgwR9LlETGv\nsNrvIuKKvP5bSdUA2+Zly4COiFhUZT5t0PBtMMwGsaqrpKaRut09kot7F5KKwMvlXjc1tS5xNeqH\nPNrgUW1x1/rK788IV2kJg9QdrdhN7TFy3XiRpANJDZfrsnI30wCukbSU1If5jArzagMod7f1LSMG\nsYgY9I2yvYmIEwY6D0PZoPj3HhGXRcS2wIGkBq6a6RGxC6kR6rO594+ZmQ2AqksY81m5X/PGNOjT\nHBE3Stpc0loR8Wy+DoCIeErSpaTSyY316SS5qGxm1qSIaKrdsOoSxhxgS6UbqY0jdTm7oriCpC0K\n47uQbq/xrKSJkibn+ZNIXdb+1NOOqr4kfqgMxx9//IDnYTAMPg8+Fz4XjYdWVFrCiIilko4m9duu\ndaudK+motDhmAR+WdATp3kWvku7JAumS+ktz6WEMcF5EzK4yv2Zm1rOqq6SIdPHT1nXzTi+Mn0y6\n5UF9uodIV0WbmdkgMCgava19Ojo6BjoLg4LPwwo+Fyv4XPRN5Vd69wdJMRyOw8ysv0giBlmjt5mZ\nDRMOGGZmVooDhpmZleKAYWZmpThgmJlZKQ4YZmZWigOGmZmV4oBhZmalOGCYmVkpDhhmZlbKsA0Y\nr70G83t88oaZmTVr2AaMO+6Agw8e6FyYmQ0fwzZgTJ4ML7000LkwMxs+HDDMzKyUYRswVl3VAcPM\nrJ2GbcCYPBlefHGgc2FmNnwM24AxYQJ0daXBzMz6btgGDCmVMl5+eaBzYmY2PAzbgAGpHcPVUmZm\n7TGsA4Z7SpmZtY8DhpmZlVJ5wJA0Q9I8SQ9IOrab5ftLulvSnZJulTS9bNreuGutmVn7VBowJI0C\nTgXeD2wPHCZpm7rVfhcRO0bEzsCngJ82kbYhd601M2ufqksY04AHI+KRiOgCLgQOKK4QEa8UJicD\ny8qm7Y2rpMzM2qfqgDEFeLQw/VietxJJB0qaC1wJfLKZtI04YJiZtc+gaPSOiMsiYlvgQOA/2rVd\nt2GYmbXPmIq3Px/YtDC9cZ7XrYi4UdLmktZqNu3MmTOXj3d0dNDR0eE2DDOzrLOzk87Ozj5tQxHR\nntx0t3FpNHA/sBewALgVOCwi5hbW2SIi/prHdwEuj4hNyqQtbCO6O46TT4annoJTTmn/sZmZDWWS\niAg1k6bSEkZELJV0NDCbVP11ZkTMlXRUWhyzgA9LOgJ4HXgVOKRR2mb2P3kyPPRQGw/IzGwEq7SE\n0V96KmGccw5cfTWce+4AZMrMbBBrpYQxKBq9q+JeUmZm7eOAYWZmpQzrgOFutWZm7TOsA4a71ZqZ\ntc+wDxguYZiZtYcDhpmZlTKsA4bbMMzM2mdYB4xx42DZMnj99YHOiZnZ0DesA4bkaikzs3YZ1gED\nHDDMzNpl2AcMt2OYmbXHsA8YvhbDzKw9eg0Ykt4s6VpJf8rTO0j6evVZaw9XSZmZtUeZEsYZwL8B\nXQARcQ9waJWZaicHDDOz9igTMCZGxK1185ZUkZkqrLqqq6TMzNqhTMB4WtIWQABIOpj0BLwhwSUM\nM7P2KPPEvc8Cs4BtJM0HHgL+vtJctZEDhplZe/QaMCLib8DfSZoEjIqIIVXB4261ZmbtUaaX1Lck\nrRERL0fEi5LWlPQf/ZG5dnC3WjOz9ijThrFPRDxXm4iIRcAHqstSe7lKysysPcoEjNGSxtcmJK0C\njG+w/qDigGFm1h5lGr3PA66VdFaePhI4u7ostZfbMMzM2qNMo/dJku4B9sqz/j0irq42W+3jNgwz\ns/YoU8IgIq4CrmplB5JmAN8nVX+dGREn1S3/GHBsnnwR+Kd8NTmSHgaeB5YBXRExrdn9u0rKzKw9\neg0Ykg4CTgLWA5SHiIjVSqQdBZxKKp08DsyRdHlEzCus9jfgPRHxfA4us4Dd8rJlQEduaG+JA4aZ\nWXuUKWGcDOwXEXNb2P404MGIeARA0oXAAcDygBERNxfWvxmYUpgWfbyjrtswzMzao8yP8cIWgwWk\nH/9HC9OPsXJAqPdpVq76CuAaSXMkfaaVDLgNw8ysPcqUMG6TdBFwGbC4NjMiftXOjEjak9QDa/fC\n7OkRsUDSuqTAMTcibuwu/cyZM5ePd3R00NHRAayokopIj2w1MxuJOjs76ezs7NM2FBGNV1jRnbYo\nIuKTvW5c2g2YGREz8vRXctr6hu8dgF8CMyLirz1s63jgxYj4bjfLotFxjB8PL7yQXs3MDCQREU39\njS7TrfbI1rPEHGBLSVNJd7g9FDisuIKkTUnB4vBisJA0kXTvqpfyfaz2Bk5oJRO1aikHDDOz1pXp\nJTUB+BSwPTChNr9MCSMilko6GpjNim61cyUdlRbHLOA4YC3gx5LEiu6z6wOXSoqcz/MiYnbTR8iK\naql11mkltZmZQbkqqUtIvZo+BpwIfByYGxGfrz575fRWJbX99nDRRfCWt/RjpszMBrFWqqTK9JLa\nMiKOA16OiLOBfYF3tJLBgeKutWZmfVcmYHTl1+ckvQVYnXQR35DhrrVmZn1XplvtLElrktoargAm\nA9+oNFdt5qu9zcz6rkwvqZ/m0RuAzavNTjUcMMzM+q7HgCHp7yPiXEnHdLe8u+shBiu3YZiZ9V2j\nEsak/Lpqf2SkSm7DMDPrux4DRkScLmk08EJEfK8f89R2rpIyM+u7hr2kImIpdVdmD0WukjIz67sy\nvaT+IOlU4CLg5drMiLijsly1mUsYZmZ9VyZg7JRfTyzMC+C97c9ONdyGYWbWd2W61e7ZHxmpkksY\nZmZ9V+qZ3pL25Y03Hzyx5xSDi9swzMz6rtdbg0j6CfBR4HOkR6Z+BJhacb7aylVSZmZ9V+ZeUu+K\niCOARRFxAvBO4M3VZqu9XCVlZtZ3ZQLGq/n1FUkbkW5GuGF1WWo/Bwwzs74r04bxa0lrAKcAd5B6\nSJ1Raa7azG0YZmZ91+sDlFZaWRoPTIiI56vLUvN6e4BSVxdMmABLloCaelyImdnwVMkDlCTdI+mr\nkraIiMWDLViUMXZsGl57baBzYmY2dJVpw9gPWAJcLGmOpH+VtGnF+Wo7V0uZmfVNrwEjIh6JiJMj\n4m2k53rvADxUec7azA3fZmZ9U/bCvamkazE+CiwFvlxlpqrgazHMzPqm14Ah6RZgLHAx8JGI+Fvl\nuaqASxhmZn1TpoRxRETcX3lOKuY2DDOzvinThtGnYCFphqR5kh6QdGw3yz8m6e483Chph7Jpm+ES\nhplZ35TpJdUySaOAU4H3k25eeJikbepW+xvwnojYEfgPYFYTaUtzG4aZWd9UGjCAacCDuadVF3Ah\ncEBxhYi4uXBtx83AlLJpm+EShplZ3/TYhiHpoEYJI+JXJbY/BXi0MP0YKRD05NPAVS2mbchtGGZm\nfdOo0Xu//Loe8C7gujy9J3ATUCZglCZpT+BIYPdW0s+cOXP5eEdHBx0dHSstd5WUmY1knZ2ddHZ2\n9mkbPQaMiDgSQNJsYLuIWJCnNwR+VnL784HiVeEb53kryQ3ds4AZEbGombQ1xYDRncmT4ZlnSuXZ\nzGzYqf8jfcIJJzS9jTJtGJvUgkW2kJV/yBuZA2wpaaqkccChwBXFFfJtRn4JHB4Rf20mbTNcJWVm\n1jdlrsO4VtLVwAV5+qPA78psPCKWSjoamE0KTmdGxFxJR6XFMQs4DlgL+LEkAV0RMa2ntE0dXYEb\nvc3M+qbU7c0lfQh4T578fURcWmmumtTb7c0Bfv1rOO00+H//r58yZWY2iLVye/NS95IiPTjpxYj4\nnaSJklaNiCHVhOwShplZ35R5HsZngF8Ap+dZU4DLqsxUFdyGYWbWN2UavT8LTAdeAIiIB0ldbYcU\nlzDMzPqmTMBYHBGv1yYkjSE913tI8XUYZmZ9UyZg3CDpq8Aqkt4HXAJcWW222s8lDDOzvum1l1S+\nCeCngL0BAVcDP+21W1I/KtNLaulSGDcOliwBNdUvwMxs+Gmll1SpbrWDXZmAATBxIjz9dHo1MxvJ\nKulWK2k6MBOYmtcX6aK7zVvJ5ECqtWM4YJiZNa/MdRhnAl8Abic9z3vIqnWtXX/9gc6JmdnQUyZg\nPB8RV/W+2uDnhm8zs9aVCRjXSzqFdDvzxbWZEXFHZbmqiLvWmpm1rkzAeEd+3bUwL4D3tj871XIJ\nw8ysdb0GjIjYsz8y0h98exAzs9Y1ekTr30fEuZKO6W55RHy3umxVwyUMM7PWNSphTMqvq/ZHRvqD\n2zDMzFrX6BGtp+fX5p/jN0i5hGFm1royF+5NIN0aZHtgQm1+RHyywnxVwm0YZmatK3PzwXOADYD3\nAzcAGwNDsmLHJQwzs9aVCRhbRsRxwMsRcTawLyu62g4pbsMwM2tdmYDRlV+fk/QWYHWG4AOUwFVS\nZmZ9UebCvVmS1gSOA64AJgPfqDRXFXGVlJlZ68pcuPfTPHoDMOTuUFvkKikzs9Y1unCv2wv2anzh\nnpnZyNKoDWPVXoZSJM2QNE/SA5KO7Wb51pJukvRafZCS9LCkuyXdKenWsvvsidswzMxa1+jCvT5f\nsJcf73oqsBfwODBH0uURMa+w2jPA54ADu9nEMqAjIhb1NS/gEoaZWV/02ktK0uaSrpT0lKQnJV0u\nqWxbxjTgwYh4JCK6gAuBA4orRMTTEXE7sKS73ZfJY1luwzAza12ZH+PzgYuBDYGNgEuAC0pufwrw\naGH6sTyvrACukTRH0meaSNetSZPglVdg2bK+bsnMbOQp0612YkScU5g+V9KXqspQnekRsUDSuqTA\nMTcibuxuxZkzZy4f7+jooKOj4w3rjBqVnuf9yiuptGFmNlJ0dnbS2dnZp20oIhqvIJ0ELCJVJwXw\nUWBN4BSAiHi2QdrdgJkRMSNPfyUliZO6Wfd44MWeel81Wi4pejuOmg02gLvuSq9mZiOVJCJCzaQp\nU8I4JL8eVTf/UFIAadSeMQfYUtJUYEFOc1iD9ZdnXtJEYFREvCRpErA30OeG+Fo7hgOGmVlzyly4\nt1mrG4+IpZKOBmaT2kvOjIi5ko5Ki2OWpPWB20hddZdJ+jywHbAucKmkyPk8LyJmt5qXGnetNTNr\nTZnbm/87qVppaZ5eDfhBRBxZZgcR8Vtg67p5pxfGFwKbdJP0JWCnMvtohrvWmpm1pkwvqTHArZJ2\nkPQ+UjXT7dVmqzoOGGZmrSlTJfVvkn4H3EJq/H5PRPyl8pxVxNdimJm1psyFe+8BfgicCHQCP5K0\nUcX5qozbMMzMWlOml9R/AR+JiPsAJB0EXAdsU2XGquIqKTOz1pQJGO+sNXgDRMSvJN1QYZ4q5Sop\nM7PWlGn0XkfSmZJ+CyBpO7q/UeCQ4BKGmVlrygSMnwFXk+4lBfAA8C9VZahqbsMwM2tNqRJGRFxM\nutU4EbEEWNo4yeDlEoaZWWvKBIyXJa1Nug1I7f5Qz1eaqwq5DcPMrDVlGr2PAa4AtpD0B9ItOw6u\nNFcVcpWUmVlryly4d4ekPUi39xBwf34Y0pDkKikzs9aUKWHU2i3+XHFe+oUDhplZa9r2+NOhwm0Y\nZmatGXEBw20YZmat6bFKStIujRJGxB3tz071XCVlZtaaHh/RKun6PDoB2BW4m9TovQNwW0S8s19y\nWEIzj2hdtgzGjoXXX4fRoyvOmJnZINXKI1p7rJKKiD0jYk/So1V3iYhdI+JtwM7A/L5ldeCMGgUT\nJ8LLLw90TszMhpYybRhbR8S9tYmI+BOwbXVZqp7bMczMmlemW+09kn4KnJunPw7cU12Wqud2DDOz\n5pUJGEcC/wh8Pk//Hjitshz1A3etNTNrXpkrvV+T9BPgNxFxfz/kqXKukjIza16ZR7TuD9wF1J6H\nsZOkK6rOWJVcJWVm1rwyjd7HA9OA5wAi4i5gsyozVTUHDDOz5pUJGF0RUX8783IXPQCSZkiaJ+kB\nScd2s3xrSTdJek3SMc2kbZXbMMzMmlcmYPxZ0seA0ZK2kvQj4KYyG5c0CjgVeD+wPXCYpG3qVnsG\n+BxwSgtpW+I2DDOz5pUJGJ8j/WAvBi4AXqD8I1qnAQ9GxCP5lugXAgcUV4iIpyPidmBJs2lbtcEG\n8Nhj7diSmdnI0WvAiIhXIuJrEfH2fLX31yLitZLbnwI8Wph+LM+rOm1DO+wAd9/dji2ZmY0cjW4+\neCUN2ioiYv9KctSimTNnLh/v6Oigo6Ojx3V33DEFjAhQU3dSMTMbmjo7O+ns7OzTNhpdh/Ff+fUg\nYANWXOl9GLCw5PbnA5sWpjem/H2omkpbDBi92WijdBPCJ56ADTcsnczMbMiq/yN9wgknNL2NHgNG\nRNwAIOk7EbFrYdGVkm4ruf05wJaSppJuYngoKeD0pPh/v9m0pUkrShkOGGZm5ZRp9J4kafPahKTN\ngEllNh4RS4GjgdmkR7xeGBFzJR0l6f/k7a0v6VHgC8DXJP2vpMk9pW3m4BqpBQwzMyunx+dhLF9B\nmgHMAv5GKgFMBY6KiKurz145zTwPo+bss+Hqq+H88yvKlJnZINbK8zB6DRh5w+OB2jUQ8yJicQv5\nq0wrAeOuu+DjH4c//7miTJmZDWJtDRiSDmqUMCJ+1cyOqtRKwFi8GNZYAxYtggkTKsqYmdkg1UrA\naNRLar/8uh7wLuBaUpXUnqQrvQdNwGjF+PGw1VaphPG2tw10bszMBr9Gj2g9MiKOBMYC20XEwRHx\nYdJV32P7K4NV2mEHuGdIPwrKzKz/lOkltUlELChML2Tl6yOGLPeUMjMrr8wT966VdDXpPlKQrof4\nXXVZ6j877gi/+c1A58LMbGgo20vqQ8B78uTvI+LSSnPVpFYavQEWLoRtt4VnnvEtQsxsZKmsW23d\nTt4NHBoRn20qYYVaDRiQ7lw7Zw5sskmbM2VmNoi1EjDKtGEgaWdJJ0t6GDgRmNdC/gYlt2OYmZXT\nY8CQ9GZJx0uaB/yIdKtxRcSeEfGjfsthxRwwzMzKaVTCmAe8F/hgROyeg8TS/slW/3HAMDMrp1HA\nOIh0l9jrJZ0haS9WvpvssOCAYWZWTpmbD04iPRr1MFKJ4+fApRExu/rsldOXRu+uLlh9dXjqKZhU\n6h68ZmZDXyWN3hHxckScHxH7kR5idCdwbIt5HHTGjoVttoE//Wmgc2JmNriV6iVVExGLImJWROxV\nVYYGgqulzMx611TAGK4cMMzMeueAgQOGmVkZTV/pPRj1pdEb0q1BNtsMnnsORjmEmtkIUNmV3sPd\n2mvDaqvBww8PdE7MzAYvB4zM1VJmZo05YGQOGGZmjTlgZDvu6KfvmZk14oCRuYRhZtZY5QFD0gxJ\n8yQ9IKkufldwAAANpUlEQVTbK8Ql/VDSg5LukrRzYf7Dku6WdKekW6vM51ZbwRNPwAsvVLkXM7Oh\nq9KAIWkUcCrwfmB74DBJ29Stsw+wRURsBRwFnFZYvAzoiIidI2JalXkdPRq23x7uvbfKvZiZDV1V\nlzCmAQ9GxCMR0QVcSLqRYdEBpBsaEhG3AKtLWj8vUz/kcTlXS5mZ9azqH+MppAcv1TyW5zVaZ35h\nnQCukTRH0mcqy2XmgGFm1rMxA52BXkyPiAWS1iUFjrkRcWN3K86cOXP5eEdHBx0dHU3vbMcd4dxz\nW8ypmdkg1tnZSWdnZ5+2UemtQSTtBsyMiBl5+itARMRJhXV+AlwfERfl6XnAHhGxsG5bxwMvRsR3\nu9lPn24NUvP88zBlSnodPbrPmzMzG7QG461B5gBbSpoqaRxwKHBF3TpXAEfA8gDzXEQslDRR0uQ8\nfxKwN1DpUytWXx022ABuuKHKvZiZDU2VVklFxFJJRwOzScHpzIiYK+motDhmRcRvJH1A0l+Al4Ej\nc/L1gUslRc7nef3xlL8f/hAOPxxuvTWVNszMLPHdarvxn/8Jl12WShoTJrRts2Zmg0YrVVIOGN2I\ngI9+ND3j+7//G9TUKTUzG/wGYxvGkCTBWWfB7bfDqacOdG7MzAYHlzAaeOgheOc74YILYM892755\nM7MB4xJGm222GZx3Hhx2mB+uZGbmgNGLvfaCY4+FD30IXnlloHNjZjZwXCVVQgQccQQsWgQnnZRu\nUmhmNpS5SqoiEsyalW4dsvfesNtucMYZvhW6mY0sLmE0ackS+O1vU3fb666DAw+ET30Kdt/d3W/N\nbOjwdRj9bOFCOOccOPNMeP11OOSQNOy0k4OHmQ1uDhgDJALuvBMuuQQuvhhGjVoRPHbYwcHDzAYf\nB4xBIALuuCMFjosvhvHj4Zhj4JOfhDGD/WbyZjZiOGAMMhFw003wjW/A/PnwzW/CQQe5xGFmA88B\nY5CKgGuuSddzjBuXuua28HwnM7O2ccAY5JYtg4sugq99DbbZBr71rdRAbmbW33wdxiA3alS6zci8\nebDPPvCBD6RrOk47DZ59dqBzZ2bWmEsYA2jJklRVdfbZcNVV8Hd/l64o32efVHVlZlYVV0kNYc89\nB7/4RQoe998PM2bA1lvDm98MW20FW24JkycPdC7NbLhwwBgm/vpX6OyEBx+EBx5Ir3/9K6y5Zgoe\nu+6aGs3f/e70HHIzs2Y5YAxjy5bBY4+lAHLzzSmg3HJLajzv6EjD9OkpgLjbrpn1xgFjhFm8GObM\nScGjsxP++Mc0b+JEWGWVlV9XXRXWWw/WX/+NrxtsABtt5HYTs5HEAcPo6oJXX03P7qi9vvJKurPu\nk0+mYeHCFa8LF8KCBel1zTVhypSVh402SgFlww3TsN56MHr0QB+lmfWVA4a1bOnSFETmz09VX/Pn\np2HBgjQ88UR6ffZZWGedVDJZe21Ya62Vh7XXhjXWgNVWS9Vjq622YnyVVVxdZjZYDMqAIWkG8H3S\nNR9nRsRJ3azzQ2Af4GXgExFxV9m0eT0HjH7S1bWidPLssysPzzyThuefTyWa2mttvKsrVY9NmNDc\nMG5cKtWMGbPitTY+YULaZv2wyior1hk9Ol0DUxsfO3bl7Y8Z40BmI8+gCxiSRgEPAHsBjwNzgEMj\nYl5hnX2AoyNiX0nvAH4QEbuVSVvYhgNG1tnZSccgve/I66/Da691P7z6amp/6W7Z4sWpBLR0abp2\nZcmSNN7VlZbVqt2Kw8KFnUyc2MHSpanDQC19MV1t+8uWrRxAxo9/42ttGDduxWtxGDs2DWPGrBiv\nTdeCVf3r+PEpsK2yStpPbXz8+DduozZeS99MgBvMn4n+5nOxQisBo+r7p04DHoyIRwAkXQgcABR/\n9A8Afg4QEbdIWl3S+sBmJdJancH8haj9uK62WvX7mjmzk5kzO0qtu2TJigBSDCT1AawWaF5/PQ3F\n8a6uNCxZsmLdri5WClj1r4sXp0BZHIr7qm2vOL5kSbo3WbHEVAxE0opgUht/9dVO1lmno9vOEOPH\nr9hOfUmsuD1p5enu1q2Nd7f+qFErr9PdUCwRFkuTtbTdDfX7Kr7WD6NHwy9/2ckmm3R0m6b+3PX0\nWr9ed3kfNUzvoVF1wJgCPFqYfowURHpbZ0rJtGZ9VqvimjRpoHNSTsTKJaZiIKoVtCNWDCedBEcf\n3X1niGLprb4kVttecajNK65bHF+8uOf167ff3VArPRZfi+mLw9Klb9xPd/ssnp+nn4bf/GbldYqv\ntXPX02ttKE53l3dYEUgbBbGeAlt3r/WBrX6Axsvrg38rBuMTGlybbNaAtCLIlTF5MrzpTZVmaciY\nOTMNVSsGtFrQKgbaRkGw0Wt9AO8ugJUdOjubP66q2zB2A2ZGxIw8/RUgio3Xkn4CXB8RF+XpecAe\npCqphmkL23ADhplZkwZbG8YcYEtJU4EFwKHAYXXrXAF8FrgoB5jnImKhpKdLpAWaP2gzM2tepQEj\nIpZKOhqYzYqusXMlHZUWx6yI+I2kD0j6C6lb7ZGN0laZXzMz69mwuHDPzMyqN6Q7f0maIWmepAck\nHTvQ+elPks6UtFDSPYV5a0qaLel+SVdLGhH3spW0saTrJP1Z0r2S/jnPH3HnQ9J4SbdIujOfi+Pz\n/BF3LiBdCybpDklX5OkReR4AJD0s6e782bg1z2vqfAzZgJEv7DsVeD+wPXCYpG0GNlf96izSsRd9\nBfhdRGwNXAf8W7/namAsAY6JiO2BdwKfzZ+FEXc+ImIxsGdE7AzsBOwjaRoj8FxknwfuK0yP1PMA\nsAzoiIidI6J2iUJT52PIBgwKFwVGRBdQu7BvRIiIG4FFdbMPAM7O42cDB/ZrpgZIRDxRu51MRLwE\nzAU2ZuSej1fy6HhSO2UwAs+FpI2BDwA/LcweceehQLzxN7+p8zGUA0ZPF/yNZOtFxEJIP6LAegOc\nn34n6U2kf9Y3A+uPxPORq2HuBJ4AromIOYzMc/E94EukgFkzEs9DTQDXSJoj6dN5XlPnYzBeuGft\nM6J6NEiaDPwC+HxEvNTN9Tkj4nxExDJgZ0mrAZdK2p43HvuwPheS9gUWRsRdkjoarDqsz0Od6RGx\nQNK6wGxJ99Pk52IolzDmA5sWpjfO80ayhfk+XEjaAHhygPPTbySNIQWLcyLi8jx7xJ4PgIh4AegE\nZjDyzsV0YH9JfwMuAN4r6RzgiRF2HpaLiAX59SngMlK1flOfi6EcMJZfFChpHOnCvisGOE/9Tax8\nK5UrgE/k8X8ALq9PMIz9N3BfRPygMG/EnQ9J69R6ukhaBXgfqU1nRJ2LiPhqRGwaEZuTfhuui4jD\ngSsZQeehRtLEXAJH0iRgb+BemvxcDOnrMPLzMn7Aigv7vj3AWeo3ks4HOoC1gYXA8aR/DZcAmwCP\nAIdExHMDlcf+Imk68HvSFyDy8FXgVuBiRtD5kPRWUuPlqDxcFBHflLQWI+xc1EjaA/hiROw/Us+D\npM2AS0nfjTHAeRHx7WbPx5AOGGZm1n+GcpWUmZn1IwcMMzMrxQHDzMxKccAwM7NSHDDMzKwUBwwz\nMyvFAcPMzEpxwLBhRdIySacUpr8o6RsDmaf+JGlHSfsMdD5seHLAsOFmMXBQvoJ1JNqJdEtvs7Zz\nwLDhZgkwCzimzMqS1pP0K0l35SeR7ZbnH5OfWHePpM/neVMlzZV0Vn5C2bmS9pJ0Y57eNa93vKSf\nS7opz/90YX+n5O3eLemQPG8PSddLuiRv/5zC+rtI6sy3pL6qcKO46yV9Oz9db56k6ZLGAicCh+Sn\nzH1E0nvycd0h6fZ8HyGzlvj25jbcBPB/gXslnVRi/R8CnRFxkCQBkyXtQroR29uB0cAtkjqB54At\ngA9HxH2SbgMOi4jdJe0PfA34UN7uW4F3AKsCd0r6NfAuYIeIeKuk9YA5km7I6+8EbEd6hsUfJL2L\ndC+sHwH7R8QzOcB8C/hUTjM6It6Rq6BmRsT7cvXb2yKi9pjaK4B/iog/SpoIvNbk+TRbzgHDhp38\nLIyzSY/nfLWX1d8LHJ7TBfCipN2BSyPiNQBJvwLeTbrT6UMRUXvk55+Ba/P4vcDUwnYvj4jXgWck\nXUcKHruTbrVNRDyZg9DbgReBW2u3n5Z0F/Am4HngLaSH3tSelvZ4YR+/yq+31+276A/A9ySdB/wq\nIkb6IwCsDxwwbLj6AXAH6bbnjTR7983FhfFlhellrPx9Km5XeXm94q3pi9tdmrcl4E8RMb2XvNTW\nf4OIOCmXbvYllVz2jogHetieWUNuw7DhRgARsYh02+ZPN16da4F/guWPNl0N+B/gQEkTcp3/h/K8\n5dsv4QBJ4yStDexBen7L/wAfzftZl1RqubXBNu4H1i20q4yRtF0P69by9SKw2vKZ0uYR8eeIODnn\nYZuS+Td7AwcMG26K/+y/Q3peSKNSxL8Ae0q6B7gN2DYi7gR+RvqB/SMwKyLu7mb7jbZ7D+lpdzcB\nJ0bEExFxaZ5/N/A74EsR0d0TzgIgIrqAg4GTcjXVncA7e9h3bfp6YLtaozfwL7mR/S7gdeCqBnk2\na8jPwzBrM0nHAy9GxHcHOi9m7eQShpmZleISho0Ikr4KfIRUdaP8eklE/OeAZsxsCHHAMDOzUlwl\nZWZmpThgmJlZKQ4YZmZWigOGmZmV4oBhZmal/P9iPhen8oiwmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c98c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEaCAYAAAAVJPDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFdW19/HvAhUHwBEnJgdUHFFUEudWETEm4qx41ZtE\nb0iMNxpjosYYiTcmDtE3McYBNXGKEBUVJ+IQbUVFQQEZBCEEUBFHVEQRsHu9f+zddPWhh+ruU32G\n/n2ep56uqlPDOtXn1Dq1d+1d5u6IiIjU6FDoAEREpLgoMYiISB1KDCIiUocSg4iI1KHEICIidSgx\niIhIHUoMsoqZDTOzp9p4n53MrNrMtkyx7HZmtjijOH5vZiOy2HY9+/qbmZ3fFvsSaQklhhTMbL6Z\nfWlmS8xsUfxir5t4/XAzey6+/r6ZPWtm38nZRkU8Af68Gfs9xcw+j9v90syq4vjnZrYkxfojzeyX\nzXu3FKJhS6p9uvscd98o62Cy5u7fc/c/FDoOyV5b/uDIJyWGdBw40t27Av2BvYBfAZjZ8cC9wO1A\nd3ffDPg18O2cbZwOfBz/ptup+z3u3iXu9whgobt3TcwrF1boANqKmZX1d87MOhY6BskDd9fQxADM\nAw5JTF8FPBzHFwDnNbH+usAS4ETgK6B/C2I4CHirnvm7As8DnwBTgMFx/v8CK4Blcd//iPMvAf4T\n500FvpXY1jDgyUZiOAB4Oe7rVWDfOH8TYBEwME6vD8wHjo/TI4HrgGfifp8CtoyvdQKqE9NHx/fx\nWdzGRYn97wCsTEyPJyTh8XH5R4H1m4o3vrYt8EJc7zHgJmBEA+97bs7/v1PcZl+gI3A/8B6wGPgX\nsH1i2ZHAn4AngM+BfeO8XyaO3ePAB8BHwEPA5s14jxXxtU/j8To5zl8b+CPwFvBujGHNBt7fsBj3\nTXEf04EDEq9vCNwR/8cLgF/Xs+718f3/sp7tdwQujcfxU+AVYNPE5/q1eDxfAvbKee+XxuU/j8d5\nI+AfMc6X6vkc/ZjwfX0f+G1iWx2A38T4FwG3AuslP1fAd4G347rn56x7SYz/A+AuoGtT6wJDgOVx\nWAK8XOhzWerzTaEDKIWBRGIAesYvzvD4oagGejex/mnAQsIv44eBP+W8/nrNF7qRbayWGOKXYQFw\nbvzyDYpfoN7x9VUnoMQ6JyS+lP8VP7AbxekGEwPQm3DiOjhOD45fkvXj9LfjF2PD+MW5I7HuSMJJ\nYwCwFnAj8FTiPSQTw8HAjnF897jPQXF6B2BFYrvjgZnAVsA6wIvEk1ac11i8k4DfAmsAhwBf0HBi\n+C1wS2L6WGBSHO8InBr3vxbwF2B8znv/iHjCi8skE8OmwHfi/C7Ag8A9Kd9jn/j/Pppw8toY2DW+\ndiPhSrZLHMYClzTw/oYRTm7D4vs5jXB12zm+/jghyXQCNiOcyE9LrLsC+D7h892pnu1fEtfZOk73\nA7oC3Qgn+ONi/P8d/0ddEu99BuE7twEwG3gD2D8uPwr4S87naGx8v70JJ/JT4utnxW31ADoDj9T8\nv6n9Hv85/h/2IpzMt4qvXwA8F9/7WsBfgb+mXPf3NPC5Kuah4AGUwkBIDEsIJ7d58UPQifDrrwpY\nq4n1nwKuieMnE35VdGxmDPUlhoHAvJx5DwC/iOOrJYZ6tjsTOCyON5YYfg3cnDOvEjghMX0z4Spk\nXs2XOxHHXxPTG8Yv08bkJIZ69nsjcHkcry8xnJeY/inwQFPxAtsBXyb/b8Dohr7AwM7xf79GnL6f\nxC/KnGU3B76u2XZ87zflLNPg/wX4JqHIMM17HA78vZ5tdIwnpy0S8yqANxrY5zBgbs681wkn7F7A\n0uTnlfDr+PHEurOa+IzNJ15N5sw/E6jMmTcJODHx3n+aeO16YHRi+njgpThe8zlKXun8FHgkjr8A\nfDfx2m7AF4nPVRWwYc77PyqO/wfYJ/Ha1s1YtyQTwxpIWkPc/dnkDDP7OI5uQfjlvhoz60H4FXxh\nnPUwMAI4Mo63xpaEooKkBUD3hlYwszOAnxB+hRmwHqE4oym9gaFmdkLNpgi/tpN3E90C/A/hl+nn\nOeu/XTPi7p+Y2dK47uyc+PYDLgd2IvwCW4twBdKQ9xLjXxJ+DTYV7wfAh+6+IrHugsS6dbj7DDN7\nCzjCzCoJ9T3nxHg7EooWjyYkOo/72phQZFHnvecys86EYp6BhCI4IxQDpXmPPQm/inNtCawJzDBb\nVX3TgfDLviHv5EwviNvpHeP5MG7L4jAnsWyD7y/qTji51hdn7vcm9/P7fmJ8WT3Tuf+z5PuoeQ/1\n7WsBsLaZbRinq9z9k8Trucf5cTPzOB0OhNlGKdYtSWVdEZZnq1WQuvubhC/FcY2sd3pc9xEzW0T4\nInciXDa31ruEX3RJvQjFVpBzt4+ZbUco6z/T3Tdy9w1jPGkqf98mFKdsVLOuh0rwP8Vtr0Eoo/4b\ncK6Z9cxZf9V0/EKtF+PP9Q/CL+ru7r4BoWy7JZXTjcW7CNjEzNZKLJ97HHONAk4h/K8nuHvNMf4e\ncChwUIy3b5yfjLnO/yHHRYQT4Z5x/UGkf79vE4qTci0iFA1tm3j/G7j7po1sq0fOdC/C/+dt4POc\n47iBu++dWLax9wfhZL1tPfPfJRSR5e534eqLppb83NW8h5p99U681htYlnNCb8g7hKLk5DFYz93T\n3Drd1LEpSkoMrfcz4BIz+28z62LB/mZ2U3z9dMIl/+6EstV+hEvgIxO/VlpqHNDBzH5iZh3N7DDg\nMMLJFcKvq20Sy3cmXPZ+ZGZrmNkPqf/EUp87gBPM7BAz62Bm68TxmpPNb4DP3P0M4Abgzpz1h5jZ\n3mbWiVBm/6y7f8zq1gMWu/tKM9uXUPSTlPak2WC87j4beJPwf1vTzA4m1EE0ZiShHuVM4J7E/C6E\nGwo+ib/+L08ZX43OhF+YS8xsE+LdbindRfgcDYn//03MbFd3/5pQDn6dmW0MYGY9zWxgI9vqaWY/\niNs5lZAonnT3+cDLZnaVmXWOn+8+8courduA35nZ1jGW3c2sK+GKeXczOzbu93TCiX1sM7ad6wIz\n62pmWwFnExI6hP/f+fE4dAH+D/h7Yr3GPlc3A1fGq3/MbFMzS9512Ni67xOKnkqKEkM6DWZ9dx8N\nnAScQfil8x5wGTDGzL5B+NVyg7t/kBgeIVyKDwUws+lmNrTZQbkvJ5ysTiBUFv6BUD5bc8k8Ahhg\nZovN7B53n0z4kL8WY+0NTEy5r3mEX8u/IVSmziMUSXUws30IZc01V0H/B6xnZj9NbOJu4ErgQ0K5\nbPKKKXl8fwhcY2afAecTKlBpYNnG/i8NxhsXOZFQ6fxx3E9jxVXEY/o6sCdwX+Kl2+L234uvP99I\nvPXN+wOhEvbjuO5jKdaviWku4c6Xiwl1IBMJRXAQytffBV41s08JFcj1/Wqv8TywR9zORcAxieLA\noYTK31kxzlGESvO0riC8r2fi//VGQiX1B8BRhGT4EaGC+MjEflvya/sxwv9hAuFOvJqT/42E+reX\nCN+9jwg/6mrk7is5fRWhnrAm/hcIxyrNuqMI34XFZvZC899OYVisIMluB2aDCXc0dABuc/crc17v\nSjhp9CJUml3j7rdnGpS0KTMbCUxz998VOhZZnZkNA45z90GFjqWl4pXoMqCHu9dXRCnNkOkVQ2zM\ncz1wOOHOjqFm1jdnsR8DM9x9d0Il7TWxvFpERAog66KkAcAcd1/g7isJl1VDcpZxQjkt8e/HsYxU\nykdJVsBJydHnLE+y/mXenbq3sr1DSBZJ1wMPm9m7hIq4kzKOSdqYu59S6BikYe5+M6HuqWTF+jZ1\nx5EnxVD5fDgw2d23JFTo/CXe3SEiIgWQ9RXDQureH96D1e9R/h6hdSDuPtfM5hHuBX81uVCicYmI\niDSDuzerLVDWVwwTgT5m1js2JjqZ1Vv7LiC0+sTMNgO2p/5WkgVvJl4sw6WXXlrwGIpl0LHQsdCx\naHxoiUyvGNy9yszOBp6k9nbVmfH2OHf3EYTGTreb2dS42i88XYtCERHJQOa3hbr7PwkNmpLzbk6M\nLyLUM4iISJ589hm8lduTWkpqL1CCKioqCh1C0dCxqKVjUavcj0V1Nbz3HixYAPPnhwSwYEHdv1VV\n0Lt3k5uqV+Ytn/PFzLxUYhURaY2vv4aFC2tP/PPn144vWADvvAMbbBBO/MmhV68w9O4dXjcDM8Ob\nWfmsxCAi0saqquDdd2HevHCyr/lbM7z7LnTrBlttFYaaE3/NeK9esM466falxCAiUiSWLIH//Afm\nzg1/k8Nbb8Emm4QT/dZb1/7t3Tv87dkT1lqrqT2ko8QgItJG3OGjj+Df/w7D3Ll1/y5bBttuC9ts\ns/rQuzesnfs4powoMYiI5NnixTB7NsyZs/rQoQNstx306ROSQJ8+tcOmm4Yy/kJTYhARaYGVK0M5\n/6xZ8OabYagZX74ctt8+JIDcYaONiuPk3xglBhGRRnz6aTjh1ww1CWDePNhyS+jbF3bYoXbo2xc2\n37z4T/6NUWIQkXavujpU7iYTQM3wxRe1J/++fWuHPn3arsy/rSkxiEi7sWJFKOefObPuMHs2bLgh\n7Lhj3ZN/377hqqCUf/23hBKDiJSd5cvDyX7GDHjjjdq/8+aF+/l33DEMO+1Umwy6dGl6u+2FEoOI\nlKyVK8MVwIwZMH167d/588O9/TvtBDvvXPt3++2hU6dCR138lBhEpOi5h24dpk0Lw/TpYZgzB3r0\ngF12CSf+mr9KAK2jxCAiReXzz2HqVHj99TDUJIIuXWDXXWuHXXYJRUDrrlvoiMuPEoOIFIR76Nht\n8mSYMqU2ESxaFH719+sXhppEsNFGhY64/VBiEJHMVVeHLh8mT4ZJk2r/duwIe+wBu+8ehn79QiOw\nNdS5f0EpMYhIXlVVhfv/J00Kw2uvhSuCjTcOSWCPPaB///B3iy3a362gpUCJQURarKoqtAR+9dXa\nYerUcMLfc8+QAPbcMyQBFQWVDiUGEUnFPRQHJZPA5Mmh47e99oK9965NAuuvX+hopTWUGESkXgsX\nwsSJYZgwISSCLl1CAth775AM+vfXlUA5UmIQEZYuDSf+V16pHZYvDwlgwIDaZLDZZoWOVNqCEoNI\nO+MeuoZ4/nl46aWQBP79b9htN/jGN2qHrbdWxXB7pcQgUuaqq0M/QePGhWTw/PNh/oEHwn77hSTQ\nr1/+HgsppU+JQaTMfP11aCj23HMhCYwbF3oOPfDAMBxwQHhUpK4GpCFKDCIlbuXKUD9QkwhefDH0\nH3TggXDQQSERdO9e6CillCgxiJSYlStDo7HKSnj2WRg/PlwBHHRQbSLo1q3QUUopU2IQKXLV1aHl\n8NNPh0Tw4oshEVRUhOHAA3XLqOSXEoNIkXGHuXNDIvjXv+CZZ0IjsoED4eCDw1XBxhsXOkopZ0oM\nIkVg6dKQBB5/HJ54IhQXHXpoSAaHHqo6AmlbSgwiBeAe+hgaOzYkg5dfDreNfutbMHhweNyk7hqS\nQlFiEGkjX30V7hx67LEwrFwJRxwRksEhh+iZw1I8WpIY1FO6SEoLF4YrgkcfDXcR7borfPvb8NBD\n4QlkuiqQcqErBpEGuIfGZWPGwMMPh4fSH344HHlkKCJSpbGUAhUlibTSypWhYVlNMujYEYYMCcN+\n++lpZFJ6VJQk0gLLloW7h0aPDvUFffqERPDoo+F5xSoikvZGVwzSLi1dGpLA6NHw5JPhWQTHHQdH\nH63bSaW8FGVRkpkNBv4IdABuc/crc14/H/gvwIE1gR2BTdz905zllBikVb74Ah55BEaNCg3N9t23\nNhmo2wkpV0WXGMysAzAbOBR4F5gInOzusxpY/tvAue4+sJ7XlBik2ZYvh3/+E0aODO0M9t0XTjop\nFBVtuGGhoxPJXjHWMQwA5rj7AgAzGwUMAepNDMBQYGTGMUmZq6oK/RDdc0+4lXTXXWHoUPjzn3Vl\nIJJG1omhO/B2YvodQrJYjZmtAwwGfpxxTFKmZsyAu+6Cu+8O/RGdeipcdlnotlpE0iumu5K+A7yQ\nW7eQNHz48FXjFRUVVFRUZB+VFLUPPwzFRHfeCYsWhWTwz3+GBmci7VFlZSWVlZWt2kbWdQzfBIa7\n++A4fSHguRXQ8bUHgHvdfVQD21IdgwCwYkW4o+j220O3FN/5Dpx+euiKomPHQkcnUlyKsfK5I/Am\nofJ5ETABGOruM3OWWx/4D9DD3Zc1sC0lhnZu8uSQDEaODB3Tffe7cPzx6pdIpDFFV/ns7lVmdjbw\nJLW3q840s2HhZR8RFz0aeKKhpCDt10cfhTqD22+HTz8NyeDll8PDbUQkG2rgJkXHHV54AW66KRQZ\nfec78P3vh4fadOhQ6OhESkvRFSXlkxJD+fvkk1CJfPPNITkMGxbqDvSoS5GWK7qiJJGmuMMrr4Sr\ng4ceCs8zuOkmOOAA9VEkUihNXjHE9gXnAr3d/Ydm1gfYzt3HtkWAiTh0xVBGvvgiVCLfcAN89hn8\n8Ifwve/BJpsUOjKR8pJJUZKZjQSmAae4+y5mti7worvv0fJQm0+JoTzMnBmuCO6+G/bfH846Cw47\nTHUHIllpSWJI83Xczt1/B6wEcPcvAV3kS2rV1aHzukMPhYMPDreXTp4cnnlw+OFKCiLFJk0dwwoz\nW5vQ+ylmtjWwItOopCwsWQJ/+1voo2jDDeGcc+DEE2GttQodmYg0Jk1iuAz4J9DDzO4ADgLOyDQq\nKWlz54ZkcOedoZjozjthn31UmSxSKlLdrmpm3YB9CUVIL7n7B1kHVk8MqmMocuPHw9VXw7hxcOaZ\nof6gZ89CRyXSvmVV+XwU8Jy7fxanNwD2d/dHWxxpCygxFKfq6vBs5Kuvhvfeg/POC62T11uv0JGJ\nCGSXGKa4++458ybrrqT2bdmyUER0zTWwwQbw85/DsceqEzuRYpNVA7f6NqiGce3UF1/AjTeGhLDn\nnnDLLXDggao/ECknaU7wk83sKuAvcfpsYHJ2IUkxWro0NEa79trQKvmJJ2C33QodlYhkIc0d5GfH\n5cbEAeCszCKSovL55/D738O228KkSfD003DffUoKIuWsySsGd18KnN8GsUgRWbYMrrsuFBkNHBie\nobzTToWOSkTaQpOJIfaNdB6wVXJ5dx+UXVhSKFVVoVL517+Gb3wDnn8e+vYtdFQi0pbS1DHcD9wG\n3A1UZRuOFIo7jB0LF1wQ7jK6997QKE1E2p80iaHa3f+ceSRSMBMnwi9+EdohXHlleDCO7jISab/S\nVD6PMbMfmFk3M+taM2QemWTuww9DV9dHHw2nnALTpsFRRykpiLR3aa4Yzox/L0nMc6BX/sORtlBd\nDbfeCr/6FZx2GsyaFXo8FRGBdHclqbebMjJlCvzoR+Gq4KmnoF+/QkckIsUmVQtmM+sL7ASsXTPP\n3e/JKijJvyVLwp1GI0fC5ZfD97+v5yCISP3S3K76K2AQ0Bd4AjgceAFQYigRTzwRejs97DCYMUOP\nzxSRxqW5YjgJ2B2Y5O6nmdkWwO2ZRiV58cUXcP758PjjcPvt4QlqIiJNSVOYsMzdq4CvzawL8B7Q\nO9uwpLVeeinUHyxbBlOnKimISHppO9HbAPgr8CqwBJiQaVTSYsuXw/Dh4QrhhhvgmGMKHZGIlJpU\nT3BbtXDoHqOru0/KLqQG963nMTRh2jQ49VTYemsYMQI23bTQEYlIobXkeQwNFiWZ2Xbx7241A7Au\noUhJfWsWEXe4+WY45BA491x48EElBRFpucaKki4EzqD2OQxJDhyYSUTSLEuWwA9+AG+8AS+8ADvs\nUOiIRKTUNVqUZGYdgAHu/nLbhdRgLCpKyjF5Mpx4YrhS+OMfYZ11Ch2RiBSbvBYlAbh7NXBTq6KS\nvHMPj9ccNAguuywUIykpiEi+pLkr6VkzG+LuY5peVLK2ZAn8z//Am2/Ciy/C9tsXOiIRKTdp2jF8\nF3jQzJaZ2WIz+8TMFmccl9Rj+nTYa6/wvITx45UURCQbaa4Y1IFCEfj738MdR9dcA6efXuhoRKSc\npeldtcrM1ge2JdGJHvBSZlHJKsuXw89+Fvo7evpp9YYqItlL04neGYRnPncHpgF7Ay8DFZlGJrz9\nNpxwAmy+eXjK2gYbFDoiEWkP0tQxnAvsBcx39wOAPYGP0+7AzAab2Swzm21mFzSwTIWZTTaz6Wb2\nbNptl7Onn4a99w5dWjzwgJKCiLSdNHUMX7n7MjPDzNZy9xlmlqoZVWwHcT1wKPAuMNHMxrj7rMQy\n6xMa0Q1y94Vm1u7rNO64Ay68EO65J7RREBFpS2kSw6LYid4jwBPxjqR3Um5/ADDH3RcAmNkoYAgw\nK7HMKcBod18I4O4fpQ2+HN1wA1xxBTz7LPTtW+hoRKQ9SlP5fFQcvcTMDgXWBx5Luf3uwNuJ6XcI\nySJpe2DNWITUGbjO3e9Kuf2ycuWVofO7554LHeGJiBRCmsrna4FR7j7B3f+VUQz9gUOA9YDxZjbe\n3f+dwb6KkjtcckmoS3j+eejevdARiUh7lqYoaQbwWzPbGhhNSBJTUm5/IdArMd0jzkt6B/jI3b8C\nvjKz54F+wGqJYfjw4avGKyoqqKioSBlG8aquhp/+FMaNC1cK3boVOiIRKWWVlZVUVla2ahupn8dg\nZt2A4wmP+tzc3ZssATezjsCbhMrnRYQH/Ax195mJZfoCfwYGA52AV4CT3P2NnG2VXSd6VVWhZ9SZ\nM8PjN3XnkYjkW0s60UtzxVCjJ7AVod4gVTFPbBx3NvAk4dbY29x9ppkNCy/7CHefZWZPAFOBKmBE\nblIoR1VVcNpp8P778OST0LlzoSMSEQmavGIws98BxxEqkUcBD7p76nYM+VJOVwzV1aEjvPnz4dFH\n1TOqiGQnqyuGhcCB7v5+y8KSJPdQpzBrVujmQklBRIpNs575XEjlcsVw8cUwdiw884zqFEQke1nX\nMUgrXXFFeB7zc88pKYhI8VJiaCPXXw+33hraKeiWVBEpZg0mBjPr2tiK7r4k/+GUp9tvh6uuCklh\nyy0LHY2ISOMarGMws7cBBwzYEvg8jncG3nX3nm0VZIynJOsYxoyBH/0o9H20Q6quB0VE8qcldQwN\ndrvt7j3dvRehX6Rj3H0Dd18fOBp4tHWhtg///ne4LXXMGCUFESkdadoxTHP3XXPmTXX33TKNbPU4\nSuqKYdky2Gef0LL5rLMKHY2ItFdZ3ZW0yMwuBO6O0/8FqE1DE37yE9hxx1CMJCJSStIkhlOA3wBj\nCXUOzwNDswyq1N1xB7zwAkyYANasPC0iUnjN6URv7dgDakGUSlHStGnhqWuVlbDzzoWORkTau7xW\nPic2+g0zmwbMjtP9zOzPLYyxrC1ZAscfD9deq6QgIqUrTeXzy4Suth9y9z3ivOnuvksbxJeMo6iv\nGNzh5JNDi+abby50NCIiQVaVzx3cfYHVLSyvalZk7cD118OcOfDSS4WORESkddIkhrfNbADg8cE7\n/0ssVpJg+nS47DJ4+WVYe+1CRyMi0jppipI2Ba4DBsZZTwNnu/tHGceWG0dRFiW5w8EHw4knqr2C\niBSfTIqS3P0D4OQWR1Xm/vEP+PRTGDas0JGIiORHmiuGTYDvEx7ruSqRuPsPMo1s9TiK7oph6VLo\n2xdGjYL99y90NCIiq8uq8nkM8DLwAqp0ruPyy0MxkpKCiJSTNFcMU9x99zaKp7E4iuqKYfZs2Hff\n0KBtiy0KHY2ISP0yaeAGjDWzQS2MqSy5h76QLrpISUFEyk+aK4ZPgPWBL4EVhGcyuLtvlH14deIo\nmiuGMWNCUnj9dVhzzUJHIyLSsJZcMaRJDB3rm+/ubVrfUCyJYdky2GknuOUWGDiw6eVFRAopr5XP\nZradu88BGur1Z2pzdlQurroK9tpLSUFEyldjj/a8zd3PMLNx9bzs7n5gtqGtFk/Brxjmz4c994TJ\nk6FXr4KGIiKSSiZFScWiGBLDccdB//5w8cUFDUNEJLXMEoOZ9QV2Alb1BOTu9zQ7wlYodGJ45ZXQ\npfacOeoPSURKRyYN3MzsV8AgoC/wBHA4obFbmyaGQrv4YrjkEiUFESl/adoxnAQcDCxy99OAfsB6\nmUZVZJ59NtQvfO97hY5ERCR7aRLDsnhr6tdm1gV4D+idbVjFwz1cLfzmN2qzICLtQ5q+kiab2QbA\nX4FXgSXAhEyjKiKPPx4e2Xmy+pcVkXaiWXclmVkfoKu7T8oupAb33eaVz9XV4S6kSy+FY45p012L\niORFvhu47dbAS1+b2W7uXvYN3O6/PxQfHX10oSMREWk7jTVwq69hW42yb+D29dewyy5w3XUwSF0I\nikiJyusVg7sf0PqQStfdd8Nmm8FhhxU6EhGRtpWmE71OwDBgf8CBccAt7r48+/DqxNFmVwzLl8MO\nO4TkoIfwiEgpy+p5DHcAewK3ALfG8TuaEdRgM5tlZrPN7IJ6Xj/IzD41s0lx+FXabWfl1ltDD6pK\nCiLSHqW5YnjD3Xdqal4D63YAZgOHAu8CE4GT3X1WYpmDgJ+5+1FNbKtNrhi+/BK22w4eeSTckSQi\nUsqyumJ43cz2TuxkT2Byyu0PAOa4+wJ3XwmMAobUs1yzgs7SjTfCPvsoKYhI+5WmgduuwMtmNi9O\nbw3MNLPJhLuTGjuFdgfeTky/Q0gWufYxsynAQuDn7v5GirjybsUKuPZaeOyxQuxdRKQ4pEkM9f3C\nz6fXgF7u/qWZHQE8BGyf8T7rNXJkqFvYffdC7F1EpDikSQw93b0yOcPM/svd/55i3YVA8pE2PeK8\nVdx9aWJ8rJndYGYbufvi3I0NHz581XhFRQUVFRUpQkjHHf7wB7jmmrxtUkSkzVVWVlJZWdmqbaSp\nfH4RmAT8AugMjIjrNdkeOD4v+k1C5fMiQh9LQ919ZmKZzdz9/Tg+ALjX3beqZ1uZVj6PHQsXXRSe\nzmZFU+MhItI6mTyPATiAkBQmAx2By9z9rjQbd/cqMzsbeJJQ0X2bu880s2HhZR8BHG9mPwJWAssI\n3Xy3uauvhvPPV1IQEUlzxbABcCPQDdiS0MvqNW3do12WVwyvvRY6yZs7V11ri0h5yep21QnAs+4+\nENgb2IZ3UlzNAAAN/klEQVTQ+rlsXH01nHOOkoKICKS7YtjK3efnzDvE3Z/JMrB64sjkimHePNhr\nr/C3a9e8b15EpKAyuWJw9/lmdrKZXRx30hP4rIUxFp0//hHOPFNJQUSkRporhuuBNYED3X1HM9sI\neMLd9250xTzL4oph8WLo0wemT4ctt8zrpkVEikJWdyXt6+79Y0tn3H2xma3VogiLzI03wpAhSgoi\nIklpEsPK2BmeA5jZxkB1plG1ga++guuvh6efLnQkIiLFJc1dSX8BRgPdzOw3wAvAlZlG1Qbuugv2\n2AN23rnQkYiIFJcm6xgAzGxnYCChF9Sn3X161oHVE0Pe6hiqq0OfSDfeCAcfnJdNiogUpazqGHD3\nGcCMFkVVhJ55BtZZB/LY1ZKISNlIU5RUdu67D045Rd1fiIjUJ1VRUjHIV1FSVVW4C2n8eNhmmzwE\nJiJSxLLqEqOsjBsH3bsrKYiINKTBOgYz+4R4i2ruS4SeUTfKLKoM3X8/HH98oaMQESleDRYlxWcp\nNMjdqzKJqAH5KEqqroYePaCyErYvyDPiRETaVl7vSso98ceuMNZOzHq3eeEV3vjxsPHGSgoiIo1p\nso7BzI40s9nAO8Ar8W+b9qyaLypGEhFpWprK58uB/YA33b0ncDgl+DwGdxg9WolBRKQpaRLD1+7+\nIdDBQkH/U8CAjOPKu4kTYd11Q4tnERFpWJqWz5+ZWWdCH0l3mtkHhGczl5SaYiQ1ahMRaVya5zF0\nAb4kXF2cDqwP3OnuH2UfXp04WnxXknt47sLo0bD77nkOTESkiGXVwO0id69y95Xufpu7Xwuc17IQ\nC2PKlPC3X7/CxiEiUgrSJIbB9cw7Mt+BZEnFSCIi6TXW8nkY8ENgezOblHipC/Ba1oHli3tIDHff\nXehIRERKQ2OVz/cC/wJ+D1yYmP+5u3+QaVR5NGNGeFrbXnsVOhIRkdLQWMvnT4BPgBPig3oOiC+N\nA0omMdx/Pxx3nIqRRETSStPy+cfAfUCvONxrZmdlHVi+qFGbiEjzpLlddSqwr7svjdOdgZfcfbc2\niC8ZR7NvV501Cw49FN5+Gzq0uw7GRUSyu13VgBWJ6ZVxXtEbPRqOPVZJQUSkORq7K2kNd/8auAt4\nxcxGx5eOAe5oi+Ba64EH4NprCx2FiEhpaex5DJPcvX8cHwDsH18a5+4T2yi+ZDzNKkpauRI6d4Yl\nS6BTpwwDExEpYnl9HgOJ4iJ3nwBMaGlghTBvXngoj5KCiEjzNJYYuplZg11fxK4xitbs2bDddoWO\nQkSk9DSWGDoCnSmRiuZcc+YoMYiItERjiWGRu1/WZpHk2Zw50LdvoaMQESk9jd3IWZJXCjV0xSAi\n0jKNJYZD2yyKDMyZA9tvX+goRERKT4OJwd0X52MHZjbYzGaZ2Wwzu6CR5fY2s5Vmdmxr9/nVV/De\ne9C7d2u3JCLS/mTaJtjMOgDXA4cDOwNDzWy1kv+43BXAE/nY79y5ISmskebBpSIiUkfWnUUMAOa4\n+wJ3XwmMAobUs9z/AveTp15bVb8gItJyWSeG7sDbiel34rxVzGxL4Gh3v5E8VXirfkFEpOWKobDl\nj0Cy7qHB5DB8+PBV4xUVFVRUVNS73OzZ0L9/foITESkllZWVVFZWtmobTXa73aqNm30TGO7ug+P0\nhYC7+5WJZf5TMwpsAnwB/MDdH87ZVuq+kioq4Fe/goEDW/8eRERKWb77SsqHiUAfM+sNLAJOBoYm\nF3D3bWrGzexvwCO5SaG5VMcgItJymSYGd68ys7OBJwn1Gbe5+0wzGxZe9hG5q7R2n0uXwuLF0LNn\na7ckItI+ZVqUlE9pi5KmTIFTT4Xp09sgKBGRIpfVE9xKioqRRERaR4lBRETqUGIQEZE6yjIxqHGb\niEjLlV1i0JPbRERap6wSw2efwZdfwhZbFDoSEZHSVVaJYc4c6NMHrKQfMSQiUlhllxhUjCQi0jpl\nlRhmz1bFs4hIa5VVYtAVg4hI6ykxiIhIHWWTGNx1q6qISD6UTWL4+OOQHLp1K3QkIiKlrWwSQ00x\nkm5VFRFpnbJLDCIi0jpKDCIiUkdZJQa1YRARab2ySQy6I0lEJD/K4tGe7tC1K7z1Fmy4YRsHJiJS\nxNrtoz3ffx86dVJSEBHJh7JIDKp4FhHJn7JIDOo8T0Qkf8oiMeiKQUQkf5QYRESkDiUGERGpo+Rv\nV62uhs6dw51JXboUIDARkSLWLm9XXbgQ1l9fSUFEJF9KPjGoGElEJL+UGEREpA4lBhERqWONQgfQ\nWoMGwRZbFDoKEZHyUfJ3JYmISMPa5V1JIiKSX0oMIiJShxKDiIjUkXliMLPBZjbLzGab2QX1vH6U\nmb1uZpPNbIKZ7Zd1TCIi0rBME4OZdQCuBw4HdgaGmlnfnMWedvd+7r4HcAZwa5YxlYPKyspCh1A0\ndCxq6VjU0rFonayvGAYAc9x9gbuvBEYBQ5ILuPuXicnOQHXGMZU8fehr6VjU0rGopWPROlknhu7A\n24npd+K8OszsaDObCTwCfD/jmEREpBFFUfns7g+5+47A0cBvCx2PiEh7lmkDNzP7JjDc3QfH6QsB\nd/crG1lnLrC3uy/Oma/WbSIiLdDcBm5Zd4kxEehjZr2BRcDJwNDkAma2rbvPjeP9gbVykwI0/42J\niEjLZJoY3L3KzM4GniQUW93m7jPNbFh42UcAx5nZ6cAKYBlwYpYxiYhI40qmryQREWkbRVH53JSm\nGsmVMzO7zczeN7OpiXkbmtmTZvammT1hZusXMsa2YGY9zOwZM5thZtPM7Cdxfns8Fp3M7JXYKHSa\nmV0a57e7Y1HDzDqY2SQzezhOt8tjYWbzkw2G47xmH4uiTwwpG8mVs78R3nvShYSGgTsAzwAXtXlU\nbe9r4Dx33xnYB/hx/By0u2Ph7suBg2Oj0N2BI8xsAO3wWCScA7yRmG6vx6IaqHD3Pdx9QJzX7GNR\n9ImBFI3kypm7vwB8kjN7CHBHHL+DcJtvWXP399x9ShxfCswEetAOjwXUaRjaiVBX6LTTY2FmPYBv\nUbfXhHZ5LABj9fN6s49FKSSGVI3k2plN3f19CCdMYNMCx9OmzGwrwi/ll4HN2uOxiEUnk4H3gKfc\nfSLt9FgA/w/4OSE51mivx8KBp8xsopmdGec1+1iU/BPcBKj7hShrZtYZuB84x92X1tO+pV0cC3ev\nBvYws67Ag2a2M6u/97I/FmZ2JPC+u08xs4pGFi37YxHt5+6LzKwb8KSZvUkLPhelcMWwEOiVmO4R\n57Vn75vZZgBmtjnwQYHjaRNmtgYhKdzl7mPi7HZ5LGq4+xKgEhhM+zwW+wFHmdl/gJHAIWZ2F/Be\nOzwWuPui+PdD4CFCUXyzPxelkBhWNZIzs7UIjeQeLnBMbc3iUONh4Ltx/L+BMbkrlKm/Am+4+58S\n89rdsTCzTWruLDGzdYDDCHUu7e5YuPsv3b2Xu29DODc84+6nEfpd+25crF0cCzNbN15RY2brAYOA\nabTgc1ES7RjMbDDwJ2obyV1R4JDajJndA1QAGwPvA5cSfgncB/QEFgAnuvunhYqxLcTndDxP+KB7\nHH4JTADupX0di10JlYgd4vAPd7/czDainR2LJDM7CPiZux/VHo+FmW0NPEj4bqwB/N3dr2jJsSiJ\nxCAiIm2nFIqSRESkDSkxiIhIHUoMIiJShxKDiIjUocQgIiJ1KDGIiEgdSgwiIlKHEoOUJDOrNrOr\nE9M/M7NfFzKmtmRm/czsiELHIeVJiUFK1XLg2Niqsz3andDVtEjeKTFIqfoaGAGcl2ZhM9vUzB4w\nsynx6VbfjPPPi09Bm2pm58R5vc1sppn9LT716m4zO9TMXojTe8XlLjWzO83spTj/zMT+ro7bfd3M\nTozzDjKzZ83svrj9uxLL9zezythd8thEp2fPmtkV8Ylts8xsPzNbE7gMODE+tewEMzswvq9JZvZa\n7CtHpGXcXYOGkhuAJUBnYB7QBfgZ8OtGlh8F/CSOW1ynP/A6sDawHjAd6Af0BlYAO8XlXwVujeNH\nAQ/G8UuBycBahL6s3gI2B44FnojLbEron2Yz4CDCQ5e2iDG8BOxL6NfmRWDjuM6JhD7BAJ4Fro7j\nRxCevQChM7TrEu/vYWCfOL4u0KHQ/yMNpTvoeQxSsjw8j+EOwmMdlzWx+CHAaXE9Bz43s/0JJ/mv\nAMzsAeAAQs+c89y95lGRM4B/xfFphMRRY4y7rwA+NrNngG8A+xO6gMbdPzCzSmBv4HNggseukc1s\nCrAV8BmwC+EBKzVP4Ho3sY8H4t/Xcvad9CLw/8zs78AD7t7eu6aXVlBikFL3J2ASoUvuxjS3t8jl\nifHqxHQ1db83ye1afD1Xssv05Har4rYMmO7u+zURS83yq3H3K83sUeBI4EUzG+TusxvYnkijVMcg\npcoA3P0TQpfCZza+OP8CzoJVj8XsCowDjjaztWOZ/DFx3qrtpzDEzNYys40JRUUT4zZOivvpRrgK\nmdDINt4EuiXqPdYws50aWLYmrs+Brqtmmm3j7jPc/aoYQ9+U8YusRolBSlXyl/o1hDL+xq4KzgUO\nNrOphDqDHd19MnA74UQ6Hhjh7q/Xs/3GtjuV8AS1l4DL3P09d38wzn8deBr4ubvX99QsB3D3lcDx\nwJWxeGkysE8D+66ZfhbYqabyGTg3VnZPIdSPjG0kZpFG6XkMIi1kZpcCn7v7tYWORSSfdMUgIiJ1\n6IpByoqZ/RI4gVDkYvHvfe7++4IGJlJClBhERKQOFSWJiEgdSgwiIlKHEoOIiNShxCAiInUoMYiI\nSB3/H1fZ8hiL1jYyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119002a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOTTING Components Explained Variance\n",
    "def plot_variance(pca_components_ids,pca_var_inc,pca_var_total):\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.plot(pca_components_ids, pca_var_inc)\n",
    "    plt.title(\"PCA: added explained variance per component\")\n",
    "    plt.xlabel('N_components')\n",
    "    plt.ylabel('Added explained variance')\n",
    "    plt.show()\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "\n",
    "    plt.plot(pca_components_ids, pca_var_total)\n",
    "    plt.title(\"PCA: Total explained variance per component\")\n",
    "    plt.xlabel('N_components')\n",
    "    plt.ylabel('Total explained variance')\n",
    "    plt.show()\n",
    "\n",
    "plot_variance(pca_components_ids,pca_var_inc,pca_var_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Choosing number of components for LDA and PCA\n",
    "This analysis suggests that the more PCA components there are, the more variance we explain, but we do aim to minimize the number of components use. So I would choose about 50 PCA components - where the additional explained variance drops below 0.002 and it remains the same 4 decimals for 4 iterations (or 3 iterations afterward). With PCA, the explained variance keeps improving as we add components, so at 50 components, the variance explained is fair, ~0.79, and the marginal utility of adding another PCA component is flattened and doesn't contribute significantly anymore. We can see that with the text output as well as from the second plot of Total Explained Variance, bring after the convex point and going flatter and flatter, and from the incremental value added plot seeing it's close to 0 and pretty flat at least from components 35 or 40. We could, of course, continue adding components, but the whole goal of dimensionality reduction is to simplify with much FEWER components than the original data, so we don't want to over-add components.\n",
    "\n",
    "However, for LDA there is only a maximum of 1 component to choose in this case. It can't give us the same kind of measure of variance explained. The maximum number of dimensions for LDA, in this case, is #of classes-1, which would be (2 classes - 1) = 1 dimension maximum anyway. Therefore we can't optimize the number of LDA dimensions to choose.\n",
    "Let's see how good will LDA will perform with one class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:57: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Dimensions: \n",
      "component 1 adds 0.3223 to variance, now total variance is 0.3223\n",
      "component 2 adds 0.1032 to variance, now total variance is 0.4254\n",
      "component 3 adds 0.0459 to variance, now total variance is 0.4713\n",
      "component 4 adds 0.0329 to variance, now total variance is 0.5042\n",
      "component 5 adds 0.0288 to variance, now total variance is 0.5330\n",
      "component 6 adds 0.0253 to variance, now total variance is 0.5583\n",
      "component 7 adds 0.0196 to variance, now total variance is 0.5779\n",
      "component 8 adds 0.0158 to variance, now total variance is 0.5937\n",
      "component 9 adds 0.0143 to variance, now total variance is 0.6080\n",
      "component 10 adds 0.0116 to variance, now total variance is 0.6196\n",
      "component 11 adds 0.0112 to variance, now total variance is 0.6308\n",
      "component 12 adds 0.0103 to variance, now total variance is 0.6411\n",
      "component 13 adds 0.0092 to variance, now total variance is 0.6503\n",
      "component 14 adds 0.0082 to variance, now total variance is 0.6585\n",
      "component 15 adds 0.0074 to variance, now total variance is 0.6659\n",
      "component 16 adds 0.0068 to variance, now total variance is 0.6728\n",
      "component 17 adds 0.0065 to variance, now total variance is 0.6793\n",
      "component 18 adds 0.0062 to variance, now total variance is 0.6855\n",
      "component 19 adds 0.0060 to variance, now total variance is 0.6915\n",
      "component 20 adds 0.0056 to variance, now total variance is 0.6971\n",
      "component 21 adds 0.0055 to variance, now total variance is 0.7026\n",
      "component 22 adds 0.0052 to variance, now total variance is 0.7078\n",
      "component 23 adds 0.0049 to variance, now total variance is 0.7127\n",
      "component 24 adds 0.0046 to variance, now total variance is 0.7173\n",
      "component 25 adds 0.0044 to variance, now total variance is 0.7217\n",
      "component 26 adds 0.0043 to variance, now total variance is 0.7261\n",
      "component 27 adds 0.0040 to variance, now total variance is 0.7300\n",
      "component 28 adds 0.0038 to variance, now total variance is 0.7338\n",
      "component 29 adds 0.0037 to variance, now total variance is 0.7375\n",
      "component 30 adds 0.0036 to variance, now total variance is 0.7411\n",
      "component 31 adds 0.0032 to variance, now total variance is 0.7443\n",
      "component 32 adds 0.0031 to variance, now total variance is 0.7474\n",
      "component 33 adds 0.0031 to variance, now total variance is 0.7505\n",
      "component 34 adds 0.0030 to variance, now total variance is 0.7534\n",
      "component 35 adds 0.0029 to variance, now total variance is 0.7563\n",
      "component 36 adds 0.0028 to variance, now total variance is 0.7591\n",
      "component 37 adds 0.0026 to variance, now total variance is 0.7617\n",
      "component 38 adds 0.0025 to variance, now total variance is 0.7643\n",
      "component 39 adds 0.0025 to variance, now total variance is 0.7668\n",
      "component 40 adds 0.0024 to variance, now total variance is 0.7692\n",
      "component 41 adds 0.0023 to variance, now total variance is 0.7715\n",
      "component 42 adds 0.0023 to variance, now total variance is 0.7738\n",
      "component 43 adds 0.0023 to variance, now total variance is 0.7761\n",
      "component 44 adds 0.0022 to variance, now total variance is 0.7783\n",
      "component 45 adds 0.0021 to variance, now total variance is 0.7804\n",
      "component 46 adds 0.0020 to variance, now total variance is 0.7824\n",
      "component 47 adds 0.0019 to variance, now total variance is 0.7843\n",
      "component 48 adds 0.0019 to variance, now total variance is 0.7861\n",
      "component 49 adds 0.0018 to variance, now total variance is 0.7880\n",
      "component 50 adds 0.0018 to variance, now total variance is 0.7898\n",
      "PCA components selected:  50\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3d2f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEaCAYAAAAVJPDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSB3/H1fZ8hiL1jYyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119002c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PCA! - find the principal components, now with selected 50 components\n",
    "pca = RandomizedPCA(n_components=50, random_state=0)\n",
    "X_PCA = pca.fit_transform(data)\n",
    "pca_var_ratios = pca.explained_variance_ratio_\n",
    "\n",
    "print \"PCA Dimensions: \"\n",
    "pca_n_components, pca_components_ids, pca_var_inc, pca_var_total = select_n_components(pca_var_ratios, 0.95)\n",
    "print \"PCA components selected: \", pca_n_components\n",
    "plot_variance(pca_components_ids,pca_var_inc,pca_var_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIVE MODELING AND RESULTS\n",
    "#### Prep: splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Train N: 336, PCA Test N: 84\n",
      "PCA Train N: 336, PCA Test N: 84\n",
      "Y   Train N: 336, Y   Test N: 84\n"
     ]
    }
   ],
   "source": [
    "# PREDICTIVE MODELING\n",
    "\n",
    "# split the data into a training set and a test set; since the photos are randomly shuffled, we can just split by order.\n",
    "# determine split cutoff 80% training data, 20% test data\n",
    "train_split = int(0.8*len(data)) \n",
    "#splitting: \n",
    "X_PCA_train , X_PCA_test = X_PCA[:train_split] , X_PCA[train_split:] \n",
    "X_LDA_train , X_LDA_test = X_LDA[:train_split] , X_LDA[train_split:] \n",
    "y_train , y_test = y[:train_split] , y[train_split:]\n",
    "# for debug:\n",
    "print(\"PCA Train N: {}, PCA Test N: {}\".format(len(X_PCA_train), len(X_PCA_test)))\n",
    "print(\"PCA Train N: {}, PCA Test N: {}\".format(len(X_LDA_train), len(X_LDA_test)))\n",
    "print(\"Y   Train N: {}, Y   Test N: {}\".format(len(y_train), len(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA & LDA on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Logistic Regression ***\n",
      "\n",
      "PCA: Logistic Regression\n",
      "Accuracy score PCA Classifier, training set:  0.741071428571\n",
      "Accuracy score PCA Classifier, test set:  0.654761904762\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[27 10]\n",
      " [19 28]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.73      0.65        37\n",
      "          1       0.74      0.60      0.66        47\n",
      "\n",
      "avg / total       0.67      0.65      0.66        84\n",
      "\n",
      "\n",
      "LDA: Logistic Regression\n",
      "Accuracy score LDA Classifier, training set:  0.925595238095\n",
      "Accuracy score LDA Classifier, test set:  0.892857142857\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[33  4]\n",
      " [ 5 42]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.89      0.88        37\n",
      "          1       0.91      0.89      0.90        47\n",
      "\n",
      "avg / total       0.89      0.89      0.89        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE AND FIT CLASSIFIER MODEL\n",
    "print \"*** Logistic Regression ***\\n\"\n",
    "#over PCA Data\n",
    "print \"PCA: Logistic Regression\"\n",
    "clf_pca = LogisticRegression(penalty='l2')\n",
    "clf_pca.fit(X_PCA_train,y_train)\n",
    "print \"Accuracy score PCA Classifier, training set: \",clf_pca.score(X_PCA_train,y_train)\n",
    "print \"Accuracy score PCA Classifier, test set: \",clf_pca.score(X_PCA_test,y_test)\n",
    "predicted_pca_log = clf_pca.predict(X_PCA_test)\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_test, predicted_pca_log))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_test, predicted_pca_log))\n",
    "\n",
    "#over LDA Data\n",
    "print \"\\nLDA: Logistic Regression\"\n",
    "clf_lda_log = LogisticRegression(penalty='l2')\n",
    "clf_lda_log.fit(X_LDA_train,y_train)\n",
    "print \"Accuracy score LDA Classifier, training set: \",clf_lda_log.score(X_LDA_train,y_train)\n",
    "print \"Accuracy score LDA Classifier, test set: \"  ,  clf_lda_log.score(X_LDA_test,y_test)\n",
    "predicted_lda_log = clf_lda_log.predict(X_LDA_test)\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_test, predicted_lda_log))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_test, predicted_lda_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and LDA on SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SVC MODEL ***\n",
      "\n",
      "PCA: SVC Model\n",
      "Accuracy score PCA Classifier, training set:  0.613095238095\n",
      "Accuracy score PCA Classifier, test set:  0.535714285714\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[20 17]\n",
      " [22 25]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.54      0.51        37\n",
      "          1       0.60      0.53      0.56        47\n",
      "\n",
      "avg / total       0.54      0.54      0.54        84\n",
      "\n",
      "\n",
      "LDA: SVC Model\n",
      "Accuracy score LDA Classifier, training set:  0.931547619048\n",
      "Accuracy score LDA Classifier, test set:  0.892857142857\n",
      "\n",
      "Confusion Matrix on test set: \n",
      "[[33  4]\n",
      " [ 5 42]]\n",
      "\n",
      "Classification report on test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.89      0.88        37\n",
      "          1       0.91      0.89      0.90        47\n",
      "\n",
      "avg / total       0.89      0.89      0.89        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE AND FIT CLASSIFIER MODEL\n",
    "clf_svc =  LinearSVC(random_state = 42) #(if we give it the answer to the question about the meaning of life and everything as a random seed it might perform better since it knows already the answer to everything)\n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print \"*** SVC MODEL ***\\n\"\n",
    "\n",
    "#over PCA Data\n",
    "print \"PCA: SVC Model\"\n",
    "clf_pca_svc = LinearSVC(random_state = 42)\n",
    "clf_pca_svc.fit(X_PCA_train,y_train)\n",
    "print \"Accuracy score PCA Classifier, training set: \",clf_pca_svc.score(X_PCA_train,y_train)\n",
    "print \"Accuracy score PCA Classifier, test set: \",clf_pca_svc.score(X_PCA_test,y_test)\n",
    "predicted_pca_svc = clf_pca_svc.predict(X_PCA_test)\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_test, predicted_pca_svc))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_test, predicted_pca_svc))\n",
    "\n",
    "#over LDA Data\n",
    "print \"\\nLDA: SVC Model\"\n",
    "clf_lda_svc = LinearSVC(random_state = 42)\n",
    "clf_lda_svc.fit(X_LDA_train,y_train)\n",
    "print \"Accuracy score LDA Classifier, training set: \",clf_lda_svc.score(X_LDA_train,y_train)\n",
    "print \"Accuracy score LDA Classifier, test set: \"  ,  clf_lda_svc.score(X_LDA_test,y_test)\n",
    "predicted_lda_svc = clf_lda_svc.predict(X_LDA_test)\n",
    "print \"\\nConfusion Matrix on test set: \"\n",
    "print(metrics.confusion_matrix(y_test, predicted_lda_svc))\n",
    "print \"\\nClassification report on test set\"\n",
    "print(metrics.classification_report(y_test, predicted_lda_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS\n",
    "\n",
    "#### Results and conclusion\n",
    "The models results showed surprisingly that LDA (although having only one dimension) performed much better than PCA, on both Logistic Regression or SVC classifiers:\n",
    "LDA data resulted in Accuracy of 0.89 (with either svc or logistic regression)\n",
    "Whereas PCA data resulted in Accuracy of 0.66 for logistic regression and 0.53 for SVC classifiers.\n",
    "Therefore, I would choose an LDA reduced dimensionality representation of the data for this problem; however, once choosing it, the choice of the model between logistic regression or linear SVC apparently didn't matter much regarding accuracy of prediction on the test set. However, logistic regression performed better for the PCA fitted data, suggesting that it might do a better job with even more dimensioned data for this particular problem. Also, Logistic regression is a simpler model and can be more easily interpreted, and now that there are no differences in accuracy - I would choose Logistic Regression as a model and LDA as a dimensionality reduction technique.\n",
    "\n",
    "#### Interpretation\n",
    "Generally, LDA can indeed be superior, since it is a case of CLASSIFICATION, and LDA is built to find optimal dimensions that maximize the SEPARABILITY of classes, so it directly helps with classification; whereas PCA is merely focuses at preserving as much of the original \"information\" in the data as possible, but without any relation to the separability or later uses of the data, be it classification or not. Therefore LDA might be better for some classification problems, and it seemed to have performed better for this one, where we only care about classification, and care to preserve nothing more from the data after we classify.\n",
    "However, I highly doubted this results, since we are classifying into two categories alone. Thus *LDA's maximal number of dimensions was 1, and it used **only one dimension** for the classification*. Thus it's weird that it performed so well.\n",
    "I have tried to see if there were any bugs or abnormalities have caused that, but found none so far. \n",
    "Therefore I'm left with concluding, that if everything was handled correctly indeed, then the LDA simply did a great job at finding a one dimensional representation of the factor that clearly was the key determinant of boys and girls.\n",
    "\n",
    "Conversely, PCA does seem to perform rather poorly, especially with the SVC classifier. There, the accuracy on the training set is only around 0.61. ** How come it doesn't even classify well on its own training set?**. Also bad were the results on the test set - only about 0.53 accuracy, which is pretty bad - it is *not much better than chance!*. \n",
    "Therefore PCA seems not the right path to go, especially not with SVC classifier.\n",
    "\n",
    "#### Possible problems with the Data \n",
    "From looking more closely at the sample of the ~250 shirts I used. It does seem that the majority of male shirts were in darker, bluer colors, where the majority of women's shirts were in lighter, reddish / pinkish hues. Therefore, the LDA might have successfully picked up on that color difference in an efficient *\"DarkBlueish to LightRedPinkish\"* 1D scale that served as the necessary information. \n",
    "Therefore, I would try this on a much larger dataset than the sample of ~250 shirts I used.\n",
    "\n",
    "Other potential problems with the Data, is that firstly, the classification is sometimes ambiguous and it's a difficult problem because of that. The images are not standardized (not in a uniform format regarding how many clothes per picture, which positioning, which background, etc.). \n",
    "\n",
    "From looking at the pictures, there are quite a bit of shirt that *I myself wouldn't know for which gender to classify*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# APPENDIX\n",
    "\n",
    "### More Detailed Results\n",
    "Presenting model probabilities and scores for each shirt in database, and finding the most extreme values (most \"manly\" shirts, most \"girly\" shirts, most ambigious shirts) for debuggin and more detailed look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# RESULTS METRICS\n",
    "\n",
    "# First: Here are the Model SCORES for EVERY SHIRT in our dataset\n",
    "probs_PCA = zip(clf_pca.decision_function(X_PCA),raw_data)\n",
    "probs_LDA = zip(clf_lda.decision_function(X_LDA),raw_data)\n",
    "\n",
    "def evaluate_and_find_extremes(model):\n",
    "    \"\"\"find most extreme shirts, for either PCA or LDA, as specified in the parameters above\"\"\"\n",
    "    ## get variables for this model:\n",
    "    if model in (\"pca\",\"PCA\"):\n",
    "        probs = probs_PCA\n",
    "        x_model = X_PCA\n",
    "        X_test_model = X_PCA_test\n",
    "        clf_model = clf_pca\n",
    "    \n",
    "    elif model in(\"lda\",\"LDA\"):\n",
    "        probs = probs_LDA\n",
    "        x_model = X_LDA\n",
    "        X_test_model = X_LDA_test\n",
    "        clf_model = clf_lda\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"please enter either 'PCA' or 'LDA', with quotes, like this: extreme_shirts('PCA') \")\n",
    "        \n",
    "    girliest_girl_shirt = sorted(probs,key=lambda (p,(cd,g,f)): (0 if g == 'girl' else 1,p))[0]\n",
    "    girliest_boy_shirt = sorted(probs,key=lambda (p,(cd,g,f)): (0 if g == 'boy' else 1,p))[0]\n",
    "    boyiest_girl_shirt = sorted(probs,key=lambda (p,(cd,g,f)): (0 if g == 'girl' else 1,-p))[0]\n",
    "    boyiest_boy_shirt = sorted(probs,key=lambda (p,(cd,g,f)): (0 if g == 'boy' else 1,-p))[0]\n",
    "    most_androgynous_shirt = sorted(probs,key=lambda (p,(cd,g,f)): abs(p))[0]\n",
    "\n",
    "    # and let's look at the most and least extreme shirts\n",
    "    cd = zip( x_model ,raw_data) #the x specified from the function: either of PCA or LDA\n",
    "    least_extreme_shirt = sorted(cd,key=lambda (x,(d,g,f)): sum([abs(c) for c in x]))[0]\n",
    "    most_extreme_shirt =  sorted(cd,key=lambda (x,(d,g,f)): sum([abs(c) for c in x]),reverse=True)[0]\n",
    "\n",
    "    least_interesting_shirt = sorted(cd,key=lambda (x,(d,g,f)): max([abs(c) for c in x]))[0]\n",
    "    most_interesting_shirt =  sorted(cd,key=lambda (x,(d,g,f)): min([abs(c) for c in x]),reverse=True)[0]\n",
    "\n",
    "    # and now let's look at precision-recall\n",
    "    probs = zip(clf_model.decision_function(X_test_model),raw_data[train_split:])\n",
    "    num_boys = len([c for c in y_test if c == 1])\n",
    "    num_girls = len([c for c in y_test if c == 0])\n",
    "    # take lowest and highest probabilities for later comparing to model scores\n",
    "    lowest_score = round(min([p[0] for p in probs]),1) - 0.1\n",
    "    highest_score = round(max([p[0] for p in probs]),1) + 0.1\n",
    "    INTERVAL = 0.1\n",
    "\n",
    "    # first do the girls\n",
    "    score = lowest_score\n",
    "    while score <= highest_score:\n",
    "        true_positives  = len([p for p in probs if p[0] <= score and p[1][1] == 'girl'])\n",
    "        false_positives = len([p for p in probs if p[0] <= score and p[1][1] == 'boy'])\n",
    "        positives = true_positives + false_positives\n",
    "        if positives > 0:\n",
    "            precision = 1.0 * true_positives / positives\n",
    "            recall = 1.0 * true_positives / num_girls\n",
    "            print \"WOMEN - score: %.2f, precision: %.3f ,recall: %.3f \" % (score,precision,recall)\n",
    "        score += INTERVAL\n",
    "\n",
    "    # then do the boys\n",
    "    score = highest_score\n",
    "    while score >= lowest_score:\n",
    "        true_positives  = len([p for p in probs if p[0] >= score and p[1][1] == 'boy'])\n",
    "        false_positives = len([p for p in probs if p[0] >= score and p[1][1] == 'girl'])\n",
    "        positives = true_positives + false_positives\n",
    "        if positives > 0:\n",
    "            precision = 1.0 * true_positives / positives\n",
    "            recall = 1.0 * true_positives / num_boys\n",
    "            #print \"boys\",score,precision,recall\n",
    "            print \"MEN -  score: %.2f, precision: %.3f ,recall: %.3f \" % (score,precision,recall)\n",
    "        score -= INTERVAL\n",
    "\n",
    "    # now do both\n",
    "    score = lowest_score\n",
    "    while score <= highest_score:\n",
    "        girls  = len([p for p in probs if p[0] <= score and p[1][1] == 'girl'])\n",
    "        boys = len([p for p in probs if p[0] <= score and p[1][1] == 'boy'])\n",
    "        print \"score: %.2f. Women: %d, Men: %d\" % (score, girls, boys)\n",
    "        score += INTERVAL\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOMEN - score: -8.50, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -8.40, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -8.30, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -8.20, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -8.10, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -8.00, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.90, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.80, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.70, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.60, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.50, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.40, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.30, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.20, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.10, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -7.00, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.90, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.80, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.70, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.60, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.50, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.40, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.30, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.20, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.10, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -6.00, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.90, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.80, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.70, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.60, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.50, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.40, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.30, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.20, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.10, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.00, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.90, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.80, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.70, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.60, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.50, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.40, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -4.30, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -4.20, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -4.10, precision: 1.000 ,recall: 0.081 \n",
      "WOMEN - score: -4.00, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -3.90, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -3.80, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -3.70, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -3.60, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -3.50, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -3.40, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -3.30, precision: 0.833 ,recall: 0.135 \n",
      "WOMEN - score: -3.20, precision: 0.833 ,recall: 0.135 \n",
      "WOMEN - score: -3.10, precision: 0.714 ,recall: 0.135 \n",
      "WOMEN - score: -3.00, precision: 0.556 ,recall: 0.135 \n",
      "WOMEN - score: -2.90, precision: 0.556 ,recall: 0.135 \n",
      "WOMEN - score: -2.80, precision: 0.556 ,recall: 0.135 \n",
      "WOMEN - score: -2.70, precision: 0.556 ,recall: 0.135 \n",
      "WOMEN - score: -2.60, precision: 0.583 ,recall: 0.189 \n",
      "WOMEN - score: -2.50, precision: 0.583 ,recall: 0.189 \n",
      "WOMEN - score: -2.40, precision: 0.583 ,recall: 0.189 \n",
      "WOMEN - score: -2.30, precision: 0.583 ,recall: 0.189 \n",
      "WOMEN - score: -2.20, precision: 0.615 ,recall: 0.216 \n",
      "WOMEN - score: -2.10, precision: 0.643 ,recall: 0.243 \n",
      "WOMEN - score: -2.00, precision: 0.667 ,recall: 0.270 \n",
      "WOMEN - score: -1.90, precision: 0.688 ,recall: 0.297 \n",
      "WOMEN - score: -1.80, precision: 0.667 ,recall: 0.324 \n",
      "WOMEN - score: -1.70, precision: 0.632 ,recall: 0.324 \n",
      "WOMEN - score: -1.60, precision: 0.600 ,recall: 0.324 \n",
      "WOMEN - score: -1.50, precision: 0.619 ,recall: 0.351 \n",
      "WOMEN - score: -1.40, precision: 0.560 ,recall: 0.378 \n",
      "WOMEN - score: -1.30, precision: 0.560 ,recall: 0.378 \n",
      "WOMEN - score: -1.20, precision: 0.593 ,recall: 0.432 \n",
      "WOMEN - score: -1.10, precision: 0.607 ,recall: 0.459 \n",
      "WOMEN - score: -1.00, precision: 0.613 ,recall: 0.514 \n",
      "WOMEN - score: -0.90, precision: 0.588 ,recall: 0.541 \n",
      "WOMEN - score: -0.80, precision: 0.600 ,recall: 0.568 \n",
      "WOMEN - score: -0.70, precision: 0.615 ,recall: 0.649 \n",
      "WOMEN - score: -0.60, precision: 0.625 ,recall: 0.676 \n",
      "WOMEN - score: -0.50, precision: 0.605 ,recall: 0.703 \n",
      "WOMEN - score: -0.40, precision: 0.591 ,recall: 0.703 \n",
      "WOMEN - score: -0.30, precision: 0.591 ,recall: 0.703 \n",
      "WOMEN - score: -0.20, precision: 0.600 ,recall: 0.730 \n",
      "WOMEN - score: -0.10, precision: 0.587 ,recall: 0.730 \n",
      "WOMEN - score: -0.00, precision: 0.587 ,recall: 0.730 \n",
      "WOMEN - score: 0.10, precision: 0.596 ,recall: 0.757 \n",
      "WOMEN - score: 0.20, precision: 0.583 ,recall: 0.757 \n",
      "WOMEN - score: 0.30, precision: 0.580 ,recall: 0.784 \n",
      "WOMEN - score: 0.40, precision: 0.588 ,recall: 0.811 \n",
      "WOMEN - score: 0.50, precision: 0.588 ,recall: 0.811 \n",
      "WOMEN - score: 0.60, precision: 0.577 ,recall: 0.811 \n",
      "WOMEN - score: 0.70, precision: 0.577 ,recall: 0.811 \n",
      "WOMEN - score: 0.80, precision: 0.566 ,recall: 0.811 \n",
      "WOMEN - score: 0.90, precision: 0.545 ,recall: 0.811 \n",
      "WOMEN - score: 1.00, precision: 0.536 ,recall: 0.811 \n",
      "WOMEN - score: 1.10, precision: 0.542 ,recall: 0.865 \n",
      "WOMEN - score: 1.20, precision: 0.541 ,recall: 0.892 \n",
      "WOMEN - score: 1.30, precision: 0.556 ,recall: 0.946 \n",
      "WOMEN - score: 1.40, precision: 0.538 ,recall: 0.946 \n",
      "WOMEN - score: 1.50, precision: 0.545 ,recall: 0.973 \n",
      "WOMEN - score: 1.60, precision: 0.545 ,recall: 0.973 \n",
      "WOMEN - score: 1.70, precision: 0.537 ,recall: 0.973 \n",
      "WOMEN - score: 1.80, precision: 0.522 ,recall: 0.973 \n",
      "WOMEN - score: 1.90, precision: 0.522 ,recall: 0.973 \n",
      "WOMEN - score: 2.00, precision: 0.522 ,recall: 0.973 \n",
      "WOMEN - score: 2.10, precision: 0.522 ,recall: 0.973 \n",
      "WOMEN - score: 2.20, precision: 0.514 ,recall: 0.973 \n",
      "WOMEN - score: 2.30, precision: 0.493 ,recall: 0.973 \n",
      "WOMEN - score: 2.40, precision: 0.480 ,recall: 0.973 \n",
      "WOMEN - score: 2.50, precision: 0.474 ,recall: 0.973 \n",
      "WOMEN - score: 2.60, precision: 0.468 ,recall: 0.973 \n",
      "WOMEN - score: 2.70, precision: 0.462 ,recall: 0.973 \n",
      "WOMEN - score: 2.80, precision: 0.456 ,recall: 0.973 \n",
      "WOMEN - score: 2.90, precision: 0.444 ,recall: 0.973 \n",
      "WOMEN - score: 3.00, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.10, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.20, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.30, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.40, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.50, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.60, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.70, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.80, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.90, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 4.00, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 4.10, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 4.20, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 4.30, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 4.40, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 4.50, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 4.60, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 4.70, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 4.80, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 4.90, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.00, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.10, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.20, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.30, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.40, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.50, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.60, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.70, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.80, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 5.90, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 6.00, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 6.10, precision: 0.440 ,recall: 1.000 \n",
      "WOMEN - score: 6.20, precision: 0.440 ,recall: 1.000 \n",
      "MEN -  score: 6.00, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.90, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.80, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.70, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.60, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.50, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.40, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.30, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.20, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.10, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 5.00, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.90, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.80, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.70, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.60, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.50, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.40, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 4.30, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 4.20, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 4.10, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 4.00, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.90, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.80, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.70, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.60, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.50, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.40, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.30, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.20, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.10, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.00, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 2.90, precision: 0.667 ,recall: 0.043 \n",
      "MEN -  score: 2.80, precision: 0.800 ,recall: 0.085 \n",
      "MEN -  score: 2.70, precision: 0.833 ,recall: 0.106 \n",
      "MEN -  score: 2.60, precision: 0.857 ,recall: 0.128 \n",
      "MEN -  score: 2.50, precision: 0.875 ,recall: 0.149 \n",
      "MEN -  score: 2.40, precision: 0.889 ,recall: 0.170 \n",
      "MEN -  score: 2.30, precision: 0.909 ,recall: 0.213 \n",
      "MEN -  score: 2.20, precision: 0.929 ,recall: 0.277 \n",
      "MEN -  score: 2.10, precision: 0.933 ,recall: 0.298 \n",
      "MEN -  score: 2.00, precision: 0.933 ,recall: 0.298 \n",
      "MEN -  score: 1.90, precision: 0.933 ,recall: 0.298 \n",
      "MEN -  score: 1.80, precision: 0.933 ,recall: 0.298 \n",
      "MEN -  score: 1.70, precision: 0.941 ,recall: 0.340 \n",
      "MEN -  score: 1.60, precision: 0.944 ,recall: 0.362 \n",
      "MEN -  score: 1.50, precision: 0.944 ,recall: 0.362 \n",
      "MEN -  score: 1.40, precision: 0.895 ,recall: 0.362 \n",
      "MEN -  score: 1.30, precision: 0.905 ,recall: 0.404 \n",
      "MEN -  score: 1.20, precision: 0.826 ,recall: 0.404 \n",
      "MEN -  score: 1.10, precision: 0.800 ,recall: 0.426 \n",
      "MEN -  score: 1.00, precision: 0.750 ,recall: 0.447 \n",
      "MEN -  score: 0.90, precision: 0.759 ,recall: 0.468 \n",
      "MEN -  score: 0.80, precision: 0.774 ,recall: 0.511 \n",
      "MEN -  score: 0.70, precision: 0.781 ,recall: 0.532 \n",
      "MEN -  score: 0.60, precision: 0.781 ,recall: 0.532 \n",
      "MEN -  score: 0.50, precision: 0.788 ,recall: 0.553 \n",
      "MEN -  score: 0.40, precision: 0.788 ,recall: 0.553 \n",
      "MEN -  score: 0.30, precision: 0.765 ,recall: 0.553 \n",
      "MEN -  score: 0.20, precision: 0.750 ,recall: 0.574 \n",
      "MEN -  score: 0.10, precision: 0.757 ,recall: 0.596 \n",
      "MEN -  score: 0.00, precision: 0.737 ,recall: 0.596 \n",
      "MEN -  score: -0.10, precision: 0.737 ,recall: 0.596 \n",
      "MEN -  score: -0.20, precision: 0.744 ,recall: 0.617 \n",
      "MEN -  score: -0.30, precision: 0.725 ,recall: 0.617 \n",
      "MEN -  score: -0.40, precision: 0.725 ,recall: 0.617 \n",
      "MEN -  score: -0.50, precision: 0.732 ,recall: 0.638 \n",
      "MEN -  score: -0.60, precision: 0.727 ,recall: 0.681 \n",
      "MEN -  score: -0.70, precision: 0.711 ,recall: 0.681 \n",
      "MEN -  score: -0.80, precision: 0.673 ,recall: 0.702 \n",
      "MEN -  score: -0.90, precision: 0.660 ,recall: 0.702 \n",
      "MEN -  score: -1.00, precision: 0.660 ,recall: 0.745 \n",
      "MEN -  score: -1.10, precision: 0.643 ,recall: 0.766 \n",
      "MEN -  score: -1.20, precision: 0.632 ,recall: 0.766 \n",
      "MEN -  score: -1.30, precision: 0.610 ,recall: 0.766 \n",
      "MEN -  score: -1.40, precision: 0.610 ,recall: 0.766 \n",
      "MEN -  score: -1.50, precision: 0.619 ,recall: 0.830 \n",
      "MEN -  score: -1.60, precision: 0.609 ,recall: 0.830 \n",
      "MEN -  score: -1.70, precision: 0.615 ,recall: 0.851 \n",
      "MEN -  score: -1.80, precision: 0.621 ,recall: 0.872 \n",
      "MEN -  score: -1.90, precision: 0.618 ,recall: 0.894 \n",
      "MEN -  score: -2.00, precision: 0.609 ,recall: 0.894 \n",
      "MEN -  score: -2.10, precision: 0.600 ,recall: 0.894 \n",
      "MEN -  score: -2.20, precision: 0.592 ,recall: 0.894 \n",
      "MEN -  score: -2.30, precision: 0.583 ,recall: 0.894 \n",
      "MEN -  score: -2.40, precision: 0.583 ,recall: 0.894 \n",
      "MEN -  score: -2.50, precision: 0.583 ,recall: 0.894 \n",
      "MEN -  score: -2.60, precision: 0.583 ,recall: 0.894 \n",
      "MEN -  score: -2.70, precision: 0.573 ,recall: 0.915 \n",
      "MEN -  score: -2.80, precision: 0.573 ,recall: 0.915 \n",
      "MEN -  score: -2.90, precision: 0.573 ,recall: 0.915 \n",
      "MEN -  score: -3.00, precision: 0.573 ,recall: 0.915 \n",
      "MEN -  score: -3.10, precision: 0.584 ,recall: 0.957 \n",
      "MEN -  score: -3.20, precision: 0.590 ,recall: 0.979 \n",
      "MEN -  score: -3.30, precision: 0.590 ,recall: 0.979 \n",
      "MEN -  score: -3.40, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -3.50, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -3.60, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -3.70, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -3.80, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -3.90, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -4.00, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -4.10, precision: 0.580 ,recall: 1.000 \n",
      "MEN -  score: -4.20, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -4.30, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -4.40, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -4.50, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.60, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.70, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.80, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.90, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.00, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.10, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.20, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.30, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.40, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.50, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.60, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.70, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.80, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.90, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.00, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.10, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.20, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.30, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.40, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.50, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.60, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.70, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.80, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -6.90, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.00, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.10, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.20, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.30, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.40, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.50, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.60, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.70, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.80, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -7.90, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.00, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.10, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.20, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.30, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.40, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.50, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -8.60, precision: 0.560 ,recall: 1.000 \n",
      "MEN -  score: -8.70, precision: 0.560 ,recall: 1.000 \n",
      "score: -8.70. Women: 0, Men: 0\n",
      "score: -8.60. Women: 0, Men: 0\n",
      "score: -8.50. Women: 1, Men: 0\n",
      "score: -8.40. Women: 1, Men: 0\n",
      "score: -8.30. Women: 1, Men: 0\n",
      "score: -8.20. Women: 1, Men: 0\n",
      "score: -8.10. Women: 1, Men: 0\n",
      "score: -8.00. Women: 1, Men: 0\n",
      "score: -7.90. Women: 1, Men: 0\n",
      "score: -7.80. Women: 1, Men: 0\n",
      "score: -7.70. Women: 1, Men: 0\n",
      "score: -7.60. Women: 1, Men: 0\n",
      "score: -7.50. Women: 1, Men: 0\n",
      "score: -7.40. Women: 1, Men: 0\n",
      "score: -7.30. Women: 1, Men: 0\n",
      "score: -7.20. Women: 1, Men: 0\n",
      "score: -7.10. Women: 1, Men: 0\n",
      "score: -7.00. Women: 1, Men: 0\n",
      "score: -6.90. Women: 1, Men: 0\n",
      "score: -6.80. Women: 1, Men: 0\n",
      "score: -6.70. Women: 1, Men: 0\n",
      "score: -6.60. Women: 1, Men: 0\n",
      "score: -6.50. Women: 1, Men: 0\n",
      "score: -6.40. Women: 1, Men: 0\n",
      "score: -6.30. Women: 1, Men: 0\n",
      "score: -6.20. Women: 1, Men: 0\n",
      "score: -6.10. Women: 1, Men: 0\n",
      "score: -6.00. Women: 1, Men: 0\n",
      "score: -5.90. Women: 1, Men: 0\n",
      "score: -5.80. Women: 1, Men: 0\n",
      "score: -5.70. Women: 1, Men: 0\n",
      "score: -5.60. Women: 1, Men: 0\n",
      "score: -5.50. Women: 1, Men: 0\n",
      "score: -5.40. Women: 1, Men: 0\n",
      "score: -5.30. Women: 1, Men: 0\n",
      "score: -5.20. Women: 1, Men: 0\n",
      "score: -5.10. Women: 1, Men: 0\n",
      "score: -5.00. Women: 1, Men: 0\n",
      "score: -4.90. Women: 1, Men: 0\n",
      "score: -4.80. Women: 1, Men: 0\n",
      "score: -4.70. Women: 1, Men: 0\n",
      "score: -4.60. Women: 1, Men: 0\n",
      "score: -4.50. Women: 1, Men: 0\n",
      "score: -4.40. Women: 2, Men: 0\n",
      "score: -4.30. Women: 2, Men: 0\n",
      "score: -4.20. Women: 2, Men: 0\n",
      "score: -4.10. Women: 3, Men: 0\n",
      "score: -4.00. Women: 4, Men: 0\n",
      "score: -3.90. Women: 4, Men: 0\n",
      "score: -3.80. Women: 4, Men: 0\n",
      "score: -3.70. Women: 5, Men: 0\n",
      "score: -3.60. Women: 5, Men: 0\n",
      "score: -3.50. Women: 5, Men: 0\n",
      "score: -3.40. Women: 5, Men: 0\n",
      "score: -3.30. Women: 5, Men: 1\n",
      "score: -3.20. Women: 5, Men: 1\n",
      "score: -3.10. Women: 5, Men: 2\n",
      "score: -3.00. Women: 5, Men: 4\n",
      "score: -2.90. Women: 5, Men: 4\n",
      "score: -2.80. Women: 5, Men: 4\n",
      "score: -2.70. Women: 5, Men: 4\n",
      "score: -2.60. Women: 7, Men: 5\n",
      "score: -2.50. Women: 7, Men: 5\n",
      "score: -2.40. Women: 7, Men: 5\n",
      "score: -2.30. Women: 7, Men: 5\n",
      "score: -2.20. Women: 8, Men: 5\n",
      "score: -2.10. Women: 9, Men: 5\n",
      "score: -2.00. Women: 10, Men: 5\n",
      "score: -1.90. Women: 11, Men: 5\n",
      "score: -1.80. Women: 12, Men: 6\n",
      "score: -1.70. Women: 12, Men: 7\n",
      "score: -1.60. Women: 12, Men: 8\n",
      "score: -1.50. Women: 13, Men: 8\n",
      "score: -1.40. Women: 14, Men: 11\n",
      "score: -1.30. Women: 14, Men: 11\n",
      "score: -1.20. Women: 16, Men: 11\n",
      "score: -1.10. Women: 17, Men: 11\n",
      "score: -1.00. Women: 19, Men: 12\n",
      "score: -0.90. Women: 20, Men: 14\n",
      "score: -0.80. Women: 21, Men: 14\n",
      "score: -0.70. Women: 24, Men: 15\n",
      "score: -0.60. Women: 25, Men: 15\n",
      "score: -0.50. Women: 26, Men: 17\n",
      "score: -0.40. Women: 26, Men: 18\n",
      "score: -0.30. Women: 26, Men: 18\n",
      "score: -0.20. Women: 27, Men: 18\n",
      "score: -0.10. Women: 27, Men: 19\n",
      "score: -0.00. Women: 27, Men: 19\n",
      "score: 0.10. Women: 28, Men: 19\n",
      "score: 0.20. Women: 28, Men: 20\n",
      "score: 0.30. Women: 29, Men: 21\n",
      "score: 0.40. Women: 30, Men: 21\n",
      "score: 0.50. Women: 30, Men: 21\n",
      "score: 0.60. Women: 30, Men: 22\n",
      "score: 0.70. Women: 30, Men: 22\n",
      "score: 0.80. Women: 30, Men: 23\n",
      "score: 0.90. Women: 30, Men: 25\n",
      "score: 1.00. Women: 30, Men: 26\n",
      "score: 1.10. Women: 32, Men: 27\n",
      "score: 1.20. Women: 33, Men: 28\n",
      "score: 1.30. Women: 35, Men: 28\n",
      "score: 1.40. Women: 35, Men: 30\n",
      "score: 1.50. Women: 36, Men: 30\n",
      "score: 1.60. Women: 36, Men: 30\n",
      "score: 1.70. Women: 36, Men: 31\n",
      "score: 1.80. Women: 36, Men: 33\n",
      "score: 1.90. Women: 36, Men: 33\n",
      "score: 2.00. Women: 36, Men: 33\n",
      "score: 2.10. Women: 36, Men: 33\n",
      "score: 2.20. Women: 36, Men: 34\n",
      "score: 2.30. Women: 36, Men: 37\n",
      "score: 2.40. Women: 36, Men: 39\n",
      "score: 2.50. Women: 36, Men: 40\n",
      "score: 2.60. Women: 36, Men: 41\n",
      "score: 2.70. Women: 36, Men: 42\n",
      "score: 2.80. Women: 36, Men: 43\n",
      "score: 2.90. Women: 36, Men: 45\n",
      "score: 3.00. Women: 36, Men: 46\n",
      "score: 3.10. Women: 36, Men: 46\n",
      "score: 3.20. Women: 36, Men: 46\n",
      "score: 3.30. Women: 36, Men: 46\n",
      "score: 3.40. Women: 36, Men: 46\n",
      "score: 3.50. Women: 36, Men: 46\n",
      "score: 3.60. Women: 36, Men: 46\n",
      "score: 3.70. Women: 36, Men: 46\n",
      "score: 3.80. Women: 36, Men: 46\n",
      "score: 3.90. Women: 36, Men: 46\n",
      "score: 4.00. Women: 36, Men: 46\n",
      "score: 4.10. Women: 36, Men: 46\n",
      "score: 4.20. Women: 36, Men: 46\n",
      "score: 4.30. Women: 36, Men: 46\n",
      "score: 4.40. Women: 36, Men: 47\n",
      "score: 4.50. Women: 36, Men: 47\n",
      "score: 4.60. Women: 36, Men: 47\n",
      "score: 4.70. Women: 36, Men: 47\n",
      "score: 4.80. Women: 36, Men: 47\n",
      "score: 4.90. Women: 36, Men: 47\n",
      "score: 5.00. Women: 36, Men: 47\n",
      "score: 5.10. Women: 36, Men: 47\n",
      "score: 5.20. Women: 36, Men: 47\n",
      "score: 5.30. Women: 36, Men: 47\n",
      "score: 5.40. Women: 36, Men: 47\n",
      "score: 5.50. Women: 36, Men: 47\n",
      "score: 5.60. Women: 36, Men: 47\n",
      "score: 5.70. Women: 36, Men: 47\n",
      "score: 5.80. Women: 36, Men: 47\n",
      "score: 5.90. Women: 36, Men: 47\n",
      "score: 6.00. Women: 36, Men: 47\n",
      "score: 6.10. Women: 37, Men: 47\n",
      "score: 6.20. Women: 37, Men: 47\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_find_extremes(\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOMEN - score: -5.10, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -5.00, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.90, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.80, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.70, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.60, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.50, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.40, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.30, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.20, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.10, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -4.00, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -3.90, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -3.80, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -3.70, precision: 1.000 ,recall: 0.027 \n",
      "WOMEN - score: -3.60, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -3.50, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -3.40, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -3.30, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -3.20, precision: 1.000 ,recall: 0.054 \n",
      "WOMEN - score: -3.10, precision: 1.000 ,recall: 0.081 \n",
      "WOMEN - score: -3.00, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -2.90, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -2.80, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -2.70, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -2.60, precision: 1.000 ,recall: 0.108 \n",
      "WOMEN - score: -2.50, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -2.40, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -2.30, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -2.20, precision: 1.000 ,recall: 0.135 \n",
      "WOMEN - score: -2.10, precision: 1.000 ,recall: 0.189 \n",
      "WOMEN - score: -2.00, precision: 0.800 ,recall: 0.216 \n",
      "WOMEN - score: -1.90, precision: 0.833 ,recall: 0.270 \n",
      "WOMEN - score: -1.80, precision: 0.857 ,recall: 0.324 \n",
      "WOMEN - score: -1.70, precision: 0.800 ,recall: 0.324 \n",
      "WOMEN - score: -1.60, precision: 0.824 ,recall: 0.378 \n",
      "WOMEN - score: -1.50, precision: 0.857 ,recall: 0.486 \n",
      "WOMEN - score: -1.40, precision: 0.864 ,recall: 0.514 \n",
      "WOMEN - score: -1.30, precision: 0.826 ,recall: 0.514 \n",
      "WOMEN - score: -1.20, precision: 0.792 ,recall: 0.514 \n",
      "WOMEN - score: -1.10, precision: 0.793 ,recall: 0.622 \n",
      "WOMEN - score: -1.00, precision: 0.793 ,recall: 0.622 \n",
      "WOMEN - score: -0.90, precision: 0.774 ,recall: 0.649 \n",
      "WOMEN - score: -0.80, precision: 0.750 ,recall: 0.649 \n",
      "WOMEN - score: -0.70, precision: 0.758 ,recall: 0.676 \n",
      "WOMEN - score: -0.60, precision: 0.722 ,recall: 0.703 \n",
      "WOMEN - score: -0.50, precision: 0.711 ,recall: 0.730 \n",
      "WOMEN - score: -0.40, precision: 0.718 ,recall: 0.757 \n",
      "WOMEN - score: -0.30, precision: 0.707 ,recall: 0.784 \n",
      "WOMEN - score: -0.20, precision: 0.721 ,recall: 0.838 \n",
      "WOMEN - score: -0.10, precision: 0.689 ,recall: 0.838 \n",
      "WOMEN - score: -0.00, precision: 0.696 ,recall: 0.865 \n",
      "WOMEN - score: 0.10, precision: 0.667 ,recall: 0.865 \n",
      "WOMEN - score: 0.20, precision: 0.647 ,recall: 0.892 \n",
      "WOMEN - score: 0.30, precision: 0.642 ,recall: 0.919 \n",
      "WOMEN - score: 0.40, precision: 0.621 ,recall: 0.973 \n",
      "WOMEN - score: 0.50, precision: 0.621 ,recall: 0.973 \n",
      "WOMEN - score: 0.60, precision: 0.610 ,recall: 0.973 \n",
      "WOMEN - score: 0.70, precision: 0.581 ,recall: 0.973 \n",
      "WOMEN - score: 0.80, precision: 0.571 ,recall: 0.973 \n",
      "WOMEN - score: 0.90, precision: 0.571 ,recall: 0.973 \n",
      "WOMEN - score: 1.00, precision: 0.562 ,recall: 0.973 \n",
      "WOMEN - score: 1.10, precision: 0.562 ,recall: 0.973 \n",
      "WOMEN - score: 1.20, precision: 0.554 ,recall: 0.973 \n",
      "WOMEN - score: 1.30, precision: 0.545 ,recall: 0.973 \n",
      "WOMEN - score: 1.40, precision: 0.529 ,recall: 0.973 \n",
      "WOMEN - score: 1.50, precision: 0.522 ,recall: 0.973 \n",
      "WOMEN - score: 1.60, precision: 0.522 ,recall: 0.973 \n",
      "WOMEN - score: 1.70, precision: 0.514 ,recall: 0.973 \n",
      "WOMEN - score: 1.80, precision: 0.500 ,recall: 0.973 \n",
      "WOMEN - score: 1.90, precision: 0.486 ,recall: 0.973 \n",
      "WOMEN - score: 2.00, precision: 0.486 ,recall: 0.973 \n",
      "WOMEN - score: 2.10, precision: 0.480 ,recall: 0.973 \n",
      "WOMEN - score: 2.20, precision: 0.480 ,recall: 0.973 \n",
      "WOMEN - score: 2.30, precision: 0.468 ,recall: 0.973 \n",
      "WOMEN - score: 2.40, precision: 0.462 ,recall: 0.973 \n",
      "WOMEN - score: 2.50, precision: 0.456 ,recall: 0.973 \n",
      "WOMEN - score: 2.60, precision: 0.450 ,recall: 0.973 \n",
      "WOMEN - score: 2.70, precision: 0.444 ,recall: 0.973 \n",
      "WOMEN - score: 2.80, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 2.90, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.00, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.10, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.20, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.30, precision: 0.439 ,recall: 0.973 \n",
      "WOMEN - score: 3.40, precision: 0.434 ,recall: 0.973 \n",
      "WOMEN - score: 3.50, precision: 0.440 ,recall: 1.000 \n",
      "WOMEN - score: 3.60, precision: 0.440 ,recall: 1.000 \n",
      "MEN -  score: 3.40, precision: 0.000 ,recall: 0.000 \n",
      "MEN -  score: 3.30, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.20, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.10, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 3.00, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 2.90, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 2.80, precision: 0.500 ,recall: 0.021 \n",
      "MEN -  score: 2.70, precision: 0.667 ,recall: 0.043 \n",
      "MEN -  score: 2.60, precision: 0.750 ,recall: 0.064 \n",
      "MEN -  score: 2.50, precision: 0.800 ,recall: 0.085 \n",
      "MEN -  score: 2.40, precision: 0.833 ,recall: 0.106 \n",
      "MEN -  score: 2.30, precision: 0.857 ,recall: 0.128 \n",
      "MEN -  score: 2.20, precision: 0.889 ,recall: 0.170 \n",
      "MEN -  score: 2.10, precision: 0.889 ,recall: 0.170 \n",
      "MEN -  score: 2.00, precision: 0.900 ,recall: 0.191 \n",
      "MEN -  score: 1.90, precision: 0.900 ,recall: 0.191 \n",
      "MEN -  score: 1.80, precision: 0.917 ,recall: 0.234 \n",
      "MEN -  score: 1.70, precision: 0.929 ,recall: 0.277 \n",
      "MEN -  score: 1.60, precision: 0.933 ,recall: 0.298 \n",
      "MEN -  score: 1.50, precision: 0.933 ,recall: 0.298 \n",
      "MEN -  score: 1.40, precision: 0.938 ,recall: 0.319 \n",
      "MEN -  score: 1.30, precision: 0.944 ,recall: 0.362 \n",
      "MEN -  score: 1.20, precision: 0.947 ,recall: 0.383 \n",
      "MEN -  score: 1.10, precision: 0.950 ,recall: 0.404 \n",
      "MEN -  score: 1.00, precision: 0.950 ,recall: 0.404 \n",
      "MEN -  score: 0.90, precision: 0.952 ,recall: 0.426 \n",
      "MEN -  score: 0.80, precision: 0.952 ,recall: 0.426 \n",
      "MEN -  score: 0.70, precision: 0.955 ,recall: 0.447 \n",
      "MEN -  score: 0.60, precision: 0.960 ,recall: 0.511 \n",
      "MEN -  score: 0.50, precision: 0.962 ,recall: 0.532 \n",
      "MEN -  score: 0.40, precision: 0.962 ,recall: 0.532 \n",
      "MEN -  score: 0.30, precision: 0.903 ,recall: 0.596 \n",
      "MEN -  score: 0.20, precision: 0.879 ,recall: 0.617 \n",
      "MEN -  score: 0.10, precision: 0.861 ,recall: 0.660 \n",
      "MEN -  score: -0.00, precision: 0.868 ,recall: 0.702 \n",
      "MEN -  score: -0.10, precision: 0.846 ,recall: 0.702 \n",
      "MEN -  score: -0.20, precision: 0.854 ,recall: 0.745 \n",
      "MEN -  score: -0.30, precision: 0.814 ,recall: 0.745 \n",
      "MEN -  score: -0.40, precision: 0.800 ,recall: 0.766 \n",
      "MEN -  score: -0.50, precision: 0.783 ,recall: 0.766 \n",
      "MEN -  score: -0.60, precision: 0.771 ,recall: 0.787 \n",
      "MEN -  score: -0.70, precision: 0.765 ,recall: 0.830 \n",
      "MEN -  score: -0.80, precision: 0.750 ,recall: 0.830 \n",
      "MEN -  score: -0.90, precision: 0.755 ,recall: 0.851 \n",
      "MEN -  score: -1.00, precision: 0.745 ,recall: 0.872 \n",
      "MEN -  score: -1.10, precision: 0.745 ,recall: 0.872 \n",
      "MEN -  score: -1.20, precision: 0.700 ,recall: 0.894 \n",
      "MEN -  score: -1.30, precision: 0.705 ,recall: 0.915 \n",
      "MEN -  score: -1.40, precision: 0.710 ,recall: 0.936 \n",
      "MEN -  score: -1.50, precision: 0.698 ,recall: 0.936 \n",
      "MEN -  score: -1.60, precision: 0.657 ,recall: 0.936 \n",
      "MEN -  score: -1.70, precision: 0.638 ,recall: 0.936 \n",
      "MEN -  score: -1.80, precision: 0.643 ,recall: 0.957 \n",
      "MEN -  score: -1.90, precision: 0.625 ,recall: 0.957 \n",
      "MEN -  score: -2.00, precision: 0.608 ,recall: 0.957 \n",
      "MEN -  score: -2.10, precision: 0.610 ,recall: 1.000 \n",
      "MEN -  score: -2.20, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -2.30, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -2.40, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -2.50, precision: 0.595 ,recall: 1.000 \n",
      "MEN -  score: -2.60, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -2.70, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -2.80, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -2.90, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -3.00, precision: 0.588 ,recall: 1.000 \n",
      "MEN -  score: -3.10, precision: 0.580 ,recall: 1.000 \n",
      "MEN -  score: -3.20, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -3.30, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -3.40, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -3.50, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -3.60, precision: 0.573 ,recall: 1.000 \n",
      "MEN -  score: -3.70, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -3.80, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -3.90, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.00, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.10, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.20, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.30, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.40, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.50, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.60, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.70, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.80, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -4.90, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.00, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.10, precision: 0.566 ,recall: 1.000 \n",
      "MEN -  score: -5.20, precision: 0.560 ,recall: 1.000 \n",
      "MEN -  score: -5.30, precision: 0.560 ,recall: 1.000 \n",
      "score: -5.30. Women: 0, Men: 0\n",
      "score: -5.20. Women: 0, Men: 0\n",
      "score: -5.10. Women: 1, Men: 0\n",
      "score: -5.00. Women: 1, Men: 0\n",
      "score: -4.90. Women: 1, Men: 0\n",
      "score: -4.80. Women: 1, Men: 0\n",
      "score: -4.70. Women: 1, Men: 0\n",
      "score: -4.60. Women: 1, Men: 0\n",
      "score: -4.50. Women: 1, Men: 0\n",
      "score: -4.40. Women: 1, Men: 0\n",
      "score: -4.30. Women: 1, Men: 0\n",
      "score: -4.20. Women: 1, Men: 0\n",
      "score: -4.10. Women: 1, Men: 0\n",
      "score: -4.00. Women: 1, Men: 0\n",
      "score: -3.90. Women: 1, Men: 0\n",
      "score: -3.80. Women: 1, Men: 0\n",
      "score: -3.70. Women: 1, Men: 0\n",
      "score: -3.60. Women: 2, Men: 0\n",
      "score: -3.50. Women: 2, Men: 0\n",
      "score: -3.40. Women: 2, Men: 0\n",
      "score: -3.30. Women: 2, Men: 0\n",
      "score: -3.20. Women: 2, Men: 0\n",
      "score: -3.10. Women: 3, Men: 0\n",
      "score: -3.00. Women: 4, Men: 0\n",
      "score: -2.90. Women: 4, Men: 0\n",
      "score: -2.80. Women: 4, Men: 0\n",
      "score: -2.70. Women: 4, Men: 0\n",
      "score: -2.60. Women: 4, Men: 0\n",
      "score: -2.50. Women: 5, Men: 0\n",
      "score: -2.40. Women: 5, Men: 0\n",
      "score: -2.30. Women: 5, Men: 0\n",
      "score: -2.20. Women: 5, Men: 0\n",
      "score: -2.10. Women: 7, Men: 0\n",
      "score: -2.00. Women: 8, Men: 2\n",
      "score: -1.90. Women: 10, Men: 2\n",
      "score: -1.80. Women: 12, Men: 2\n",
      "score: -1.70. Women: 12, Men: 3\n",
      "score: -1.60. Women: 14, Men: 3\n",
      "score: -1.50. Women: 18, Men: 3\n",
      "score: -1.40. Women: 19, Men: 3\n",
      "score: -1.30. Women: 19, Men: 4\n",
      "score: -1.20. Women: 19, Men: 5\n",
      "score: -1.10. Women: 23, Men: 6\n",
      "score: -1.00. Women: 23, Men: 6\n",
      "score: -0.90. Women: 24, Men: 7\n",
      "score: -0.80. Women: 24, Men: 8\n",
      "score: -0.70. Women: 25, Men: 8\n",
      "score: -0.60. Women: 26, Men: 10\n",
      "score: -0.50. Women: 27, Men: 11\n",
      "score: -0.40. Women: 28, Men: 11\n",
      "score: -0.30. Women: 29, Men: 12\n",
      "score: -0.20. Women: 31, Men: 12\n",
      "score: -0.10. Women: 31, Men: 14\n",
      "score: -0.00. Women: 32, Men: 14\n",
      "score: 0.10. Women: 32, Men: 16\n",
      "score: 0.20. Women: 33, Men: 18\n",
      "score: 0.30. Women: 34, Men: 19\n",
      "score: 0.40. Women: 36, Men: 22\n",
      "score: 0.50. Women: 36, Men: 22\n",
      "score: 0.60. Women: 36, Men: 23\n",
      "score: 0.70. Women: 36, Men: 26\n",
      "score: 0.80. Women: 36, Men: 27\n",
      "score: 0.90. Women: 36, Men: 27\n",
      "score: 1.00. Women: 36, Men: 28\n",
      "score: 1.10. Women: 36, Men: 28\n",
      "score: 1.20. Women: 36, Men: 29\n",
      "score: 1.30. Women: 36, Men: 30\n",
      "score: 1.40. Women: 36, Men: 32\n",
      "score: 1.50. Women: 36, Men: 33\n",
      "score: 1.60. Women: 36, Men: 33\n",
      "score: 1.70. Women: 36, Men: 34\n",
      "score: 1.80. Women: 36, Men: 36\n",
      "score: 1.90. Women: 36, Men: 38\n",
      "score: 2.00. Women: 36, Men: 38\n",
      "score: 2.10. Women: 36, Men: 39\n",
      "score: 2.20. Women: 36, Men: 39\n",
      "score: 2.30. Women: 36, Men: 41\n",
      "score: 2.40. Women: 36, Men: 42\n",
      "score: 2.50. Women: 36, Men: 43\n",
      "score: 2.60. Women: 36, Men: 44\n",
      "score: 2.70. Women: 36, Men: 45\n",
      "score: 2.80. Women: 36, Men: 46\n",
      "score: 2.90. Women: 36, Men: 46\n",
      "score: 3.00. Women: 36, Men: 46\n",
      "score: 3.10. Women: 36, Men: 46\n",
      "score: 3.20. Women: 36, Men: 46\n",
      "score: 3.30. Women: 36, Men: 46\n",
      "score: 3.40. Women: 36, Men: 47\n",
      "score: 3.50. Women: 37, Men: 47\n",
      "score: 3.60. Women: 37, Men: 47\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_find_extremes(\"LDA\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
