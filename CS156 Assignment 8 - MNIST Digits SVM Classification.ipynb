{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification of MNIST Digits \n",
    "## CS156 Assignment 8\n",
    "\n",
    "\n",
    "Load the entire MNIST digit dataset: http://yann.lecun.com/exdb/mnist/. \n",
    "Choose two digit classes (I'm choosing 4 and 8) from the training data, and plot some of the examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import mnist\n",
    "import scipy.misc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" I'm using the MNIST python library, which proved to be a much more convenient and tidy way. \"\"\"\n",
    "\n",
    "# Loading train images\n",
    "X_train_img = mnist.train_images()\n",
    "y_train_label = mnist.train_labels()\n",
    "# Loading Test images\n",
    "X_test_img = mnist.test_images()\n",
    "y_test_label = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Choosing digits 4 & 8, Slicing the dataset to only those \"\"\"\n",
    "# np.where gives the indexes of where the query is correct; \n",
    "# i'm querying where the labels are 4 or 8\n",
    "\n",
    "# creating a list of locations of chosen digits. This location is valid for both X and Y\n",
    "chosen_locs_train = np.where(np.logical_or(y_train_label == 4 , y_train_label == 8 ))\n",
    "X_train = X_train_img[chosen_locs_train] #Slicing Training Set \n",
    "y_train = y_train_label[chosen_locs_train] #Slicing Test Set\n",
    "\n",
    "chosen_locs_test = np.where(np.logical_or(y_test_label == 4 , y_test_label == 8 ))\n",
    "X_test = X_test_img[chosen_locs_test]\n",
    "y_test = y_test_label[chosen_locs_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training and test images arrays: 11693 , 11693\n",
      "length of training and test labels arrays: 1956 , 1956\n",
      "Test set proportion:  0.143307201993\n"
     ]
    }
   ],
   "source": [
    "# data length verification\n",
    "print \"length of training and test images arrays:\", len(X_train), \",\" ,len(y_train)\n",
    "print \"length of training and test labels arrays:\", len(X_test), \",\" ,len(y_test)\n",
    "print \"Test set proportion: \", float(len(X_test))/(len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAAAAACi5bZQAAAX/UlEQVR4nO1dbXsauQ6V5JeBtP//\nd95NgPHYlnQ/yB5IN9MCgQS2c5LNs7ttAjkjybJ0LKPCio9A3/0GHhUrMQtYiVnASswCVmIWsBKz\ngJWYBazELGAlZgErMQtYiVnASswCVmIWsBKzgJWYBazELGAlZgErMQtYiVnASswCVmIWsBKzgJWY\nBazELGAlZgErMQtYiVnASswCVmIWsBKzgJWYBazELGAlZgErMQtYiVnASswCVmIWsBKzgJWYBazE\nLGAlZgErMQtYiVnASswC/He/gZtA26cqqKoqKGAHAAK0LxfgP0GMCqioqipL+0BHzoCIgICXUvNf\nIEZVRUVUhGuttdZSK/oQQvAhBHdqN+fjv0AMqIiwCEvNOZecc8kwDEMchkFAkZAQ6UJq/ivEVGZm\nLlOapjSlKeF2s91uqigBKQGS/p2uxMKVa82HdBjHcRxHfPnxI1cB8qSqSoB/qStxLVzKNB72+/1h\nv9/Tz5SrKPngnAIA6oVnHP87xNSayzTu33a73dvujVJmAQpx8Oqu4OU/QYyqMnMtJadxv3t9fX19\nfcWpKpCLQ4kAiIik+ncG31pynqZx//b6v3/+97//URVAFzbbUgGMl8t+5nnE9Myy/TsAWMZ0eXpw\nF6iIcK2l5ClPU0pjGilNOZfKLCJk2fBlOIsY7ej0KAAhIllW+e1QFWauJZfKqojOBQreOUeXrkVH\nnEOMqtiHKDRuFKnhIXhREeZaa2EWAHI+kPfOEeHVzJxpMSwiwqJtnwaKzpFz7hHs5cSVShVRRPLe\nee/dZ57becSIsDAzN39SVfTeKQDSZbH+PmiuVM2VgJwL7mtcyXLu2phRVSXmoIAi7tpXvh0UVIVr\nLbUyCyCSD6550tUmfbbFcK21ctvdi1IQAKKLV8G7oFtM+WKLsReuNVcWFdvkkygAkXsEXizGcIsx\ngOi8918RYxovpZSqIqoiKk4ByTl5BGJsVaq1lsIsCkQ++C9aleyB5CpGjIhTJPIsj+RKp3mM/xJX\nAhGpteZcRViMGCByNYhc+8I3hfkS11qZRVQRrGr3iWziMospRgyLOHTOh8ewGFsPhE8gRtDlO4EZ\nZ8aYthUpLMLKwuKd94H5IWKMBZn3zKCIyCd4OXe5ZuZScs69CG/E1EcgRmF2Ja51JobZmLn2DZ7t\nSrWWkjMzizCzeBdCrQ/lSicWUyvyp2i5wJW41jxllsrCLDX4GAszf2vw7b+2pVY8x19mdiIqItA7\ncBfjQoupwlW4SuWYy+NajDB/hcW0LUEpeWLmKpW5yjTk8hAxBsBijJzGGGH5XIi5IPMtJedcmStX\n5iqD1ce+nxjtqe+7VYla8IW7B1+zGGOlcpVhW8pjudIJLbXSV+QxIMJcSp6mqTZXYupefO0L3wRW\nZtW2UcrTVGqtLEetQ6sy4uV7pvNcibmWPKUxtQTvEeykqT5AtT21NB7Gysyi6DSEEELbZF+1mzw3\nwWvEiKqoPkBkAQDo1USuteQppTSOIiICSD6E4O2Ld1cxc5bFiNRixFglHK4OabdFqyayNZXG8XAA\nUAVAR977TxnMmRYzuxI0Fc4DFHpbzFVtATClw2GcdVQhhM4MObqTK6kI1zxNaUyESEj4CMo9BQUV\nVVaptZRpSuMhkSPyiI5C8MEbOUR4RSnvQotxRI4cwQOUwLvFyGwx42Fy3nvCFmOaxSC11uBFODPB\nq40Y75x36vFhYkwTmJWcp5TGFEXROXTe+xBanDmRKV6Ac/OYFnyD9yqA+AgW03mRtiqNh8MkCuRt\nVercOAuL985jgoA+TNNk3jtaHpMOWQG9KDp/ksfcUc4694YrITE9Rn4H1rvgymlMaZqmnEvxrApI\nrqUvjoiuXCieVx+jUmsupZSye9sdxqlU2wpQF/h+qnvyzMRwzXma8jTt3vaHlEtlhc7Msat0175S\nfyvfvWd8B5VaUkppTLvX3X5MuQqAbRudda6J7t5XekSo1JzH8XA47N52hzGXKmrMOOdc3wlc/ePP\nJGYubeinihw3BXNJaX/Y7d/e9oeUMrP2UoP3zpsr/aUWM42H3dvb224Ovl9uMQ8IlVpyOuxeX9/2\nFnw/jDF/YfDlmtN42L/98zaOhzHlytazfr8qXYsnt5jxsHv7521KKeVSRfFr8hhtX8Qwa1ofBNbs\nmlJK05St1AuI5JxtCO4nA7HCIaiW3t6zjsSD7AjgRH/cnxgiOedDiMMQQ/DO3UVR1V5OtJbK1WgR\n7Vrf70c/ANmUpAAKgETO+xDjEIP37i7BV6GrnkutZjPCYiK8K1/s1jBmzIrtYSGScyGEIYYQnLuP\nBq+rTqSeutID8QLvLWZ2paPF3MuV7KAhl1JqPdJioefb0QkxB2+KdSRyxkuM4U6u1HTP7WTqicU8\nSFcJ4H30vbHF/K6MY51yU1yb7G9+OI+BY/TtC4JZzLwqfSLGLBPT6obtHHNt4gqLvQ/BTF+V5hTi\ndL3uq/W9XMmO1L3PYx4iwgDAqSfBv/KYcMc8puvQcyml1jonv9e+1s1wUgCR4/MCmDPfuTd7j8z3\nKIqZplxMJfQIxPTe+Xtdr3STsYKMc1eqP2b8wWJqyVOecim1HkVt30qNdvkHn0iFLO8EsJovknPk\n6HMnE39LjLX47NilWUxj5VuZUZB34rLKpzLwpha6qpF/ijNcKTdX6i/+3a7UF6N3niTSaZl5uVtp\nU1W41Dwl46Vyyy/hO8kxo1UVZZsAwtVODqiAAgIgIZIj56yVf/ULXRB8H8Zi2qne77OYdv49v4sx\n381LO+0sx2MVprs7hpjTVen6gzm/D77Wy3+3KpkpX/lqN0CrEb3X9cpR02urEs2r0rX43Zbgl+D7\nKHmM9nrIt+Ux/UCd0WKe9En17OehIlxZKid7Xv2J9dbJPELgc6v1H8sOXaAv/cXnnNt9ao92LVS4\n1lxrqfvd/jBOU5OnIxKAztqPq0RU7/DbLsEsRD8J+kTGjHOfzqGugUotOecyld3b/jCmXCoLKACC\nQ7AGZAsviFcIqWb8brlux5lrO4Mp3V5d26N9qgV6LYRLTtOUpv1uf0iNGFDT2bbHRZ8MvABn1HyZ\na7WoD8fW8Imw+FOvfjmsMZvSIe2OrjTPsvkqV5ojf2VRO0ncWn0hOP8NjgSgzGWaTPyxH8eUcxUx\npyHAuTXbV4a7tGhPLGaueDyIxYyH3f7taDFkzwvpC4OvxRhhkWNeeRpjvpwZYZvR9bY7xhhEACTT\nxbi2H2glh7sF35Z1C2s7Af/LqnTt614LlVomk8XsD31VQlTAeezo0WDu6UqWx5jFPEge0yzm7TDO\nrqTNx7/OlZrZHEvgRNYdDjaF5I6nLdqG9XgQSEFB03gYD4f9Yb/bjynlPmOInA/Bhc0whPB5bQzA\nxfqY28ksfofOyQyxSoOKpn/++ef1bbc/jK1MJKpILoQYY4g/f7xsh6G1IBG+Tjg0x15T6d9pwZ7t\nRFp1QbnPpZX0+vr69rbfH1KaplxYxJTgcRiGYfj542W7iTF0je8nDlZdajHUOjfWtblL9O0z5U7m\nOc91l5p2r29vu93+cEg5W1NHEZ2Pw2Y7bH/+2G42R4v5DK5xpWYx93OlVsAUMTLatqTUWmva7d52\ntlKXkkttFhPCsNluX37+eNkOMXhHBF/qStYcds6OdrjPTA1bRBcBCQhXrqXaFKiG8bDb73eHw3iY\ninXVVYGcj8N2++Pl54+X7WaIwTvsrZOvUG3CF1qMgoKIDeQqNXcUW5LGw2HMTWlwtJgfP361mHtP\nHJrRqg6+W8ydMrw+R5+51pJLKdM05SnlaZrSOB4O4zgeUraxQiJqwXfz8vLz54+XzWaIsy1/YfBF\npKPF3GdVmkWR1gnNOecppSmllKY0TuOYxmkcx2rrlR0n9nHYbH/8/Plju93EEBxdOjv9X3jMPKYR\n09o3eUrjmMZxTGMap2mcUk5pqqoqNt6yu9LPny/bnsd8MTF9aMIfc+5/N1r05PPjP9RZbmjJXJ3y\nNE19RG8ax5TSmNtZtsJ9hAEAOed9HIbNZpjVmhf+Xv/CpcRYGqryh47B7A7vvnNWs8y5yvyH0Icj\nqKiK/VfNJeep5Nz4sWMU0J9O/87jBu7o4Bf+Vh/gQmLaQ7X3/5u2ZE/kT5jpbbKmyZoljvY3TB8q\ntlsVVVWBWpo4p+SSS5kKs9U+AAARm7gMrOTg/KmDf56ZKyymdwJ/02HSmQaYzcKazSzCJ6bTqVHl\nJtqqjRgQ5VprqaXndrXWWRfZD8bO0g9qFuO8I7zFWnnxIYt5x/0HV9I2DelITOV230SzN2lOBQoA\n7daSWiubzAME+hAf5to3B92VusV0s/luV7JfpY+ubcqHD/+itE1O/2MFYRN/mmbNROfzDHKVObfl\nviipiAkZWg+0NxtP57g3gQMhOe+C98E7cnSLfPwKi5FzLYb5OA5OQarl8KU1Hbo9GTGcS8tta9ea\nqFqiIiIKCKjz7/urxTxA8O0zOP6o3ux19O5KCmYTuZbMIsIqLHJcoOo0ZVuaK4DY/50PAikQoesz\nPSz2Gkldkfi+GPINMUbn9aar5z/+a/PZfjkS02/4ycxsEzv5OE6pWnI7pVRnKzpmQ+h8O+cI8MsN\nW0bTvFFpV01906p0Th7TFAGnxExlmnKecmVmVmZmnZO0MqYxjWMaU5m/Q1upCZF89MGrtsE1v+Yx\n3x98QYW55GlKDvrS8tFPkFJsX3xCDE85T3nKeao22JCZjzGmpjGllNI0FcAWPBCRAIEQnfch+hhi\nUERV4ffegq0S3edh36IS/Xti8BTQmhfpEDzWadhsxmEzbM4l5sSVhC3GcH8ZUGYFckEUA6Llb4Bd\n1YHex/ahY3IOQer7zBdPmzo32dn+jpieYvYeDQIIl2n0nqCmYRg2cRM3H42SkVJLqb8QU3IrOB1X\nJYDuLSyK5IKiY0IgRAIkU0aRI+ejDz6E4CUGItCaCRsrcNof/fQZ0bOIadycvIxyLVNwBJKHOMRh\niEvEfGAxLYMt1XYGKiqA1l1FUFZAp+C8tJlS/TisnS8P7aN6hyA1e8J5te6+5G5oMH9ypVNvguZK\nHlE5R0MY/khMo8YuE2Ou1faJqiqIhERISMoK5JDEg+uqS+d8+/TOhuQEVx2q1DI5RMvvTlzp6glm\nlxHTOzNzhRABVTg7AuWSYggxhhjiR5HuI1dStnGyLNJ23qpIjpDIEaIqEqq3/9csxXe4dpmJ9xlB\nmtUiqH3aJpKcc1dOMLuMGOPi3xZDoFxzDMHH4KMPfySmUyNsDSI7h4sKqqYidI6IAIEsv7fRDN45\n74NvI9tm28kgJechOEQA/JfFXDfB7HJi5hjTZ9lLNV5SnOeGnWsxYMmPySbA7pIh8s45ai0qasuu\n61YSYvv0zpM1piepeUpD8IQKfVtwlBqcPsX7EXOyKoHxokxFuOT+zr134aP38KHFwHHX2beB5IJz\nzotz3iGh842VNpMsxBBjCDE65xx5cs6NNU/jGIOjVpCxhMeWrmsnmF1KzId5jNZKzp60d86TP5cY\nM3wENQMkQATnqveexSuAA3LBBx+DDyH6EEKIQ4hDDEP05Ig8kaNNSeN+PwTXW7Dv85jrJphdSIw1\nY2McipVEuLIDQkQVsIGFztVlYqzIJICACn1CJyIgISJZqnIMryEEy219DMH/SowjW70Ijzqufj6p\nH8Sh24ovfrMqkQ9DKcJgN4l471zVbqs2WtI5dB8R056jI6eIhHBSPUcgJCJw9mt68x7n23BMy1ba\nlxBtnJ0jBGUBQBi7ysGKVgik2LVdN+1Y/IYY58LArErHMadlriLMRzyWiEEkYvJ6VGtjuw/Pvqtl\nKq6NB2p2Y/mK86GHmqYe6LUrHdOUc26qZ0UABL1hWncOMUA+VuEmhTRlcRFQAQGVbr0LFmP2zY7B\nmWX1gzJElqegJ+ewZ7bkmt20j5bWee998M47FG1ikMOY0pTtkJCtlKjtR9+Ulz+5kio5Mxhni6Uq\nW1GSEB3SwgRbi4YszOh77uq6mZidePKOXNsKEbmmuJw3Ac5RS2acd6CgLJUrj2NqZxtso2Uh/Mtd\nSQC9j9F31d9ghVwRhp5KffiklIlJnIigDz44H3wwq7Ff23sbPOaOjtYYmslquazxpKIipZZaDiml\nnGutIojaun/0yYOhFxJDXoBcCCV6S67IpS7gYUWE3zwmVLJyN4Vg62/wsymcpPrtkm7CVrqkI1M4\nRzEiQVDrY59YDClCPzP7lRYD5IF8qEMdWoAkiqW3ObQnxB++HaXekaMYYwwxxhiMXUv1gws+NBlL\ne+zYSMKjLVLfY2IFG0I9jWmaWvA1QXhfwr8wxjhAJywik3P2VCiVWkoptZDgSR/j38TMF226odUn\nBhv47r13c55yzFSPG9b+9ZhcWh4nXEtO6ST4kvaho58/nXQRMYStAZ27epZCLiVnX4h4zi8/JAb6\nsu43w2YYNsNmiJYMBe9CiFanDO6YqZ788/5fEQDUXGlK40keg0cJ/9e60vxC2A+Roc8155JLLrz8\nfe9//mDMbIZ4zHJ9jCEYMecBtV+NkKaplGJ1dEQi771rNfAvI+b4tsj5MDAruNLlcOfeq/TOlVwv\nrFyurZF2N0LOtVQRAUWkVtjbtGExN2XmTGJCZBbAYDsgLvVci3Gxw8/B1/lgbbGzfxGd70Cf7Hym\nqj0vH2MIm2GI4TMjUT7CWe0TdD6IArpc7ebVWs+2mGCIwVNPc23fddEDVpFam+7MJrYA2LmpGOOm\nq4XO/3l/xtmupIDkY09j6rlnjG2jFVq3vVX9ey3//LdperySpzwPDTA1YIjD8G2uBOS8IpIPtWW+\nfPbFmc08fC/SUdsUkCM6P1NVWXClEOMwDIMl5zc98XGuxdisf5aOcy2m7xNb+b4nbXRRiIHF4OtD\njJvvDL5AzvcBNheN23x3PrxluHOae/7bnO+rsDET0u5O9iEOmyZJ/J7gi+TkpJP/WwXI+289UgEn\naW5Pcc+EqnJtrpTbxJZeYRw2JpL/luAL1OvYJ9qM83Cycfg1y/1c8CVsI+++z5Ue4D4l1fdTAxQA\niMj7aDEmen/j43aPcFPSdWiDWG1VCuHWe4LnJQZ6HrO5S4K3ErOA5yVmdaUlHINvXIPvCVaLWcIa\nYxZwX2Ke9yaLU1cKqyud4Cj+uEfz+omJuS9WYhawErOAlZgFrMQsYCVmASsxC1iJWcBKzAKemZh+\nCvEu9448MTF6MgyU5dbUPC8x2jsHJ0N1b4jnJaZd0zgPU7/xHWpPTExvNdU+f/+mP/0ZiekTDrSb\nTGW5OTPPSExDD76rxfyKd23bW0ff5yVm9qTjqrQGXwA4NZg1j/kFa+b7DViJWcDzE3MHNwL4LxBz\nJzwjMaZxtKkXzVhubzbPSMyXYCVmAc9PzBp8vxbPTIz221luX6Z6amLaXT6llJNbLG+G5yWm3f7U\nBqXxWtqc0S2m33t69lGhs/DMxMjqSh+hlWNKzbmW2zdQnpeY1WKWoO3YW5mvEL7lT39eYlphs5Sc\nS2mXwt7wxz8vMWses4g18/0OrMQsYCVmASsxC3jG0yfaB5r2uZL3wGoxC1iJWcBKzAJWYhbwJMRg\nv4hxvrX53q/4JKtSuwE3xoE5lPvd3HnEcxBjlyaHOGwKc7n5rJiP8BzEgN0LE+KQawneEyHeLYMx\nPAsx1CZ15VLyajEz2t3AIQ45Zx/sYr92M9ed8CyrUrOYOMQ7zKP6CM9hMW1cQRhqGaZ2p+oaYwDa\npckhlpqjXWe9Woyh5zGco83jX/MYA5HzoQ6sZVtKLpWZ+WX7sn3Zbvt8wL9yGgiicz6wCAoroAvD\nsOXtZrt52Ww3W7sxPvibMvMcxACRD6wKpILkwrB9+cmbYTNsh82w2f788bIZov/w7ohr8RzEIJLz\nERQJgJyPm5cfP6WPlo7bl+3LZgi39aXnIAbQOa8A5Ih8iJuXMY0S7fqlGIbtZrv5O10JibwlM86F\nuHlJeUrqQ7tiJw5xGIbozx3NfhaegxhApwDkHIcw5JJzyUXne2R8iCGGv9Ri1CMJB+ZY29Ry7bfw\nzBfr/IXEABK2y1RE2D6hj5PGdoXMx1c9Xf2Kd95y3Ah6/Ghzlttd6e+vCLnhKz4JMV+PJyk7fD1W\nYhawErOAlZgFrMQsYCVmASsxC1iJWcBKzAJWYhawErOAlZgFrMQsYCVmASsxC1iJWcBKzAJWYhaw\nErOAlZgFrMQsYCVmASsxC1iJWcD/ATwp9Uq0RqN7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280 at 0x1133B6410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" plotting examples as images \"\"\"\n",
    "def print_digit(i):\n",
    "    print \"labeled: \", y_train[i]\n",
    "    return scipy.misc.toimage(scipy.misc.imresize(X_train[i,:,:] * -1 + 256, 10.))\n",
    "\n",
    "print_digit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAAAAACi5bZQAAAje0lEQVR4nO1d6UIbubLWvnS3ycw9\n7/+K5wTcrV2q+6Oktklw2AkJ/iCZCQnG/lxVKtVKgVzxENjvfgKfFVdiLuBKzAVcibmAKzEXcCXm\nAq7EXMCVmAu4EnMBV2Iu4ErMBVyJuYArMRdwJeYCrsRcwJWYC7gScwFXYi7gSswFXIm5gCsxF3Al\n5gKuxFzAlZgLuBJzAVdiLuBKzAVcibmAKzEXcCXmAq7EXMCVmAu4EnMBV2IuQPzuJ/BegIGzr9GB\nJ3z/30oM1FprbbVWIIQQIIQQxjlnnHPOn8DM30tMLjnnknPbv8akFFIKSdiXJqbEmGJMsRLSJUYo\nrbXSlD3lRf+9xOQYfPDBF0I6MdJYWxph/CklvH8tMS0n7za3uUxIJ0ZNuTRChfrSxNQcw7auxzUR\n0onRqTbCuKpfm5iSvFvvbu/i/jVdgDAhyxcnJsfgjrffv4f9a6YSKpTOX54Yv6133//rCemqNAEX\nSpsvSQwAAQACEEPw3rltXT0llBBCCeUp5VJbe1Jfyd9FDLRWW2utNXdct837kFKmjDLCKGWcc86e\neCP464gptZRaanHHdXPOh5gy44wwwhgTnDPGnsbLX0YMaaXklEvK7nhcN+dDjEkAJ5RQxjln7MnM\n/F3EQKspxZRi2o7r5rwPIWUglBFKGRddlZ70UH8ZMbWUFEMIwR3XbXMhxlQIpQwI44ILVKUvGHaA\nVnMM3nk/JCamShiHry4xpJWcgnPbth3XdXM+pFSpqA0I++I2puQU/HZct7WfShF4EY0QyvkXPpWg\n1RSD39bjum1u8z6ERLisAN2N+WJ+DED/jCEE77ZtW9fuxJRKAAhlTAgpZbcyT3nMv4EYaAPheDyu\n67Ztm/Mh5lwbml0upFRaKSnEU5n5O4ippZZaS/XH47pu67Y5F2JKpTYglDHOhZBKaSnll5IY0kou\nOedc/PF4PKLEpJhyKRUIHXqkUGLYU0Lhfwcx0GrOMaWU3Hpc13XdNudyyulMlcTXVKVSYowhRHdc\nj+u6rW4LuZRcamsEPRj5FVUJWs0pBO/9tq5dYkKptdQvLjGk1Zyid5vbVjyTti22CrVVPKqZEFKq\nr2hj0N9d94/NRQBopMFJYuQXVaXo3XZcty4wW9r/9gv7MQRqLSnF4J0PIcQYU86UUkIpJYwLdGK0\n1lpJwfnXUSUC0Got6MqUWhtqEKWMMso6KcZYo7WU4gtJDAC0VmvJOedca60AQAhlCKW00toYY6xS\nSoovJTHIS0kpl1Jqaw2AEMY444wrpbTWRhtrlFRCcPakKrK/gxho7SdVYpRxJjiXnRljjJTii6lS\ng4qqVHKprTYgpPsvXA5arDVCCPG0eqq/g5iHJebkv+wSwwUT7AtJzDiVSkq51FIbNCDkJ16sZYwx\n9oVsDEBrrdacc8651FobdOPb4zBKa6ONMZQySr+Q5/uwH8O6x3vmxxBKKKGEfF5Vgv0To7UESPdU\n8e180nsK+BhASskYqMq51IpHNcFIg1FmssZovA485yn+FmL24mSM1kJrjXDOGOdnpvExcgCg4acf\n14Bcau3XRiqV1lZbY5ZltkbJJ/p1O34HMQANP1qriFKpEEIKIYRkgOUs8CgzrbbWamvIS0q5V79Q\nxoFJra21k5nmZbJaS/EHEEMAamut1VZLLqXkUjJVSimpFGGEdmYeQxukeh9CTCnn0suCKONcKG0m\nO0/TLjHPe46/h5jWaqu11pJTTinnlKgxWhsgjI+E2OOq1GoppZTSVSnlXHpoCohQShs7zfMyL5M1\n6o+QGABordRaao4xppBiTNRaW4BQLoEQYJQQeIwaaP3eGEJAXnJp0AhQyqmUylg7L8syz7PVWj7R\n4d3x2ySmllJKCiEEH4OPdM6lUcZl5ZQRIE+UmJxSTmcSA0CAUAZUSJSY5Waa/hiJGX5HKjE47733\nztNcG2Vc6NZYo49KCyGEtFZLzinGEOKwMfg3lBIhtTbTtBwOs7H6D7ExJ38sBu/c6tzmWG1AuVS5\nQmMECNDHT6VaS04xRO/DOJUoJRi4E1Jpa+d5uZm0Vn/OqYRxpRSD37Z13Y4ba4QKpVOpjQEBAo/z\ngqoUoz/zYxhlhFLKqBCoSofDpJRQf4Yf0yvCYkhuW3sqiEuljInoojHCHieGtFq6zHmPqlQr4ZRT\nzjmzA5OVUgjxXNv7e1SplRx9CC44t25ucz5EjsdKzllyAEIoPFamDK2WHIPf3Lo5H2POtQJlXHIh\nhZiXebbWaLXXfvwJxNSSY3Bu885tzjkfQhQp5ZRzLqUAB0Iefx2YNAnejUrnUhoQJqRUSqr5ME+T\nNVqhuLBnM/ObJCZF79Z127z3znsXYk1oPnMpQAilDB5Vpi4xbt2cC6MahnGpjDZ6XpbJGqOVlFh7\n90yB+X2qFNx2PDoffPAhhFh3VSqEEkobe4oqdevtnI8xlVIJZUJoY62dl3nuEtOvpn+AKpFaU/Ru\nPd6tMcQQQ4hBpZRSSqWUQgih7FETQwBqzik4t67e+5Byro0SxpU20zQtyzxZq7WSsoft/gBidom5\nXVOMKcaYIpwkhlLKMAHyyKPsEoN3glIqp0xIbaZ5WeZdYhijzzcxv/FUcuvxbk0ppZxSSuRkfJ/G\ny7mNiSHFlHNtjDAulZ2WwzLNw8ZQShml9GnRrx2/6VRKMbj1eHvMJaeSU8nkZHwpY4w3eIya06m0\nDjXEU0kbOx+WydouMT2J/edIzHa8XXPNGDogJ1Vi9Sm8nPyYbc05l1RyrYIyLrSZ5sNijTXoxxBC\nnhbguYffdLvGa06MuRbsMBIYdcKq1B9nMtz/5v4LM0kpxhBLz8wCYIxKKqWVklIIzp97eRz4TVkC\neh/9q6dY8GVxgVHtnHMppQw+z77nKY/yOH4PMfScGtLFHIDgqyKkd3c+BIyhA0AuuZRahpzdY+HR\nR3kcvzGvNGg5V35MiAxteQBAoDVoDVrJBe3TfZEBeMKjPAG/V5VI/61/ddeAX6oS2qF6YuWCKv3q\nUR7H7yBmD3dT1lODhKLxICP9hr89BGjQWq21odNTCvKC2QFK8bsefZTH8QmM7/MlptZaziWm/SUS\nQwg5UUMeNDO/ALTWaimYksLuCvjR+D7+KI/it51KJzszADDe7f6HhwAAUGstpeSyH9jQ4L7xfexR\nnoBPYHyf68egKuUzP+b+9/xxfszeh5Z6OrU2aIA2k93H5f48AAyj54RJtlJ6kSZhAIRzzrlAvCRs\nd4aPI+bUh+bWzXkfYxrVCYyDEEIIKeXoz/upvKcfN5hkSzHFbfT2AZBxRVSjFsb0aO9TW0N/xkcS\nU2uptZa6bRjnzb1vhjJOsGVRCAzpM/6jzAxvDWrtObZt8yGljI9A0Ip3XrSxBsvAnx3R3PGBqtRK\nySWXXLZ1dZjwKA2wnoV2+ZdCDIm59717pVFrJacYvPer8yGkXGoDSrCEjJ4VIqpnlMc/hA+VmIxB\nqW3tqaBc0EIyOlRpSMxDqgQEMDqVYvDOYbt5LhVgGCl6pkpSqj9HlUoOMca4riuOXcgZr5OEkM7J\nLjGU3fdt9o9acsbepM35kEtpjRBKGeOMc6W0UsZoY4zsBvilT/eDJSYGH/w6xi7ksrt5T5EYIHCS\nmO2e8aWMcy74mcQIHHLxB0gMaaWk6L1z+8ibXBhjlFBG2Sg0k1L2jvofQ25YythOAU13Mr6EMS64\n5KeaXsPZA6bqGfhwiXHbdlxX50OIKReOvh7n40DCovafPJDhzxLoErNt27YbX1QlgY2PXWQYY/QF\nebYdH02Md9vxuLrhxxBCezjy5Mf0NNCPAoOu7LkqhWF8RxebOFOl3bd+4dP9CGLQN2un99q50HPN\nQBkXUgqJxbhS4sSOn1zWEZ1qIYTgnXeuPwamZTnWf8tlnibbcwOj2vmFeHdi9rjIKH/yzvsYuwpw\nIZVSSqnlsMzWaClQjX7UpFZ7SMrh8IZuvGsFQhmV2hhjtdWHf74dZqslf8VdoOP9JWZMphuVQt67\ngAlVIJSLXue/HOZpJ+an/Bi0fpvuQ7mc8z6kXApW9UptJztNkz3c3Bxmq6VghLxKXj5ElTAkALWU\nlGIIvg+kKBVIz6haMy2HZbJGKcEf4IWQVtE3TBvO+3DOh1xybQ2JMdO8zMu8HJbDbLXirxaYD1Cl\nHtVvZ6oUw5kqaTtN03ymSg+M9R5DCmLY1uO6osRgjGpIzHJzOByWeZpmqyVnr5IWQt6fGMCa/9ag\n4O3Pe+dSQgcEGBNKGzsvy7LMk8XahB+zTaRfHYP3Pmw4B8X7EFqrrQFQLpQ283L49u3bYqw11kgx\nwoKf2fiOXooziclY9r9LzHw4LPO8Swx5SGJSDH5zbltX1CQfARrg3Rwl5p9//l0Ulmj+ARJDeiCm\ntlpKTjgbFNPVw/iaaVm+LRbPWSTmJ8PZKhZh4YgCtznnxwBjSgUS8+3f/8xSSiElSsxr5OVDTiXo\nrQMlp4Sq1EortZ6M73y4WYwxxigp8M3+MUDeSkpjKNfmNue8z1i5SinlUpt5Ofzz73/mMWAfT6XX\n4KNUqdYzVeqRvDNVulm01gpViZCfXhS0WnBa2dFtzvnNeV8Yp5xyuqvSt3//M/8URn4x3ouYkTpu\ntfR6hs1557EdggAAoZxypbTRxk7TNPUpJveuAqP9DRKOK3Pbuvpe1VuKYJQyLpgwxhhj7TRZ+3Yv\n4J2I2QPfNeeMhS/b/26Pmw8pV0A/hRJqJmutMXq/DdD7CgS9/y1s27Ztzo1bea0AlHGOCxhOB/0b\nvoJ3IwbRaky4NyFt32+PmwuxVOjjKBizdrLGar1Xs/zATG+Aa27bTQvG0Ltjp7CmF0un/gRigIzu\nvpyCDyGEELa7u+PqQsq1UcY4Z4LxCSXGaK1GwuP8UXqnVi5+RV6c8ynGXEqDbqC01nr3md/yJbyf\nxKCBzXgOeee39XjcfEile6tcCjFNFlMdSj0QcMNcSc6p7JrkfMamgUYoeofG2D9LlVqrrbaao3fb\n5vocoM3FmFsjlAuhpFSTnSZrtDZa8RFvOH+U2nNI4/u99zmX3AMWQkplJmvnaTJayee2Ufwa7ykx\ntdaaY/Dbuq7HdfPO+zOJUUrraRqapNgDvLRaMt6QThKDZ1xthDLeW2/+JIkhnZeSUnDueLy7O24h\nhhhiypUQyoRUxph5GseSQnN83/8Ysa2w7TYm1NqwUIgyjqWry+le/oYv4J2ML45bKDXnGNy63t1+\nv91ySinlWCoFynvP4n5aq4eujlBLTsE7t+6nUmg9lkcI5b3vcT+V3vIlvKMq1VoL1m4f727/930r\n/YipHFVJ22marLHGaKXVgxeBWnKM3nUT45xzEQBIIwBAGRsS8+lVac+KtVRSSSXljPZlXbdtqw3r\nCAF7ZOmYU40kYjMjIYTuFT99WFn06NhhoywFSihhQKTS2hg7TfNsLfYMvOVreUNigDQcTgCtxpRS\nzDGl7e52OLy96hTIqVs0xRjQ5S20pzvYef4eo6ExhIgVAJiNpZQyQvEmMU3TNBmjleRvELY7w1sR\ng/VLFVds1BJiiDGGGLf1eHfcXEil1oaDXWCf95Jkiphho4X3izHd09TQyUsBJxSU2vZhZZwyO1mk\nZp50n+P2Rq+FEPJWxIzilYZ1HjX5iC1a3m3buq0uxFwajELlITEixTH9smBiCShl++yUnRfMxeJM\nUcY451wwbqdpQmas1kr9cP98Nd5SlbBFoJRSovP9w3m/eedDKhWvhABkdOpnLqLsF4EqpVSNEMaA\njJrLrkophtBvSDgUHROW00mVlJJKviKB/xDegJi9OH0ftpDDhu6u2/pFCUs+GkDD191aKZnzJCPm\nqUlVugJQyuFUQ9dKKSmlEALmW7BGiAsppRTSTtZOdprnacLs7ieXmJRySn5b13Vb1231KUXsJhol\nhGcSw7gQmF+GWjBwVUdW4dS0Hn+SGKkUXic6uBD8NSUfD+Etj2uA1mXfrevxeDwe16MveLnJufXM\nGxmVl4UxLjhllEBrtTSgjBfZWD/a9pMrdYlD48sYx9z9zstsGWecs894KsGuSiWlGOK2rXd3x9u7\n422orbRaW2ltLxgbvFDaJy9Ba7URQpkQrUtMgwZnxjelhPOnKOVcSKW1mSbbjyXTrxOfVWIAWq05\nxxiC247H27vvt3ffA0DD1NKP/7IwShljBMsWG1AqhFRd4Vobc0XR+I6EC7CuSmawMk2TGXmFTyYx\nQLqXju+u896tfT3A5sKZccYwNyWMEoDKCqWUUUBBq400AoRQIvux3prrYWKsMapoYriQyphpmg7L\n3Csb5Otfw894PTH4/laAFp3DmQRuPa7Ox1zO5aS/qZQSzigFaJUSSglAayXnUmtG/182aNhP4m+/\n3x7Xzcc0xrhR2m/U8zIfDstktRRvG4bZ8RYS02pttdUaPd723LaumwtYpdEvP9jTRnB2IWfoDQIZ\n43FyKTXnlGIMAtuRoLVwd3d3XJ2PKdd9YLxQ2k7zYTkclmW2Wj13LsxT8QYS02rfsBHctq2YLHTO\n+ZBKu/cvKaWU9vZwAq0wAAK11ZKTTLmklGIIbiemxnXFqrSU22gFHTfqm5vDNE3m+QNznoq3kZhS\ncikFEz/rcV23gPVx91QJqxCx/IUANNJag1ZqlkKI1LNx3nJM6EKrads2hxIDrXdq8d6Jf/NtsdZY\nrT6vKvXDKOfk3bZux+N6PLqQQoypVBjjyfD6RznF8jpKGmm01lZFEUlwkXKONgbjtehj/2pL3jvv\nPRorgC4xShk7Hw7fDlprzN+/C95IYnJKKaHEHI93R5dyyj9JDKVj/MS4P7PKCy50VynqqI3WWtTW\nam2tthxCiCGEmErv4aPkJDEHqaSSn1liGhITo/fbth6Pd3e3vmD3WSPkZHxHxID3fDZpQPZGHKG1\nClorrRmmF1ptJaaYY4opVQzuUSyPsHZebr4dhOBCiM9sYyoOi+rJ5ePd7a3H5Em7tw4XRYZzTqAC\nFocAVgkxSkVUSmmllOQVRabWmksquaSSG/4rQvfyiMO3A2P8JfNPnoi3sTGl1zGgxNze+l6QCD8a\nX8Y556Q1isMTYUgCERLzrVIxJKXV2kot+AsYQ16GH7PcfFtO/cnvgRcTAyPShrc85z3urMSShhHV\nHp3V6IIILrgQHGqurJLCGpBxg6qt1lKzlIm2TkxP2UFtlRDCoDEAOsJ/tdazNtz+s94SLyUGxjRd\nOPm7R/R3axvhfkoI5V3gOedccMGE4JB7pDzv3YyAN25KCaGtQq1jDfGp7RPtUs3Rb0ctOXUcwTgj\ndO/efjO8XGL6U6/JjwQqLjnNpcFQEEqYFELw3iOD5PCWUkpJJM4KNBzsi92xjBJC6O7g4V/iVYsC\nADTWao5+05KRtsgesZL8ZRNifo0XE9N6NqSiv9s3Vm4+5tIIWhRCKeFCKSWVVKoTwzmvKcQYOWeU\ntdoabQSAQGu0EiBAsSSm59WGnQIK0FijNUenBCOtOK201koDQRP++uqye3i5Ko1aqeDcth7X9bhu\nwfvQVWnYRS6VwTZFiY0ynLMWVJCcMUpoZbVVApQQgEYrAWi0AQaH22in7tdzII3SmqOXnLaSnTXW\nWgOEUaCMsicNLn06XqFKBSO83m3rut4d1zsXY4wxlQZk76rmQuvJTNZaxcZY8OIx0wyN1MIKADQK\nBBohwKHRMQr8LBSKP5EQQmoOgpNWUlimOabSCBOsceCNsMenUD4Dr5CYhvObUWKOd8c7l1POKdd2\nNrqBS2XsNC/TrBnj6PnWPl23NVLw2oQSQwgAZWhNgGCAC8iZxAClNUdOW03BuyWmApQJJRhhTxkQ\n9yy83MaMDmjvt21dj3d3t76iw4uqhIvsuVRmWg7LctCccsYYpzxjnhlqBYaVdpQQaKwBNErp8ID6\n2IouLhQAKNCaGWklB2fmmEujXKgsOCH0CdM5n4WXqxIGHkNwzm3r8e7u9ntorUKFOlSJUkb7LK2b\nmxvDGGOUM8oT6lErBVWoMUobIY3QRimhMKo1CRACdNheHPJbM2k1BSnVkksjXEhTKiFACYPHB9N/\nADEArfzo7wYCQIDCiReGqjQfvn37x7Iei+GJM0JarTlXAtB4pT0vi85Iz13vqf3xO37mVjIXnAtX\nGmFSmZgrENLo60Y5/IRXqlIM3nuPcdmY+r2HUpQMyhm1y3I4HJblcDCDGCrrOOuJYIwSgFbJPlWH\nEHJi5RxACCGtEoDWGC/8lFJ5j0vByyWm9nZWF1IqDQjpVXRoYblgnAnOzLebm5ubZTZaU8oYYYxS\norQpuTVCRJRBcMYAyrCzj73t2P7I0T+SfYgx/9WcjJfhFRJTco4RA0m5NmBMCM4Fx9/Gpzksy2E5\nzJNWbAwaIDr3TiPphRCUUgA6KkgeYYaOzlKplVK4YouPzeef4rjG5FoIzoWQSmkEe6elHL8JKaQQ\nZp7neZonayQjoxUpm1obximxqaJV2lprtDX6qMQwxvsyYtklRozRKi98LQ/iNZ7vUKWIlZicSCX7\npg6ppJRSSamnPrlbS0p6KxJVmGBj+JoIgVpIq5WSJ5wr+z5MhRKDjf30rXl5zV0JVckNVaKMSqWx\niUQpLZVWSmErqDbaaHGa7qax4h03zZHWailQKa2EwC9Epgc3By9aKalEb0H4ueLztXgLiRnGF6TS\nxlhtjd6BAqSUVKIn3AilrcvLcIBrzpX1+9JjP/hBiXmbTpx7eI3EFEzJppRKbYQxihtq7GStMRo/\nlRxzYdgei2A9QSS1ZLTXB9VCCQF4PE7ZbQyGQbuN2buUXvpiHsDLjW8bqpRyyVjE0UtU59laq621\nxliFWyzHQimMiWPsVqksCLSWS06xUALQHrcU+/ZqpZUcp9Lbp/TfxI8ppaLxxYLkeV6WaZrMZCdr\nJ7lvINufN2ZppSyqCIBac0lRZQqttfb4m36uSsOP+UyhTdLvuwBAGaGsiVbFjH3Ch3m2nRj7YCUC\n9FJfVpSU++yPfUdZ/xNy+UMnoDCTsdZYVFgs1nyXlNtLiaGUcS6l0rr12EkDMc/ztKDXorX8ReXt\nKIIZS7XGzExsCRzO854FOHsUqa0xRltrpgMWhH+29AntNtDs5S9E3KtIVuJirXYvUMS+41pLHbNE\nsZBXCsGFFIKxcfc6kxhltcHBZYd5Ms9fKPVUvIIYJoRUygznhFJhp76Jxahfb5YGcsZLxZqAMXyW\nctEdRc4oo5zem1fFldF9eM48T0bhfIt3wOtViTDOGGWcMWGsQWipcH/9pW/f2/d3TerjZ4FQJpRR\nxmgjxqqFM6HgUqPjqPQ0fUpVYtilpg2OzeKcc6m1xsh3H1B2QWL2Ql7A+vpaasmt9Opogulpa63C\nGPG97bGs5xyk0toao9Xb9pyc8HqJYfuFUeJbqbTCnOOzJKbdkxg7z9OsOWeCCcbvSYyUUgol+47Z\nzycxu40RUkklpZJKKIn/J8W5U/cATjZmp6bP9uj9WXY+LIsRXRjP7AgTUkiOV/g+mfOlr+DXePWp\nlIvWSiultZJihBsY663VD3/v2alUx3DennnEJlAsJrNY58HFGTG0f2V8vmZq5C/xalXKUltttLba\nyD1MNTrxL3mjD/gxALVXTXGhtJ0PN/9MpzWsp5/LueBsBKcYp5+NGMIYl1KXWpuxuE3aiJFl/4Vr\nMSrmWxlrmDOeSMMd7s2xZpqXCVkR/D4xvUKAUUbuH+VvihdLDONCqFqBNAwvyFPo9dJTPQ3C2Lsn\n4l7bTMa0LdxqrrUxVnAhuORcnh5xr54Y3tMLn/+jeLkqcaFqA0oBI1J989Wv78dj9OxZT9+ohcer\nEu/njtbaWCO6I3DuxPGzKdHkfET9G+PlqsSFrEAoa3vwFZ/zxSfaG0kBR6dg0V7E+Wa1Ae1znKnA\noILRxgi2V8Ccfi7rSRj2DrGGM7xGlTBACbL7MZzTx1QJ+oTVLjF7m0BrSAsjlJ6pEmcMNefsERnr\no0T2ttv3wWuOa0kYFxK45IL3mPQvdl8BGWNyRtFeDCGmmHPtg0QxvcKFVFIrbaw5DVc//djeRTve\ngLeuF9rxCokBDGgTNmq+9kFtF74HRrfSgxKDKz/4ucTQIR1nP/es8o6+dXDqHK8ghrDKRavA+D1z\n+Evbi/W9rfS8dxg2BoDiHGfGhEQbgxNWf4pzj5qkUcv2Xty8XJU4Bd6gNTLS1fSR0Ou+YuJswtlJ\nYgAYpZRxxgWeStqa/dw5f0y6l8BS/OP74OUSQ9hIwt97or94plh3edYFGs78GBiTv89UiTw8/pD+\n8N93wcsl5tnPqw8hqpDQ8AbfB4fX1s2VkFyK3rempPyNS7E+dAL0GMVV/bYPIPIx4fxaJnX/OPyD\no2ff6db8VHzkMPWxRcu7AZxWPNK7xhhtzPLPt2Uy79dv80R87F6CknJO5XwcDE79JowJZSY72cku\n326W2b5bu81T8ZHbcno1Y+6kbNvmcx6joKU20zzP87wcbg6T0W88i+vZ+NBtOdiVHZGUzTnnSy11\npHetnQ/LzbIs8zzbtx5S9mx8tMTE0OfMO7dtGw4r7sbXTMvh5tvNMtnJfjGJKQWz3bvExD5phzAh\nlJnmw7d//5mN0UbrL2RjGpYBODdm2jkXCekt2VwqY5fDt3/+b8YEyReSmLGdYx+juG2JMko5ltZr\nlJj/m0e4+68npteO4VSqGLxzvs/1zZkDxwtSH1Fmp3nqsanfy8vHbLLA5r5xP/Lex9hn8tIeyhGT\n3Vd8PBI5/iB8xMxwDNwVvFH3Ok/cGIU5GCmkGBekC7tPPh7vvpfglKceI8PH3hKcHySEkkoqa3vB\nHtYy/25F+qgp8wANxkBA790+WYlSLqTUWukzVWL07Qvgn4+PsDHY3XjafeJD3CUGNwAZPVlkBqdW\n/VhH9RvwQXsJWmtlN74upfsSY6w5k5j3TqU9DR+ylwCrGnbj61OJpYdhmMCE7GR7eRofmbTfjI9Y\nCtOg1bPVJ85hwrq1XZWwPq1LzPsm0p6Kj5KYs00W3peW2+7HSKlNHx3eiSHkneO5T8H7E9OwxK7s\nI8piqoDSQqVSWls7Tbgr53XbY98W765KNSdcprV+vz2uzmPEjjLKSCNymuZ5npflME+TffOlC6/B\n+xNTUggxhHi8vTv22Q+EUgacAlV2muZ5WQ64we2tB6K/Bh8gMTE4550/Ho/HFWedYX6OUWomJGZZ\nZmuMlr87An6Gj1AlXBh13NYVJx7jNHXGGTe7Kk1G668lMa3EsK3H4926+dVvPqbacKWu4MLaaZqW\nZT4cJiWVUlJ8Fl4+SmLubr+vPrjgQsqlMkKZkFKgiZnnZTlY7DD9QhJTSwxuvfv+v2PsmywKVjlL\npdS02xjba3q/EDE5he149/2/x5RTjjmlQkfXV3dhluVwsAyHwX0l4xu9W+++//eIeyxKKaxLzHDt\n5mU5mL2T9J2f0FPxEbdrnBBeSi24tIFibYPSxhrMlWil3v15PBPvTcwYZW1syYUWIFBpb2tUe33w\nG7dMvwneXWKwglwbW3imFKAxLPmUUmH78Wc6is7w/hLDhFBKG1sYIwRn3XWJURrD3+81FfJVeH+J\n2VWJUoBS+ZiiuKvSL1oEfyM+QGK4lNrYUChtrRTGKGGMcSmlNqZ3CH6aQ/qED5QYAqVmfpUYxJmN\ngVZqEoydbIzpsamvaWP6qZRKqzljJGqcSn2j6Bc9lboq5VJLSj07cvVjsH1H6VJaw3UXFChb5nnG\nvXa9of8LGl/c5lhbI4QJrpWZpwP9dvMNq3nNUKWvJzGUcakqbkhV0pjJz54dlsNysyxIzGeKNZzh\n3VWJYicc4VwqZe0UfaAzYrJayc+UMjnHR6iSwolUSpsYYgqRTtbayU7Waim+qh9DGJd9Ka/OEbeu\n0j6/yvRZB1/S86WMC8KYkFnnkksuOReqlOqDD5lg/Gse15RxoIz3jvxaS62VyNGCL37qefw0+ADj\nS3kfjT5A90lnbDTHvfezeD4enW35WoxJzvt/gYxJ66Pf812bPl+KdyfmT8UnPA8+B67EXMCVmAu4\nEnMBV2Iu4ErMBVyJuYArMRdwJeYCrsRcwJWYC7gScwFXYi7gSswFXIm5gCsxF3Al5gKuxFzAlZgL\nuBJzAVdiLuBKzAVcibmAKzEXcCXmAq7EXMCVmAu4EnMBV2Iu4ErMBVyJuYArMRdwJeYCrsRcwP8D\n5EiiFs0+WdoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280 at 0x1131110D0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_digit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAAAAACi5bZQAAAYy0lEQVR4nO19W5vbOK4tQPAmu2r2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\nB50ESUbBWzAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280 at 0x113111090>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_digit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAAAAACi5bZQAAAjYklEQVR4nO2965rbRs41CtSZpJzM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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280 at 0x113111210>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_digit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAAAAACi5bZQAAAjeklEQVR4nO1d62ITO6+V756ZpLDP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\nMX6+UsumWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280 at 0x113111050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_digit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standartize data: \n",
    "# Rescale pixel grayness scale instead from 0-255, to from -1 to 1\n",
    "X_train_std = X_train/255.0*2 - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling SVC\n",
    "Train a support vector classifier using each of the following kernels:\n",
    "1. Linear\n",
    "2. Poly\n",
    "3. RBF\n",
    "(If you encounter any issues with training time or memory issues, then you may use a reduced dataset, but carefully detail how you reduced the dataset.)\n",
    "\n",
    "Report your *training times* on the dataset for the different kernels.\n",
    "Report your *error rates on the testing dataset* for the different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Imports and preperations for modeling with SVC\"\"\"\n",
    "from sklearn import svm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, \n",
    "# to turn the data in a (samples, feature) matrix: (as learned from http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)\n",
    "def flatten(data):\n",
    "    n_samples = len(data)\n",
    "    return data.reshape((n_samples, -1))\n",
    "\n",
    "X_train = flatten(X_train)\n",
    "X_test = flatten(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating a function which will train SVC and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "performances_cv = []\n",
    "\n",
    "def predict_svc(kernel_type,C=1, gamma=None,degree=None):\n",
    "    \"\"\" Function for training an SVC model and reporting results\n",
    "        Printing detailed results to console and adding key metrics summary to a list\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn import svm\n",
    "    import time\n",
    "    print(\"kernel_type: {}\".format(kernel_type))\n",
    "    if kernel_type != 'linear':\n",
    "        print(\"Parameters are: gamma={}, degree={}\".format(gamma,degree) )\n",
    "\n",
    "    \"\"\"Training the model using cross validation\"\"\"\n",
    "    start_time = time.time()\n",
    "    C = C  # SVM regularization parameter\n",
    "    if kernel_type == 'linear':\n",
    "        clf = svm.SVC(kernel=kernel_type, C=C) \n",
    "    if kernel_type == 'rbf':\n",
    "        clf = svm.SVC(kernel=kernel_type, C=C, gamma=gamma)\n",
    "    if kernel_type == 'poly':\n",
    "        clf = svm.SVC(kernel=kernel_type, C=C, degree=degree)\n",
    "    clf.fit(X_train,y_train) #train on training set\n",
    "    training_time = time.time() - start_time  #measuring training time\n",
    "    print(\"Training time: {}\".format(training_time))\n",
    "    # Predict using that SVC model on the test set\n",
    "    \n",
    "    \"\"\"Cross Validating the Model\"\"\"\n",
    "    cv_results = cross_validate(clf, X_train, y_train,\n",
    "                            cv=3, return_train_score=True)\n",
    "    \n",
    "    # averaging each CV result, making into a list\n",
    "    mean_cv_results = [np.mean(result) for result in cv_results.values()]   #averaging cv results\n",
    "\n",
    "    \"\"\" Testing final model on the test set\"\"\"\n",
    "    # Report performance and error rate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    error_rate = 1-accuracy\n",
    "    print(\"Got {} digits wrong out of {}\".format(len(y_test)-accuracy_score(y_test,y_pred,normalize=False),len(y_test)))\n",
    "    print(\"Accuracy Score (fraction correctly predicted): {}\").format(accuracy)\n",
    "    print(\"Error Rate): {}\").format(error_rate)\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print (\"________________________\")\n",
    "    \n",
    "    \"\"\" Add results together to a table\"\"\"\n",
    "    \n",
    "    #test_cols = ['Kernel_Type','Final_Training_time', 'Final_Test_Error_rate', 'Gamma (for RBF)','Degree (for Poly)']\n",
    "    #cv_cols = ['CV_'+col for col in cv_results.keys()] #add CV to names of columns of Cross Validation results\n",
    "    #cols = test_cols + cv_cols\n",
    "    results = [kernel_type,C,training_time, error_rate, gamma, degree] + mean_results\n",
    "    pd.DataFrame([results] , columns = cols)\n",
    "    #pd.DataFrame(performances[-1], columns = ['Kernel_Type','Training_time', 'Error_rate', 'Gamma (for RBF)','Degree (for Poly)','cv_results'])\n",
    "\n",
    "    # append performances into list\n",
    "    performances_cv.append(results)\n",
    "    \n",
    "    return performances, results\n",
    "\n",
    "## extracated the columns list resulting from the code\n",
    "cols = ['Kernel_Type','C',\n",
    " 'Final_Training_time',\n",
    " 'Final_Test_Error_rate',\n",
    " 'Gamma (for RBF)',\n",
    " 'Degree (for Poly)',\n",
    " 'CV_score_time',\n",
    " 'CV_test_score',\n",
    " 'CV_train_score',\n",
    " 'CV_fit_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_type: linear\n",
      "Training time: 6.37001991272\n",
      "Got 17 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.991308793456\n",
      "Error Rate): 0.00869120654397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: linear\n",
      "Training time: 4.93984699249\n",
      "Got 17 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.991308793456\n",
      "Error Rate): 0.00869120654397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: linear\n",
      "Training time: 5.50546193123\n",
      "Got 17 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.991308793456\n",
      "Error Rate): 0.00869120654397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: linear\n",
      "Training time: 6.48872089386\n",
      "Got 17 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.991308793456\n",
      "Error Rate): 0.00869120654397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: linear\n",
      "Training time: 5.57021999359\n",
      "Got 17 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.991308793456\n",
      "Error Rate): 0.00869120654397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel_Type</th>\n",
       "      <th>C</th>\n",
       "      <th>Final_Training_time</th>\n",
       "      <th>Final_Test_Error_rate</th>\n",
       "      <th>Gamma (for RBF)</th>\n",
       "      <th>Degree (for Poly)</th>\n",
       "      <th>CV_score_time</th>\n",
       "      <th>CV_test_score</th>\n",
       "      <th>CV_train_score</th>\n",
       "      <th>CV_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>5.57022</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Kernel_Type    C  Final_Training_time  Final_Test_Error_rate  \\\n",
       "0      linear  100              5.57022               0.008691   \n",
       "\n",
       "  Gamma (for RBF) Degree (for Poly)  CV_score_time  CV_test_score  \\\n",
       "0            None              None       0.645713       0.989566   \n",
       "\n",
       "   CV_train_score  CV_fit_time  \n",
       "0             1.0     2.404776  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Linear Kernel\"\"\"\n",
    "C_range = [0.1, 1, 10]\n",
    "results_linear = []\n",
    "for C in C_range:\n",
    "    performances_cv, results = predict_svc('linear', C=C) \n",
    "    results_linear.append(results)\n",
    "\n",
    "performances_cv\n",
    "pd.DataFrame(results_linear , columns = cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel_Type</th>\n",
       "      <th>C</th>\n",
       "      <th>Final_Training_time</th>\n",
       "      <th>Final_Test_Error_rate</th>\n",
       "      <th>Gamma (for RBF)</th>\n",
       "      <th>Degree (for Poly)</th>\n",
       "      <th>CV_score_time</th>\n",
       "      <th>CV_test_score</th>\n",
       "      <th>CV_train_score</th>\n",
       "      <th>CV_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.370020</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.939847</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.505462</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.488721</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>100.00</td>\n",
       "      <td>5.570220</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Kernel_Type       C  Final_Training_time  Final_Test_Error_rate  \\\n",
       "0      linear    0.01             6.370020               0.008691   \n",
       "1      linear    0.10             4.939847               0.008691   \n",
       "2      linear    1.00             5.505462               0.008691   \n",
       "3      linear   10.00             6.488721               0.008691   \n",
       "4      linear  100.00             5.570220               0.008691   \n",
       "\n",
       "  Gamma (for RBF) Degree (for Poly)  CV_score_time  CV_test_score  \\\n",
       "0            None              None       0.645713       0.989566   \n",
       "1            None              None       0.645713       0.989566   \n",
       "2            None              None       0.645713       0.989566   \n",
       "3            None              None       0.645713       0.989566   \n",
       "4            None              None       0.645713       0.989566   \n",
       "\n",
       "   CV_train_score  CV_fit_time  \n",
       "0             1.0     2.404776  \n",
       "1             1.0     2.404776  \n",
       "2             1.0     2.404776  \n",
       "3             1.0     2.404776  \n",
       "4             1.0     2.404776  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_linear , columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=2\n",
      "Training time: 5.89439821243\n",
      "Got 3 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.998466257669\n",
      "Error Rate): 0.00153374233129\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       1.00      1.00      1.00       982\n",
      "          8       1.00      1.00      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=2\n",
      "Training time: 5.45201897621\n",
      "Got 3 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.998466257669\n",
      "Error Rate): 0.00153374233129\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       1.00      1.00      1.00       982\n",
      "          8       1.00      1.00      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=2\n",
      "Training time: 6.46098804474\n",
      "Got 3 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.998466257669\n",
      "Error Rate): 0.00153374233129\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       1.00      1.00      1.00       982\n",
      "          8       1.00      1.00      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=3\n",
      "Training time: 7.29615497589\n",
      "Got 5 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.997443762781\n",
      "Error Rate): 0.00255623721881\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      1.00       982\n",
      "          8       1.00      0.99      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=3\n",
      "Training time: 7.24265098572\n",
      "Got 5 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.997443762781\n",
      "Error Rate): 0.00255623721881\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      1.00       982\n",
      "          8       1.00      0.99      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=3\n",
      "Training time: 5.68177509308\n",
      "Got 5 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.997443762781\n",
      "Error Rate): 0.00255623721881\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      1.00       982\n",
      "          8       1.00      0.99      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=4\n",
      "Training time: 6.51993298531\n",
      "Got 8 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.99591002045\n",
      "Error Rate): 0.0040899795501\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      1.00       982\n",
      "          8       1.00      0.99      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=4\n",
      "Training time: 6.58688688278\n",
      "Got 8 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.99591002045\n",
      "Error Rate): 0.0040899795501\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      1.00       982\n",
      "          8       1.00      0.99      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=4\n",
      "Training time: 6.56405091286\n",
      "Got 8 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.99591002045\n",
      "Error Rate): 0.0040899795501\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      1.00       982\n",
      "          8       1.00      0.99      1.00       974\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=5\n",
      "Training time: 7.13566303253\n",
      "Got 11 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.994376278119\n",
      "Error Rate): 0.00562372188139\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=5\n",
      "Training time: 7.32369494438\n",
      "Got 11 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.994376278119\n",
      "Error Rate): 0.00562372188139\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=5\n",
      "Training time: 7.49898791313\n",
      "Got 11 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.994376278119\n",
      "Error Rate): 0.00562372188139\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.99      1.00      0.99       982\n",
      "          8       1.00      0.99      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=7\n",
      "Training time: 10.1093459129\n",
      "Got 27 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.986196319018\n",
      "Error Rate): 0.0138036809816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.98      0.99      0.99       982\n",
      "          8       0.99      0.98      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=7\n",
      "Training time: 10.2210011482\n",
      "Got 27 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.986196319018\n",
      "Error Rate): 0.0138036809816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.98      0.99      0.99       982\n",
      "          8       0.99      0.98      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=7\n",
      "Training time: 9.78613495827\n",
      "Got 27 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.986196319018\n",
      "Error Rate): 0.0138036809816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.98      0.99      0.99       982\n",
      "          8       0.99      0.98      0.99       974\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=15\n",
      "Training time: 0.072909116745\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=15\n",
      "Training time: 0.0700039863586\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=15\n",
      "Training time: 0.0672030448914\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=50\n",
      "Training time: 0.0682828426361\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=50\n",
      "Training time: 0.0715510845184\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: poly\n",
      "Parameters are: gamma=None, degree=50\n",
      "Training time: 0.0702428817749\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel_Type</th>\n",
       "      <th>C</th>\n",
       "      <th>Final_Training_time</th>\n",
       "      <th>Final_Test_Error_rate</th>\n",
       "      <th>Gamma (for RBF)</th>\n",
       "      <th>Degree (for Poly)</th>\n",
       "      <th>CV_score_time</th>\n",
       "      <th>CV_test_score</th>\n",
       "      <th>CV_train_score</th>\n",
       "      <th>CV_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.894398</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.452019</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.460988</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.296155</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.242651</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.681775</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.519933</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.586887</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.564051</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.135663</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.323695</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.498988</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.109346</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.221001</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.786135</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.072909</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.070004</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.068283</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.071551</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.070243</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Kernel_Type      C  Final_Training_time  Final_Test_Error_rate  \\\n",
       "0         poly   0.01             5.894398               0.001534   \n",
       "1         poly   1.00             5.452019               0.001534   \n",
       "2         poly  10.00             6.460988               0.001534   \n",
       "3         poly   0.01             7.296155               0.002556   \n",
       "4         poly   1.00             7.242651               0.002556   \n",
       "5         poly  10.00             5.681775               0.002556   \n",
       "6         poly   0.01             6.519933               0.004090   \n",
       "7         poly   1.00             6.586887               0.004090   \n",
       "8         poly  10.00             6.564051               0.004090   \n",
       "9         poly   0.01             7.135663               0.005624   \n",
       "10        poly   1.00             7.323695               0.005624   \n",
       "11        poly  10.00             7.498988               0.005624   \n",
       "12        poly   0.01            10.109346               0.013804   \n",
       "13        poly   1.00            10.221001               0.013804   \n",
       "14        poly  10.00             9.786135               0.013804   \n",
       "15        poly   0.01             0.072909               0.502045   \n",
       "16        poly   1.00             0.070004               0.502045   \n",
       "17        poly  10.00             0.067203               0.502045   \n",
       "18        poly   0.01             0.068283               0.502045   \n",
       "19        poly   1.00             0.071551               0.502045   \n",
       "20        poly  10.00             0.070243               0.502045   \n",
       "\n",
       "   Gamma (for RBF)  Degree (for Poly)  CV_score_time  CV_test_score  \\\n",
       "0             None                  2       0.645713       0.989566   \n",
       "1             None                  2       0.645713       0.989566   \n",
       "2             None                  2       0.645713       0.989566   \n",
       "3             None                  3       0.645713       0.989566   \n",
       "4             None                  3       0.645713       0.989566   \n",
       "5             None                  3       0.645713       0.989566   \n",
       "6             None                  4       0.645713       0.989566   \n",
       "7             None                  4       0.645713       0.989566   \n",
       "8             None                  4       0.645713       0.989566   \n",
       "9             None                  5       0.645713       0.989566   \n",
       "10            None                  5       0.645713       0.989566   \n",
       "11            None                  5       0.645713       0.989566   \n",
       "12            None                  7       0.645713       0.989566   \n",
       "13            None                  7       0.645713       0.989566   \n",
       "14            None                  7       0.645713       0.989566   \n",
       "15            None                 15       0.645713       0.989566   \n",
       "16            None                 15       0.645713       0.989566   \n",
       "17            None                 15       0.645713       0.989566   \n",
       "18            None                 50       0.645713       0.989566   \n",
       "19            None                 50       0.645713       0.989566   \n",
       "20            None                 50       0.645713       0.989566   \n",
       "\n",
       "    CV_train_score  CV_fit_time  \n",
       "0              1.0     2.404776  \n",
       "1              1.0     2.404776  \n",
       "2              1.0     2.404776  \n",
       "3              1.0     2.404776  \n",
       "4              1.0     2.404776  \n",
       "5              1.0     2.404776  \n",
       "6              1.0     2.404776  \n",
       "7              1.0     2.404776  \n",
       "8              1.0     2.404776  \n",
       "9              1.0     2.404776  \n",
       "10             1.0     2.404776  \n",
       "11             1.0     2.404776  \n",
       "12             1.0     2.404776  \n",
       "13             1.0     2.404776  \n",
       "14             1.0     2.404776  \n",
       "15             1.0     2.404776  \n",
       "16             1.0     2.404776  \n",
       "17             1.0     2.404776  \n",
       "18             1.0     2.404776  \n",
       "19             1.0     2.404776  \n",
       "20             1.0     2.404776  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degrees = [2,3,4,5,7,15,50]\n",
    "results_polys = []\n",
    "for degree in degrees:\n",
    "    for C in [0.01,1,10]:\n",
    "        performances_cv, results = predict_svc('poly', C=C, degree = degree) \n",
    "        results_polys.append(results)\n",
    "\n",
    "# Showing a table of all poly results from Cross Validation and Tests\n",
    "pd.DataFrame(results_polys , columns = cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_type: rbf\n",
      "Parameters are: gamma=0.001, degree=None\n",
      "Training time: 124.099403143\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=0.001, degree=None\n",
      "Training time: 196.359272003\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=0.001, degree=None\n",
      "Training time: 267.885152817\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=0.1, degree=None\n",
      "Training time: 122.666485071\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=0.1, degree=None\n",
      "Training time: 303.459218979\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=0.1, degree=None\n",
      "Training time: 290.192517996\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=1, degree=None\n",
      "Training time: 139.801482916\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=1, degree=None\n",
      "Training time: 217.878437996\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=1, degree=None\n",
      "Training time: 299.449037075\n",
      "Got 982 digits wrong out of 1956\n",
      "Accuracy Score (fraction correctly predicted): 0.497955010225\n",
      "Error Rate): 0.502044989775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          4       0.00      0.00      0.00       982\n",
      "          8       0.50      1.00      0.66       974\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1956\n",
      "\n",
      "________________________\n",
      "kernel_type: rbf\n",
      "Parameters are: gamma=10, degree=None\n",
      "Training time: 136.973686934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-bc87be32091d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mperformances_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpredict_svc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mresults_rbfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-84e5963b0f12>\u001b[0m in \u001b[0;36mpredict_svc\u001b[0;34m(kernel_type, C, gamma, degree)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"\"\"Cross Validating the Model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     cv_results = cross_validate(clf, X_train, y_train,\n\u001b[0;32m---> 32\u001b[0;31m                             cv=3, return_train_score=True)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             return_times=True)\n\u001b[0;32m--> 195\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomereldor/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" RBF KERNEL \"\"\"\n",
    "results_rbfs = []\n",
    "for gamma in [0.001,0.1,1,10,100,'auto']:\n",
    "    for C in [0.01,1,10]:\n",
    "        performances_cv, results =  predict_svc('rbf',C=C,gamma=gamma) \n",
    "        results_rbfs.append(results)\n",
    "\n",
    "pd.DataFrame(results_rbfs, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel_Type</th>\n",
       "      <th>C</th>\n",
       "      <th>Final_Training_time</th>\n",
       "      <th>Final_Test_Error_rate</th>\n",
       "      <th>Gamma (for RBF)</th>\n",
       "      <th>Degree (for Poly)</th>\n",
       "      <th>CV_score_time</th>\n",
       "      <th>CV_test_score</th>\n",
       "      <th>CV_train_score</th>\n",
       "      <th>CV_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.01</td>\n",
       "      <td>124.099403</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.00</td>\n",
       "      <td>196.359272</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.00</td>\n",
       "      <td>267.885153</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.01</td>\n",
       "      <td>122.666485</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.100</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.00</td>\n",
       "      <td>303.459219</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.100</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.00</td>\n",
       "      <td>290.192518</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.100</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.01</td>\n",
       "      <td>139.801483</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.00</td>\n",
       "      <td>217.878438</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.00</td>\n",
       "      <td>299.449037</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Kernel_Type      C  Final_Training_time  Final_Test_Error_rate  \\\n",
       "0         rbf   0.01           124.099403               0.502045   \n",
       "1         rbf   1.00           196.359272               0.502045   \n",
       "2         rbf  10.00           267.885153               0.502045   \n",
       "3         rbf   0.01           122.666485               0.502045   \n",
       "4         rbf   1.00           303.459219               0.502045   \n",
       "5         rbf  10.00           290.192518               0.502045   \n",
       "6         rbf   0.01           139.801483               0.502045   \n",
       "7         rbf   1.00           217.878438               0.502045   \n",
       "8         rbf  10.00           299.449037               0.502045   \n",
       "\n",
       "   Gamma (for RBF) Degree (for Poly)  CV_score_time  CV_test_score  \\\n",
       "0            0.001              None       0.645713       0.989566   \n",
       "1            0.001              None       0.645713       0.989566   \n",
       "2            0.001              None       0.645713       0.989566   \n",
       "3            0.100              None       0.645713       0.989566   \n",
       "4            0.100              None       0.645713       0.989566   \n",
       "5            0.100              None       0.645713       0.989566   \n",
       "6            1.000              None       0.645713       0.989566   \n",
       "7            1.000              None       0.645713       0.989566   \n",
       "8            1.000              None       0.645713       0.989566   \n",
       "\n",
       "   CV_train_score  CV_fit_time  \n",
       "0             1.0     2.404776  \n",
       "1             1.0     2.404776  \n",
       "2             1.0     2.404776  \n",
       "3             1.0     2.404776  \n",
       "4             1.0     2.404776  \n",
       "5             1.0     2.404776  \n",
       "6             1.0     2.404776  \n",
       "7             1.0     2.404776  \n",
       "8             1.0     2.404776  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_rbfs, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Modles Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel_Type</th>\n",
       "      <th>C</th>\n",
       "      <th>Final_Training_time</th>\n",
       "      <th>Final_Test_Error_rate</th>\n",
       "      <th>Gamma (for RBF)</th>\n",
       "      <th>Degree (for Poly)</th>\n",
       "      <th>CV_score_time</th>\n",
       "      <th>CV_test_score</th>\n",
       "      <th>CV_train_score</th>\n",
       "      <th>CV_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.370020</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.939847</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.505462</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.488721</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>100.00</td>\n",
       "      <td>5.570220</td>\n",
       "      <td>0.008691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.894398</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.452019</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.460988</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.296155</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.242651</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.681775</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.519933</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.586887</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.564051</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.135663</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.323695</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.498988</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.109346</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.221001</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.786135</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.072909</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.070004</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.068283</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>poly</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.071551</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>poly</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.070243</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.01</td>\n",
       "      <td>124.099403</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.00</td>\n",
       "      <td>196.359272</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.00</td>\n",
       "      <td>267.885153</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.01</td>\n",
       "      <td>122.666485</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.00</td>\n",
       "      <td>303.459219</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.00</td>\n",
       "      <td>290.192518</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.01</td>\n",
       "      <td>139.801483</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.00</td>\n",
       "      <td>217.878438</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.00</td>\n",
       "      <td>299.449037</td>\n",
       "      <td>0.502045</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.404776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Kernel_Type       C  Final_Training_time  Final_Test_Error_rate  \\\n",
       "0       linear    0.01             6.370020               0.008691   \n",
       "1       linear    0.10             4.939847               0.008691   \n",
       "2       linear    1.00             5.505462               0.008691   \n",
       "3       linear   10.00             6.488721               0.008691   \n",
       "4       linear  100.00             5.570220               0.008691   \n",
       "5         poly    0.01             5.894398               0.001534   \n",
       "6         poly    1.00             5.452019               0.001534   \n",
       "7         poly   10.00             6.460988               0.001534   \n",
       "8         poly    0.01             7.296155               0.002556   \n",
       "9         poly    1.00             7.242651               0.002556   \n",
       "10        poly   10.00             5.681775               0.002556   \n",
       "11        poly    0.01             6.519933               0.004090   \n",
       "12        poly    1.00             6.586887               0.004090   \n",
       "13        poly   10.00             6.564051               0.004090   \n",
       "14        poly    0.01             7.135663               0.005624   \n",
       "15        poly    1.00             7.323695               0.005624   \n",
       "16        poly   10.00             7.498988               0.005624   \n",
       "17        poly    0.01            10.109346               0.013804   \n",
       "18        poly    1.00            10.221001               0.013804   \n",
       "19        poly   10.00             9.786135               0.013804   \n",
       "20        poly    0.01             0.072909               0.502045   \n",
       "21        poly    1.00             0.070004               0.502045   \n",
       "22        poly   10.00             0.067203               0.502045   \n",
       "23        poly    0.01             0.068283               0.502045   \n",
       "24        poly    1.00             0.071551               0.502045   \n",
       "25        poly   10.00             0.070243               0.502045   \n",
       "26         rbf    0.01           124.099403               0.502045   \n",
       "27         rbf    1.00           196.359272               0.502045   \n",
       "28         rbf   10.00           267.885153               0.502045   \n",
       "29         rbf    0.01           122.666485               0.502045   \n",
       "30         rbf    1.00           303.459219               0.502045   \n",
       "31         rbf   10.00           290.192518               0.502045   \n",
       "32         rbf    0.01           139.801483               0.502045   \n",
       "33         rbf    1.00           217.878438               0.502045   \n",
       "34         rbf   10.00           299.449037               0.502045   \n",
       "\n",
       "    Gamma (for RBF)  Degree (for Poly)  CV_score_time  CV_test_score  \\\n",
       "0               NaN                NaN       0.645713       0.989566   \n",
       "1               NaN                NaN       0.645713       0.989566   \n",
       "2               NaN                NaN       0.645713       0.989566   \n",
       "3               NaN                NaN       0.645713       0.989566   \n",
       "4               NaN                NaN       0.645713       0.989566   \n",
       "5               NaN                2.0       0.645713       0.989566   \n",
       "6               NaN                2.0       0.645713       0.989566   \n",
       "7               NaN                2.0       0.645713       0.989566   \n",
       "8               NaN                3.0       0.645713       0.989566   \n",
       "9               NaN                3.0       0.645713       0.989566   \n",
       "10              NaN                3.0       0.645713       0.989566   \n",
       "11              NaN                4.0       0.645713       0.989566   \n",
       "12              NaN                4.0       0.645713       0.989566   \n",
       "13              NaN                4.0       0.645713       0.989566   \n",
       "14              NaN                5.0       0.645713       0.989566   \n",
       "15              NaN                5.0       0.645713       0.989566   \n",
       "16              NaN                5.0       0.645713       0.989566   \n",
       "17              NaN                7.0       0.645713       0.989566   \n",
       "18              NaN                7.0       0.645713       0.989566   \n",
       "19              NaN                7.0       0.645713       0.989566   \n",
       "20              NaN               15.0       0.645713       0.989566   \n",
       "21              NaN               15.0       0.645713       0.989566   \n",
       "22              NaN               15.0       0.645713       0.989566   \n",
       "23              NaN               50.0       0.645713       0.989566   \n",
       "24              NaN               50.0       0.645713       0.989566   \n",
       "25              NaN               50.0       0.645713       0.989566   \n",
       "26            0.001                NaN       0.645713       0.989566   \n",
       "27            0.001                NaN       0.645713       0.989566   \n",
       "28            0.001                NaN       0.645713       0.989566   \n",
       "29            0.100                NaN       0.645713       0.989566   \n",
       "30            0.100                NaN       0.645713       0.989566   \n",
       "31            0.100                NaN       0.645713       0.989566   \n",
       "32            1.000                NaN       0.645713       0.989566   \n",
       "33            1.000                NaN       0.645713       0.989566   \n",
       "34            1.000                NaN       0.645713       0.989566   \n",
       "\n",
       "    CV_train_score  CV_fit_time  \n",
       "0              1.0     2.404776  \n",
       "1              1.0     2.404776  \n",
       "2              1.0     2.404776  \n",
       "3              1.0     2.404776  \n",
       "4              1.0     2.404776  \n",
       "5              1.0     2.404776  \n",
       "6              1.0     2.404776  \n",
       "7              1.0     2.404776  \n",
       "8              1.0     2.404776  \n",
       "9              1.0     2.404776  \n",
       "10             1.0     2.404776  \n",
       "11             1.0     2.404776  \n",
       "12             1.0     2.404776  \n",
       "13             1.0     2.404776  \n",
       "14             1.0     2.404776  \n",
       "15             1.0     2.404776  \n",
       "16             1.0     2.404776  \n",
       "17             1.0     2.404776  \n",
       "18             1.0     2.404776  \n",
       "19             1.0     2.404776  \n",
       "20             1.0     2.404776  \n",
       "21             1.0     2.404776  \n",
       "22             1.0     2.404776  \n",
       "23             1.0     2.404776  \n",
       "24             1.0     2.404776  \n",
       "25             1.0     2.404776  \n",
       "26             1.0     2.404776  \n",
       "27             1.0     2.404776  \n",
       "28             1.0     2.404776  \n",
       "29             1.0     2.404776  \n",
       "30             1.0     2.404776  \n",
       "31             1.0     2.404776  \n",
       "32             1.0     2.404776  \n",
       "33             1.0     2.404776  \n",
       "34             1.0     2.404776  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## All Modles Results\n",
    "pd.DataFrame(results_linear + results_polys + results_rbfs, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results table shows clearly that in this case, the Polynomial Kernel with a low higher degree outperforms the others (2 was the best, even up to 5 was relatively better than other model types), having the lowest error rate (0.001) and similarly short training time as the Linear Kernel (5-6 seconds vs ~3-6 minutes minimum for the RBF). \n",
    "\n",
    "The Polynomial Kernels showed that the best performance was with only a 2nd heigher degree polynomial; and accuracy declined from there, until being absurdly bad with above 15 degrees, weirdly remaining exactly the same between 15,20, or 50 polynomial degrees.\n",
    "Overfitting the training set is a probably culprit, as the higher the degree, the more it might overfit. Our data is probably simple enough to represent well with a second degree polynomial (or even with just the first degree / linear...), so that maybe above ~10 degrees there was really no more room for improvement of fit.\n",
    "\n",
    "The RBF performs surprisingly poorly, both in terms of accuracy (0.5!) and time (~x40 times slower than the linear or polynomial!). The Kernel paramters didn't seem to matter much for the RBF; it probably was just equally as bad. I tried to debug it but found no bugs there; I suspect it might just be hitting its limit of incorrect classification every time.\n",
    "\n",
    "Additionally, the Cross-Validation results might be off since they seem uniform across the board; I also tried to debug that but didn't find where is the bug on time at least.\n",
    "\n",
    "This shows that this is probably a simple enough task and dataset such that RBF might overcomplicate or learn irrelevant dimensions, and a simpler linear or polynomial kernels suffice. \n",
    "This shows us that the more complex the model is, not necessarily the better, but it can actually be worse! So it's worth it to try various directions for the parameters before choosing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
